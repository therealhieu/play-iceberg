{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Flink + Iceberg Query Examples\n",
    "\n",
    "This notebook demonstrates querying Iceberg tables using PyFlink in the **flink-dev Docker environment**.\n",
    "\n",
    "## Environment Setup\n",
    "- **Container**: `flink-dev` (from docker-compose.yml)\n",
    "- **Flink Cluster**: Running inside flink-dev container\n",
    "- **Jupyter Lab**: http://localhost:8889\n",
    "- **Flink Web UI**: http://localhost:8082\n",
    "- **JAR Configuration**: Minimal setup with iceberg-flink-runtime and hadoop-common\n",
    "\n",
    "## Prerequisites\n",
    "1. Start the development stack: `docker-compose up flink-dev`\n",
    "2. Ensure Iceberg tables exist (run notebooks 1-7 first)\n",
    "3. Access this notebook through Jupyter Lab in the container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Flink Development Environment ===\n",
      "Container: flink-dev\n",
      "PyFlink imported successfully!\n",
      "Python version: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n",
      "Working directory: /opt/workspace\n",
      "Flink home: /opt/flink\n"
     ]
    }
   ],
   "source": [
    "# Environment Verification - flink-dev container\n",
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "print(\"=== Flink Development Environment ===\")\n",
    "print(f\"Container: flink-dev\")\n",
    "print(f\"PyFlink imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Flink home: {os.environ.get('FLINK_HOME', 'Not set')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JAR Configuration Verification ===\n",
      "Location: /opt/flink/lib/ (inside flink-dev container)\n",
      "============================================================\n",
      "Iceberg JARs:\n",
      "  ✓ iceberg-aws-bundle-1.9.1.jar (57.6 MB)\n",
      "  ✓ iceberg-flink-runtime-1.20-1.9.1.jar (34.7 MB)\n",
      "\n",
      "Hadoop JARs:\n",
      "  ✓ flink-s3-fs-hadoop-1.20.0.jar (30.1 MB)\n",
      "  ✓ flink-shaded-hadoop-2-uber-2.8.3-10.0.jar (41.3 MB)\n",
      "  ✓ hadoop-common-3.3.4.jar (4.3 MB)\n",
      "  ✓ hadoop-hdfs-client-3.3.4.jar (5.2 MB)\n",
      "\n",
      "Other Flink JARs: 13 files\n",
      "Total JAR files: 19\n"
     ]
    }
   ],
   "source": [
    "# Verify JAR configuration in flink-dev environment\n",
    "jar_paths = [\"/opt/flink/lib/*.jar\"]\n",
    "\n",
    "print(\"=== JAR Configuration Verification ===\")\n",
    "print(\"Location: /opt/flink/lib/ (inside flink-dev container)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iceberg_jars = []\n",
    "hadoop_jars = []\n",
    "other_jars = []\n",
    "\n",
    "for path_pattern in jar_paths:\n",
    "    jars = glob.glob(path_pattern)\n",
    "    if jars:\n",
    "        for jar in sorted(jars):\n",
    "            jar_name = os.path.basename(jar)\n",
    "            jar_size = os.path.getsize(jar) / (1024 * 1024)  # MB\n",
    "            \n",
    "            if \"iceberg\" in jar_name.lower():\n",
    "                iceberg_jars.append((jar_name, jar_size))\n",
    "            elif \"hadoop\" in jar_name.lower():\n",
    "                hadoop_jars.append((jar_name, jar_size))\n",
    "            else:\n",
    "                other_jars.append((jar_name, jar_size))\n",
    "\n",
    "print(\"Iceberg JARs:\")\n",
    "for jar_name, size in iceberg_jars:\n",
    "    print(f\"  ✓ {jar_name} ({size:.1f} MB)\")\n",
    "\n",
    "print(\"\\nHadoop JARs:\")\n",
    "for jar_name, size in hadoop_jars:\n",
    "    print(f\"  ✓ {jar_name} ({size:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nOther Flink JARs: {len(other_jars)} files\")\n",
    "print(f\"Total JAR files: {len(iceberg_jars) + len(hadoop_jars) + len(other_jars)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Flink Environment Initialization ===\n",
      "Flink Table Environment created:\n",
      "  Mode: Batch (optimized for Iceberg)\n",
      "  Environment: flink-dev container\n",
      "  Ready for Iceberg operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Flink Table Environment for batch processing\n",
    "print(\"=== Flink Environment Initialization ===\")\n",
    "\n",
    "# Create batch environment (optimal for Iceberg queries)\n",
    "settings = EnvironmentSettings.new_instance().in_batch_mode().build()\n",
    "table_env = TableEnvironment.create(settings)\n",
    "\n",
    "print(\"Flink Table Environment created:\")\n",
    "print(f\"  Mode: Batch (optimized for Iceberg)\")\n",
    "print(f\"  Environment: flink-dev container\")\n",
    "print(f\"  Ready for Iceberg operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Available Catalogs ===\n",
      "+-----------------+\n",
      "|    catalog name |\n",
      "+-----------------+\n",
      "| default_catalog |\n",
      "+-----------------+\n",
      "1 row in set\n"
     ]
    }
   ],
   "source": [
    "# Verify available catalogs in flink-dev environment\n",
    "print(\"=== Available Catalogs ===\")\n",
    "table_env.execute_sql(\"SHOW CATALOGS\").print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Iceberg Catalog Configuration ===\n",
      "Connecting to Iceberg REST catalog...\n",
      "✓ Iceberg catalog configured successfully\n",
      "✓ Connected to REST catalog at http://iceberg-rest:8181\n",
      "✓ Using MinIO storage at http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "# Configure Iceberg catalog connection\n",
    "print(\"=== Iceberg Catalog Configuration ===\")\n",
    "print(\"Connecting to Iceberg REST catalog...\")\n",
    "\n",
    "# Create Iceberg catalog with connection to services in docker-compose stack\n",
    "catalog_sql = \"\"\"\n",
    "CREATE CATALOG IF NOT EXISTS iceberg_catalog WITH (\n",
    "    'type' = 'iceberg',\n",
    "    'catalog-type'='rest',\n",
    "    'uri' = 'http://iceberg-rest:8181',\n",
    "    'warehouse' = 's3://warehouse/',\n",
    "    'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO',\n",
    "    's3.endpoint' = 'http://minio:9000',\n",
    "    's3.access-key-id' = 'admin',\n",
    "    's3.secret-access-key' = 'password',\n",
    "    's3.path-style-access' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "table_env.execute_sql(catalog_sql)\n",
    "table_env.use_catalog(\"iceberg_catalog\")\n",
    "\n",
    "print(\"✓ Iceberg catalog configured successfully\")\n",
    "print(\"✓ Connected to REST catalog at http://iceberg-rest:8181\")\n",
    "print(\"✓ Using MinIO storage at http://minio:9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Accessing Iceberg Tables ===\n",
      "Switching to play_iceberg namespace...\n",
      "Available tables in play_iceberg namespace:\n",
      "+------------+\n",
      "| table name |\n",
      "+------------+\n",
      "|      users |\n",
      "+------------+\n",
      "1 row in set\n"
     ]
    }
   ],
   "source": [
    "# Access the play_iceberg namespace and list available tables\n",
    "print(\"=== Accessing Iceberg Tables ===\")\n",
    "print(\"Switching to play_iceberg namespace...\")\n",
    "\n",
    "table_env.execute_sql(\"USE iceberg_catalog.play_iceberg\")\n",
    "\n",
    "print(\"Available tables in play_iceberg namespace:\")\n",
    "table_env.execute_sql(\"SHOW TABLES\").print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Querying Iceberg Table ===\n",
      "Executing: SELECT * FROM users LIMIT 10\n",
      "\n",
      "Empty set\n",
      "\n",
      "✓ Successfully queried Iceberg table using PyFlink\n",
      "✓ Data retrieved from MinIO storage via Iceberg REST catalog\n",
      "✓ flink-dev environment working correctly\n"
     ]
    }
   ],
   "source": [
    "# Query Iceberg table using Flink SQL\n",
    "print(\"=== Querying Iceberg Table ===\")\n",
    "print(\"Executing: SELECT * FROM users LIMIT 10\")\n",
    "print()\n",
    "\n",
    "# Execute query and display results\n",
    "table_env.execute_sql(\"SELECT * FROM users LIMIT 10\").print()\n",
    "\n",
    "print()\n",
    "print(\"✓ Successfully queried Iceberg table using PyFlink\")\n",
    "print(\"✓ Data retrieved from MinIO storage via Iceberg REST catalog\")\n",
    "print(\"✓ flink-dev environment working correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Additional Query Examples ===\n",
      "1. Count total users:\n",
      "2025-07-01 14:13:36,501 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.zstd]\n",
      "+----------------------+\n",
      "|          total_users |\n",
      "+----------------------+\n",
      "|                    5 |\n",
      "+----------------------+\n",
      "1 row in set\n",
      "\n",
      "2. Active vs Inactive users:\n",
      "2025-07-01 14:13:39,216 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.zstd]\n",
      "+-----------+----------------------+\n",
      "| is_active |           user_count |\n",
      "+-----------+----------------------+\n",
      "|      TRUE |                    4 |\n",
      "|     FALSE |                    1 |\n",
      "+-----------+----------------------+\n",
      "2 rows in set\n",
      "\n",
      "3. Latest 5 users by updated timestamp:\n",
      "2025-07-01 14:13:40,982 INFO  org.apache.hadoop.io.compress.CodecPool                      [] - Got brand-new decompressor [.zstd]\n",
      "+----------------------+--------------------------------+----------------------------+\n",
      "|              user_id |                       username |                 updated_at |\n",
      "+----------------------+--------------------------------+----------------------------+\n",
      "|                    1 |                       john_doe | 2025-07-01 14:13:24.144741 |\n",
      "|                    2 |                     jane_smith | 2025-07-01 14:13:24.144741 |\n",
      "|                    3 |                   alice_wonder | 2025-07-01 14:13:24.144741 |\n",
      "|                    4 |                    bob_builder | 2025-07-01 14:13:24.144741 |\n",
      "|                    5 |                  charlie_brown | 2025-07-01 14:13:24.144741 |\n",
      "+----------------------+--------------------------------+----------------------------+\n",
      "5 rows in set\n",
      "\n",
      "=== flink-dev Environment Validation Complete ===\n",
      "✓ All Iceberg operations successful\n",
      "✓ PyFlink integration working\n",
      "✓ Docker services connected properly\n"
     ]
    }
   ],
   "source": [
    "# Additional query examples\n",
    "print(\"=== Additional Query Examples ===\")\n",
    "\n",
    "# Count total records\n",
    "print(\"1. Count total users:\")\n",
    "table_env.execute_sql(\"SELECT COUNT(*) as total_users FROM users\").print()\n",
    "\n",
    "# Active vs inactive users\n",
    "print(\"\\n2. Active vs Inactive users:\")\n",
    "table_env.execute_sql(\"\"\"\n",
    "    SELECT is_active, COUNT(*) as user_count \n",
    "    FROM users \n",
    "    GROUP BY is_active\n",
    "\"\"\").print()\n",
    "\n",
    "# Recent users (latest 5)\n",
    "print(\"\\n3. Latest 5 users by updated timestamp:\")\n",
    "table_env.execute_sql(\"\"\"\n",
    "    SELECT user_id, username, updated_at \n",
    "    FROM users \n",
    "    ORDER BY updated_at DESC \n",
    "    LIMIT 5\n",
    "\"\"\").print()\n",
    "\n",
    "print(\"\\n=== flink-dev Environment Validation Complete ===\")\n",
    "print(\"✓ All Iceberg operations successful\")\n",
    "print(\"✓ PyFlink integration working\")\n",
    "print(\"✓ Docker services connected properly\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
