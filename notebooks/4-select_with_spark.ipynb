{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Querying Iceberg Tables with Apache Spark\n",
    "\n",
    "This notebook demonstrates how to query Apache Iceberg tables using Apache Spark SQL and DataFrame APIs. Spark provides comprehensive support for Iceberg tables with advanced features like partition pruning, predicate pushdown, and vectorized execution.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to configure Spark for Iceberg table access\n",
    "- How to use Spark SQL for querying Iceberg tables\n",
    "- How to leverage Spark DataFrame API for data operations\n",
    "- How to optimize queries using Iceberg metadata\n",
    "- How to perform advanced analytics with Spark on Iceberg\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed notebooks 1-3 (table creation, data insertion, basic querying)\n",
    "- Apache Spark with Iceberg extensions\n",
    "- Docker environment running\n",
    "- Understanding of Spark SQL and DataFrames\n",
    "\n",
    "## Why Spark with Iceberg?\n",
    "\n",
    "Spark is the premier choice for Iceberg operations because:\n",
    "- **Native Integration**: Built-in Iceberg support in Spark 3.x\n",
    "- **Full Feature Set**: Complete ACID operations, schema evolution\n",
    "- **Performance**: Advanced optimizations like vectorized execution\n",
    "- **Scalability**: Distributed processing for large datasets\n",
    "- **Ecosystem**: Rich integration with data platforms\n",
    "- **SQL Compatibility**: Standard SQL interface with extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Initialize Spark session with Iceberg configuration. The configuration is loaded from `spark-defaults.conf` which contains all necessary Iceberg settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spark-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Configuration:\n",
      "========================================\n",
      "Spark version: 3.5.5\n",
      "Default catalog: rest\n",
      "Iceberg extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "\n",
      "Iceberg Configuration:\n",
      "==============================\n",
      "Catalog URI: http://iceberg-rest:8181/\n",
      "Warehouse: s3://warehouse/\n",
      "S3 Endpoint: http://minio:9000\n",
      "\n",
      "Spark session ready for Iceberg operations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:44:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create Spark session with Iceberg configuration\n",
    "# Configuration is loaded from spark-defaults.conf\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Analytics with Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Default catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Iceberg extensions: {spark.conf.get('spark.sql.extensions')}\")\n",
    "\n",
    "# Verify Iceberg configuration\n",
    "print(\"\\nIceberg Configuration:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Catalog URI: {spark.conf.get('spark.sql.catalog.rest.uri')}\")\n",
    "print(f\"Warehouse: {spark.conf.get('spark.sql.catalog.rest.warehouse')}\")\n",
    "print(f\"S3 Endpoint: {spark.conf.get('spark.sql.catalog.rest.s3.endpoint')}\")\n",
    "\n",
    "print(\"\\nSpark session ready for Iceberg operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog-exploration-section",
   "metadata": {},
   "source": [
    "## Catalog and Namespace Exploration\n",
    "\n",
    "Explore the Iceberg catalog structure and verify our table exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "catalog-exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog Structure Exploration:\n",
      "========================================\n",
      "\n",
      "1. Available Namespaces:\n",
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|play_iceberg|\n",
      "+------------+\n",
      "\n",
      "\n",
      "2. Switched to 'rest.play_iceberg' namespace\n",
      "\n",
      "3. Tables in Namespace:\n",
      "+------------+---------+-----------+\n",
      "|   namespace|tableName|isTemporary|\n",
      "+------------+---------+-----------+\n",
      "|play_iceberg|    users|      false|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 table(s) in the namespace\n",
      "✓ Users table found and accessible\n"
     ]
    }
   ],
   "source": [
    "# Explore catalog structure\n",
    "print(\"Catalog Structure Exploration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show available namespaces\n",
    "print(\"\\n1. Available Namespaces:\")\n",
    "namespaces_df = spark.sql(\"SHOW NAMESPACES\")\n",
    "namespaces_df.show()\n",
    "\n",
    "# Set default namespace\n",
    "spark.sql(\"USE rest.`play_iceberg`\")\n",
    "print(\"\\n2. Switched to 'rest.play_iceberg' namespace\")\n",
    "\n",
    "# Show tables in namespace\n",
    "print(\"\\n3. Tables in Namespace:\")\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show()\n",
    "\n",
    "# Get table count\n",
    "table_count = tables_df.count()\n",
    "print(f\"\\nFound {table_count} table(s) in the namespace\")\n",
    "\n",
    "# Verify users table exists\n",
    "users_table_exists = (\n",
    "    tables_df.filter(\"tableName = 'users'\")\n",
    "    .count() > 0\n",
    ")\n",
    "\n",
    "if users_table_exists:\n",
    "    print(\"✓ Users table found and accessible\")\n",
    "else:\n",
    "    print(\"✗ Users table not found - run previous notebooks first\")\n",
    "    raise Exception(\"Users table not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-inspection-section",
   "metadata": {},
   "source": [
    "## Table Structure Inspection\n",
    "\n",
    "Examine the table schema, partitioning, and metadata to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "table-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Structure Analysis:\n",
      "===================================\n",
      "\n",
      "1. Extended Table Description:\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                                             |comment|\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|user_id                     |bigint                                                                                                                |NULL   |\n",
      "|username                    |string                                                                                                                |NULL   |\n",
      "|email                       |string                                                                                                                |NULL   |\n",
      "|is_active                   |boolean                                                                                                               |NULL   |\n",
      "|created_year                |int                                                                                                                   |NULL   |\n",
      "|created_month               |int                                                                                                                   |NULL   |\n",
      "|created_day                 |int                                                                                                                   |NULL   |\n",
      "|updated_at                  |timestamp_ntz                                                                                                         |NULL   |\n",
      "|# Partition Information     |                                                                                                                      |       |\n",
      "|# col_name                  |data_type                                                                                                             |comment|\n",
      "|created_year                |int                                                                                                                   |NULL   |\n",
      "|created_month               |int                                                                                                                   |NULL   |\n",
      "|created_day                 |int                                                                                                                   |NULL   |\n",
      "|                            |                                                                                                                      |       |\n",
      "|# Metadata Columns          |                                                                                                                      |       |\n",
      "|_spec_id                    |int                                                                                                                   |       |\n",
      "|_partition                  |struct<created_year:int,created_month:int,created_day:int>                                                            |       |\n",
      "|_file                       |string                                                                                                                |       |\n",
      "|_pos                        |bigint                                                                                                                |       |\n",
      "|_deleted                    |boolean                                                                                                               |       |\n",
      "|                            |                                                                                                                      |       |\n",
      "|# Detailed Table Information|                                                                                                                      |       |\n",
      "|Name                        |rest.play_iceberg.users                                                                                               |       |\n",
      "|Type                        |MANAGED                                                                                                               |       |\n",
      "|Location                    |s3://warehouse/play_iceberg/users                                                                                     |       |\n",
      "|Provider                    |iceberg                                                                                                               |       |\n",
      "|Table Properties            |[current-snapshot-id=3824926247776142618,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd]|       |\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n",
      "\n",
      "2. Table Summary:\n",
      "   Name: rest.play_iceberg.users\n",
      "   Type: MANAGED\n",
      "   Location: s3://warehouse/play_iceberg/users\n",
      "   Provider: iceberg\n",
      "\n",
      "3. Table Schema:\n",
      "root\n",
      " |-- user_id: long (nullable = false)\n",
      " |-- username: string (nullable = false)\n",
      " |-- email: string (nullable = false)\n",
      " |-- is_active: boolean (nullable = false)\n",
      " |-- created_year: integer (nullable = false)\n",
      " |-- created_month: integer (nullable = false)\n",
      " |-- created_day: integer (nullable = false)\n",
      " |-- updated_at: timestamp_ntz (nullable = false)\n",
      "\n",
      "\n",
      "4. Schema Information:\n",
      "   Total columns: 8\n",
      "   Column names: ['user_id', 'username', 'email', 'is_active', 'created_year', 'created_month', 'created_day', 'updated_at']\n"
     ]
    }
   ],
   "source": [
    "# Detailed table inspection\n",
    "print(\"Table Structure Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Get detailed table description\n",
    "print(\"\\n1. Extended Table Description:\")\n",
    "table_desc = spark.sql(\"DESCRIBE EXTENDED users\")\n",
    "table_desc.show(50, truncate=False)\n",
    "\n",
    "# Extract key information\n",
    "desc_rows = table_desc.collect()\n",
    "table_info = {}\n",
    "\n",
    "for row in desc_rows:\n",
    "    if row['col_name'] and row['data_type']:\n",
    "        if row['col_name'].startswith('#'):\n",
    "            continue\n",
    "        if row['col_name'] in ['Name', 'Type', 'Location', 'Provider']:\n",
    "            table_info[row['col_name']] = row['data_type']\n",
    "\n",
    "print(\"\\n2. Table Summary:\")\n",
    "for key, value in table_info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Show table schema in a cleaner format\n",
    "print(\"\\n3. Table Schema:\")\n",
    "users_df = spark.table(\"users\")\n",
    "users_df.printSchema()\n",
    "\n",
    "print(\"\\n4. Schema Information:\")\n",
    "print(f\"   Total columns: {len(users_df.columns)}\")\n",
    "print(f\"   Column names: {users_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-queries-section",
   "metadata": {},
   "source": [
    "## Basic SQL Queries\n",
    "\n",
    "Execute fundamental SQL queries to explore the data and verify table functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "basic-queries",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SQL Query Examples:\n",
      "===================================\n",
      "\n",
      "1. Record Count:\n",
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|            5|\n",
      "+-------------+\n",
      "\n",
      "Total records in table: 5\n",
      "\n",
      "2. Sample Data (first 10 records):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|user_id|username     |email                    |is_active|created_year|created_month|created_day|updated_at                |\n",
      "+-------+-------------+-------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|1      |john_doe     |john.doe@example.com     |true     |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "|2      |jane_smith   |jane.smith@example.com   |true     |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "|3      |alice_wonder |alice.wonder@example.com |false    |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "|4      |bob_builder  |bob.builder@example.com  |true     |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "|5      |charlie_brown|charlie.brown@example.com|true     |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "+-------+-------------+-------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "\n",
      "\n",
      "3. Data Types and Sample Values:\n",
      "   user_id: bigint (sample: 1)\n",
      "   username: string (sample: john_doe)\n",
      "   email: string (sample: john.doe@example.com)\n",
      "   is_active: boolean (sample: True)\n",
      "   created_year: int (sample: 2025)\n",
      "   created_month: int (sample: 7)\n",
      "   created_day: int (sample: 1)\n",
      "   updated_at: timestamp_ntz (sample: 2025-07-01 13:41:16.173663)\n",
      "\n",
      "4. Basic Statistics:\n",
      "+-----------+------------+--------------+----------+--------------------------+--------------------------+\n",
      "|total_users|active_users|inactive_users|years_span|earliest_update           |latest_update             |\n",
      "+-----------+------------+--------------+----------+--------------------------+--------------------------+\n",
      "|5          |4           |1             |1         |2025-07-01 13:41:16.173663|2025-07-01 13:41:16.173663|\n",
      "+-----------+------------+--------------+----------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic SQL queries\n",
    "print(\"Basic SQL Query Examples:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Query 1: Check if table has data\n",
    "print(\"\\n1. Record Count:\")\n",
    "count_result = spark.sql(\"SELECT COUNT(*) as total_records FROM users\")\n",
    "count_result.show()\n",
    "\n",
    "record_count = count_result.collect()[0]['total_records']\n",
    "print(f\"Total records in table: {record_count}\")\n",
    "\n",
    "if record_count == 0:\n",
    "    print(\"\\nTable is empty. Let's check table history and snapshots.\")\n",
    "    \n",
    "    # Check table history\n",
    "    try:\n",
    "        history_df = spark.sql(\"SELECT * FROM rest.`play_iceberg`.users.history\")\n",
    "        print(\"\\nTable History:\")\n",
    "        history_df.show()\n",
    "        \n",
    "        if history_df.count() == 0:\n",
    "            print(\"No snapshots found. Table was created but no data inserted.\")\n",
    "            print(\"Please run notebook 2 to insert sample data.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking table history: {e}\")\n",
    "        \n",
    "else:\n",
    "    # Query 2: Show sample data\n",
    "    print(\"\\n2. Sample Data (first 10 records):\")\n",
    "    sample_data = spark.sql(\"SELECT * FROM users ORDER BY user_id LIMIT 10\")\n",
    "    sample_data.show(truncate=False)\n",
    "    \n",
    "    # Query 3: Data type verification\n",
    "    print(\"\\n3. Data Types and Sample Values:\")\n",
    "    for column in users_df.columns:\n",
    "        dtype = dict(users_df.dtypes)[column]\n",
    "        sample_val = users_df.select(column).first()[0]\n",
    "        print(f\"   {column}: {dtype} (sample: {sample_val})\")\n",
    "    \n",
    "    # Query 4: Basic aggregations\n",
    "    print(\"\\n4. Basic Statistics:\")\n",
    "    stats_query = \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_users,\n",
    "        SUM(CASE WHEN is_active THEN 1 ELSE 0 END) as active_users,\n",
    "        SUM(CASE WHEN NOT is_active THEN 1 ELSE 0 END) as inactive_users,\n",
    "        COUNT(DISTINCT created_year) as years_span,\n",
    "        MIN(updated_at) as earliest_update,\n",
    "        MAX(updated_at) as latest_update\n",
    "    FROM users\n",
    "    \"\"\"\n",
    "    \n",
    "    stats_df = spark.sql(stats_query)\n",
    "    stats_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataframe-api-section",
   "metadata": {},
   "source": [
    "## DataFrame API Operations\n",
    "\n",
    "Demonstrate Spark DataFrame API for programmatic data operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dataframe-api",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame API Examples:\n",
      "========================================\n",
      "\n",
      "1. Active Users (DataFrame API):\n",
      "+-------+-------------+--------------------+\n",
      "|user_id|     username|               email|\n",
      "+-------+-------------+--------------------+\n",
      "|      1|     john_doe|john.doe@example.com|\n",
      "|      2|   jane_smith|jane.smith@exampl...|\n",
      "|      4|  bob_builder|bob.builder@examp...|\n",
      "|      5|charlie_brown|charlie.brown@exa...|\n",
      "+-------+-------------+--------------------+\n",
      "\n",
      "\n",
      "2. User Count by Activity Status:\n",
      "+---------+----------+--------------------------------------------------+\n",
      "|is_active|user_count|usernames                                         |\n",
      "+---------+----------+--------------------------------------------------+\n",
      "|false    |1         |[alice_wonder]                                    |\n",
      "|true     |4         |[john_doe, jane_smith, bob_builder, charlie_brown]|\n",
      "+---------+----------+--------------------------------------------------+\n",
      "\n",
      "\n",
      "3. Email Domain Analysis:\n",
      "+-------------+-------------------------+------------+---------------+---------+\n",
      "|username     |email                    |email_domain|username_length|is_active|\n",
      "+-------------+-------------------------+------------+---------------+---------+\n",
      "|john_doe     |john.doe@example.com     |example.com |8              |true     |\n",
      "|jane_smith   |jane.smith@example.com   |example.com |10             |true     |\n",
      "|bob_builder  |bob.builder@example.com  |example.com |11             |true     |\n",
      "|alice_wonder |alice.wonder@example.com |example.com |12             |false    |\n",
      "|charlie_brown|charlie.brown@example.com|example.com |13             |true     |\n",
      "+-------------+-------------------------+------------+---------------+---------+\n",
      "\n",
      "\n",
      "4. User Ranking with Window Functions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:45:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/07/01 13:45:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/07/01 13:45:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/07/01 13:45:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/07/01 13:45:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+---------+-------------+\n",
      "|user_rank|user_id|     username|is_active|is_first_half|\n",
      "+---------+-------+-------------+---------+-------------+\n",
      "|        1|      1|     john_doe|     true|         true|\n",
      "|        2|      2|   jane_smith|     true|         true|\n",
      "|        3|      3| alice_wonder|    false|        false|\n",
      "|        4|      4|  bob_builder|     true|        false|\n",
      "|        5|      5|charlie_brown|     true|        false|\n",
      "+---------+-------+-------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API examples\n",
    "print(\"Spark DataFrame API Examples:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load table as DataFrame\n",
    "users_df = spark.table(\"users\")\n",
    "\n",
    "if users_df.count() > 0:\n",
    "    # Example 1: Column selection and filtering\n",
    "    print(\"\\n1. Active Users (DataFrame API):\")\n",
    "    active_users = (\n",
    "        users_df\n",
    "        .filter(F.col(\"is_active\") == True)\n",
    "        .select(\"user_id\", \"username\", \"email\")\n",
    "        .orderBy(\"user_id\")\n",
    "    )\n",
    "    active_users.show()\n",
    "    \n",
    "    # Example 2: Aggregations with groupBy\n",
    "    print(\"\\n2. User Count by Activity Status:\")\n",
    "    activity_summary = (\n",
    "        users_df\n",
    "        .groupBy(\"is_active\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"user_count\"),\n",
    "            F.collect_list(\"username\").alias(\"usernames\")\n",
    "        )\n",
    "        .orderBy(\"is_active\")\n",
    "    )\n",
    "    activity_summary.show(truncate=False)\n",
    "    \n",
    "    # Example 3: String operations\n",
    "    print(\"\\n3. Email Domain Analysis:\")\n",
    "    email_analysis = (\n",
    "        users_df\n",
    "        .withColumn(\n",
    "            \"email_domain\", \n",
    "            F.split(F.col(\"email\"), \"@\").getItem(1)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"username_length\",\n",
    "            F.length(F.col(\"username\"))\n",
    "        )\n",
    "        .select(\n",
    "            \"username\", \"email\", \"email_domain\", \n",
    "            \"username_length\", \"is_active\"\n",
    "        )\n",
    "        .orderBy(\"username_length\")\n",
    "    )\n",
    "    email_analysis.show(truncate=False)\n",
    "    \n",
    "    # Example 4: Window functions\n",
    "    print(\"\\n4. User Ranking with Window Functions:\")\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    window_spec = Window.orderBy(F.col(\"user_id\"))\n",
    "    window_spec_unbounded = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    total_count = users_df.count()\n",
    "    \n",
    "    ranked_users = (\n",
    "        users_df\n",
    "        .withColumn(\"user_rank\", F.row_number().over(window_spec))\n",
    "        .withColumn(\"total_users\", F.lit(total_count))\n",
    "        .withColumn(\"is_first_half\", F.col(\"user_rank\") <= (F.col(\"total_users\") / 2))\n",
    "        .select(\n",
    "            \"user_rank\", \"user_id\", \"username\", \n",
    "            \"is_active\", \"is_first_half\"\n",
    "        )\n",
    "        .orderBy(\"user_rank\")\n",
    "    )\n",
    "    ranked_users.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nTable is empty - DataFrame API examples skipped\")\n",
    "    print(\"Please run notebook 2 to insert sample data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-analytics-section",
   "metadata": {},
   "source": [
    "## Advanced Analytical Queries\n",
    "\n",
    "Demonstrate complex analytical operations that showcase Spark's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advanced-analytics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Analytics Examples:\n",
      "========================================\n",
      "\n",
      "1. Comprehensive User Profile Analysis:\n",
      "+-------+-------------+-----------------+-------------------------+-----------+---------+------------------+------------+--------------------------+-----------------+\n",
      "|user_id|username     |username_category|email                    |domain     |is_active|status_description|created_year|updated_at                |days_since_update|\n",
      "+-------+-------------+-----------------+-------------------------+-----------+---------+------------------+------------+--------------------------+-----------------+\n",
      "|1      |john_doe     |Medium           |john.doe@example.com     |example.com|true     |Active User       |2025        |2025-07-01 13:41:16.173663|0                |\n",
      "|2      |jane_smith   |Medium           |jane.smith@example.com   |example.com|true     |Active User       |2025        |2025-07-01 13:41:16.173663|0                |\n",
      "|3      |alice_wonder |Long             |alice.wonder@example.com |example.com|false    |Inactive User     |2025        |2025-07-01 13:41:16.173663|0                |\n",
      "|4      |bob_builder  |Medium           |bob.builder@example.com  |example.com|true     |Active User       |2025        |2025-07-01 13:41:16.173663|0                |\n",
      "|5      |charlie_brown|Long             |charlie.brown@example.com|example.com|true     |Active User       |2025        |2025-07-01 13:41:16.173663|0                |\n",
      "+-------+-------------+-----------------+-------------------------+-----------+---------+------------------+------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "2. Temporal Pattern Analysis:\n",
      "+------------+-------------+-----------+-------------+--------------------+-----------------+----------------------------------------------------------------+\n",
      "|created_year|created_month|created_day|users_created|active_users_created|active_percentage|usernames                                                       |\n",
      "+------------+-------------+-----------+-------------+--------------------+-----------------+----------------------------------------------------------------+\n",
      "|2025        |7            |1          |5            |4                   |80.00            |[john_doe, jane_smith, alice_wonder, bob_builder, charlie_brown]|\n",
      "+------------+-------------+-----------+-------------+--------------------+-----------------+----------------------------------------------------------------+\n",
      "\n",
      "\n",
      "3. Multi-Dimensional Analysis:\n",
      "+-----------+---------+----------+-------------------+----------------+-----------------------------+----------------------------+------------------------------------------------+\n",
      "|domain     |is_active|user_count|avg_username_length|avg_email_length|first_username_alphabetically|last_username_alphabetically|all_usernames                                   |\n",
      "+-----------+---------+----------+-------------------+----------------+-----------------------------+----------------------------+------------------------------------------------+\n",
      "|example.com|true     |4         |10.5               |22.5            |bob_builder                  |john_doe                    |john_doe, jane_smith, bob_builder, charlie_brown|\n",
      "|example.com|false    |1         |12.0               |24.0            |alice_wonder                 |alice_wonder                |alice_wonder                                    |\n",
      "+-----------+---------+----------+-------------------+----------------+-----------------------------+----------------------------+------------------------------------------------+\n",
      "\n",
      "\n",
      "4. Query Performance Analysis:\n",
      "Query execution plan:\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['is_active], ['is_active, 'COUNT(1) AS user_count#829, 'AVG('LENGTH('username)) AS avg_username_length#830]\n",
      "+- 'Filter ('created_year = 2025)\n",
      "   +- 'UnresolvedRelation [users], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "is_active: boolean, user_count: bigint, avg_username_length: double\n",
      "Aggregate [is_active#834], [is_active#834, count(1) AS user_count#829L, avg(length(username#832)) AS avg_username_length#830]\n",
      "+- Filter (created_year#835 = 2025)\n",
      "   +- SubqueryAlias rest.play_iceberg.users\n",
      "      +- RelationV2[user_id#831L, username#832, email#833, is_active#834, created_year#835, created_month#836, created_day#837, updated_at#838] rest.play_iceberg.users rest.play_iceberg.users\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [is_active#834], [is_active#834, count(1) AS user_count#829L, avg(length(username#832)) AS avg_username_length#830]\n",
      "+- Project [username#832, is_active#834]\n",
      "   +- RelationV2[username#832, is_active#834, created_year#835] rest.play_iceberg.users\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[is_active#834], functions=[count(1), avg(length(username#832))], output=[is_active#834, user_count#829L, avg_username_length#830])\n",
      "   +- Exchange hashpartitioning(is_active#834, 200), ENSURE_REQUIREMENTS, [plan_id=724]\n",
      "      +- HashAggregate(keys=[is_active#834], functions=[partial_count(1), partial_avg(length(username#832))], output=[is_active#834, count#857L, sum#858, count#859L])\n",
      "         +- Project [username#832, is_active#834]\n",
      "            +- BatchScan rest.play_iceberg.users[username#832, is_active#834, created_year#835] rest.play_iceberg.users (branch=null) [filters=created_year = 2025, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n",
      "Query result:\n",
      "+---------+----------+-------------------+\n",
      "|is_active|user_count|avg_username_length|\n",
      "+---------+----------+-------------------+\n",
      "|     true|         4|               10.5|\n",
      "|    false|         1|               12.0|\n",
      "+---------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced analytical queries\n",
    "print(\"Advanced Analytics Examples:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if users_df.count() > 0:\n",
    "    # Analysis 1: Comprehensive user profile analysis\n",
    "    print(\"\\n1. Comprehensive User Profile Analysis:\")\n",
    "    profile_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        username,\n",
    "        CASE \n",
    "            WHEN LENGTH(username) < 8 THEN 'Short'\n",
    "            WHEN LENGTH(username) < 12 THEN 'Medium'\n",
    "            ELSE 'Long'\n",
    "        END as username_category,\n",
    "        email,\n",
    "        SPLIT_PART(email, '@', 2) as domain,\n",
    "        is_active,\n",
    "        CASE \n",
    "            WHEN is_active THEN 'Active User'\n",
    "            ELSE 'Inactive User'\n",
    "        END as status_description,\n",
    "        created_year,\n",
    "        updated_at,\n",
    "        DATEDIFF(CURRENT_DATE(), DATE(updated_at)) as days_since_update\n",
    "    FROM users\n",
    "    ORDER BY user_id\n",
    "    \"\"\")\n",
    "    profile_analysis.show(truncate=False)\n",
    "    \n",
    "    # Analysis 2: Temporal pattern analysis\n",
    "    print(\"\\n2. Temporal Pattern Analysis:\")\n",
    "    temporal_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        created_year,\n",
    "        created_month,\n",
    "        created_day,\n",
    "        COUNT(*) as users_created,\n",
    "        COUNT(CASE WHEN is_active THEN 1 END) as active_users_created,\n",
    "        ROUND(\n",
    "            COUNT(CASE WHEN is_active THEN 1 END) * 100.0 / COUNT(*), \n",
    "            2\n",
    "        ) as active_percentage,\n",
    "        COLLECT_LIST(username) as usernames\n",
    "    FROM users\n",
    "    GROUP BY created_year, created_month, created_day\n",
    "    ORDER BY created_year, created_month, created_day\n",
    "    \"\"\")\n",
    "    temporal_analysis.show(truncate=False)\n",
    "    \n",
    "    # Analysis 3: Complex aggregations with multiple dimensions\n",
    "    print(\"\\n3. Multi-Dimensional Analysis:\")\n",
    "    multi_dim_analysis = spark.sql(\"\"\"\n",
    "    WITH user_metrics AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            LENGTH(username) as username_len,\n",
    "            LENGTH(email) as email_len,\n",
    "            SPLIT_PART(email, '@', 2) as domain\n",
    "        FROM users\n",
    "    )\n",
    "    SELECT \n",
    "        domain,\n",
    "        is_active,\n",
    "        COUNT(*) as user_count,\n",
    "        AVG(username_len) as avg_username_length,\n",
    "        AVG(email_len) as avg_email_length,\n",
    "        MIN(username) as first_username_alphabetically,\n",
    "        MAX(username) as last_username_alphabetically,\n",
    "        ARRAY_JOIN(COLLECT_LIST(username), ', ') as all_usernames\n",
    "    FROM user_metrics\n",
    "    GROUP BY domain, is_active\n",
    "    ORDER BY domain, is_active DESC\n",
    "    \"\"\")\n",
    "    multi_dim_analysis.show(truncate=False)\n",
    "    \n",
    "    # Analysis 4: Performance demonstration with explain plan\n",
    "    print(\"\\n4. Query Performance Analysis:\")\n",
    "    performance_query = \"\"\"\n",
    "    SELECT \n",
    "        is_active,\n",
    "        COUNT(*) as user_count,\n",
    "        AVG(LENGTH(username)) as avg_username_length\n",
    "    FROM users\n",
    "    WHERE created_year = 2025\n",
    "    GROUP BY is_active\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Query execution plan:\")\n",
    "    spark.sql(performance_query).explain(True)\n",
    "    \n",
    "    print(\"\\nQuery result:\")\n",
    "    spark.sql(performance_query).show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nTable is empty - advanced analytics examples skipped\")\n",
    "    print(\"Please run notebook 2 to insert sample data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partition-analysis-section",
   "metadata": {},
   "source": [
    "## Partition and Performance Analysis\n",
    "\n",
    "Analyze table partitioning and demonstrate performance optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "partition-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition and Performance Analysis:\n",
      "=============================================\n",
      "\n",
      "1. Partition Distribution:\n",
      "+------------+-------------+-----------+------------+------------+\n",
      "|created_year|created_month|created_day|record_count|unique_users|\n",
      "+------------+-------------+-----------+------------+------------+\n",
      "|        2025|            7|          1|           5|           5|\n",
      "+------------+-------------+-----------+------------+------------+\n",
      "\n",
      "\n",
      "2. Partition Pruning Demonstration:\n",
      "Querying partition: year=2025, month=7, day=1\n",
      "\n",
      "Execution plan (should show partition pruning):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [user_id#999L ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(user_id#999L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=936]\n",
      "      +- Project [user_id#999L, username#1000, email#1001, is_active#1002]\n",
      "         +- BatchScan rest.play_iceberg.users[user_id#999L, username#1000, email#1001, is_active#1002, created_year#1003, created_month#1004, created_day#1005] rest.play_iceberg.users (branch=null) [filters=created_year = 2025, created_month = 7, created_day = 1, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n",
      "\n",
      "Partition query result:\n",
      "+-------+-------------+--------------------+---------+\n",
      "|user_id|     username|               email|is_active|\n",
      "+-------+-------------+--------------------+---------+\n",
      "|      1|     john_doe|john.doe@example.com|     true|\n",
      "|      2|   jane_smith|jane.smith@exampl...|     true|\n",
      "|      3| alice_wonder|alice.wonder@exam...|    false|\n",
      "|      4|  bob_builder|bob.builder@examp...|     true|\n",
      "|      5|charlie_brown|charlie.brown@exa...|     true|\n",
      "+-------+-------------+--------------------+---------+\n",
      "\n",
      "\n",
      "3. Performance Comparison:\n",
      "Full table scan result:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n",
      "Partition-pruned query result:\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Performance Benefits:\n",
      "- Partition pruning reduces data scan volume\n",
      "- Columnar storage enables efficient projections\n",
      "- Iceberg metadata accelerates query planning\n",
      "- Vectorized execution improves computational efficiency\n"
     ]
    }
   ],
   "source": [
    "# Partition and performance analysis\n",
    "print(\"Partition and Performance Analysis:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if users_df.count() > 0:\n",
    "    # Analysis 1: Partition distribution\n",
    "    print(\"\\n1. Partition Distribution:\")\n",
    "    partition_dist = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        created_year,\n",
    "        created_month,\n",
    "        created_day,\n",
    "        COUNT(*) as record_count,\n",
    "        COUNT(DISTINCT user_id) as unique_users\n",
    "    FROM users\n",
    "    GROUP BY created_year, created_month, created_day\n",
    "    ORDER BY created_year, created_month, created_day\n",
    "    \"\"\")\n",
    "    partition_dist.show()\n",
    "    \n",
    "    # Analysis 2: Demonstrate partition pruning\n",
    "    print(\"\\n2. Partition Pruning Demonstration:\")\n",
    "    \n",
    "    # Get a specific partition value\n",
    "    sample_partition = spark.sql(\n",
    "        \"SELECT DISTINCT created_year, created_month, created_day FROM users LIMIT 1\"\n",
    "    ).collect()[0]\n",
    "    \n",
    "    year, month, day = (\n",
    "        sample_partition['created_year'],\n",
    "        sample_partition['created_month'],\n",
    "        sample_partition['created_day']\n",
    "    )\n",
    "    \n",
    "    partition_query = f\"\"\"\n",
    "    SELECT \n",
    "        user_id, username, email, is_active\n",
    "    FROM users\n",
    "    WHERE created_year = {year}\n",
    "      AND created_month = {month}\n",
    "      AND created_day = {day}\n",
    "    ORDER BY user_id\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Querying partition: year={year}, month={month}, day={day}\")\n",
    "    \n",
    "    # Show execution plan\n",
    "    print(\"\\nExecution plan (should show partition pruning):\")\n",
    "    spark.sql(partition_query).explain()\n",
    "    \n",
    "    print(\"\\nPartition query result:\")\n",
    "    spark.sql(partition_query).show()\n",
    "    \n",
    "    # Analysis 3: Query performance comparison\n",
    "    print(\"\\n3. Performance Comparison:\")\n",
    "    \n",
    "    # Full table scan\n",
    "    full_scan_query = \"SELECT COUNT(*) FROM users WHERE is_active = true\"\n",
    "    \n",
    "    # Partition-pruned query\n",
    "    pruned_query = f\"\"\"\n",
    "    SELECT COUNT(*) FROM users \n",
    "    WHERE is_active = true \n",
    "      AND created_year = {year}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Full table scan result:\")\n",
    "    spark.sql(full_scan_query).show()\n",
    "    \n",
    "    print(\"Partition-pruned query result:\")\n",
    "    spark.sql(pruned_query).show()\n",
    "    \n",
    "    print(\"\\nPerformance Benefits:\")\n",
    "    print(\"- Partition pruning reduces data scan volume\")\n",
    "    print(\"- Columnar storage enables efficient projections\")\n",
    "    print(\"- Iceberg metadata accelerates query planning\")\n",
    "    print(\"- Vectorized execution improves computational efficiency\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nTable is empty - partition analysis skipped\")\n",
    "    print(\"Please run notebook 2 to insert sample data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-exploration-section",
   "metadata": {},
   "source": [
    "## Iceberg Metadata Exploration\n",
    "\n",
    "Explore Iceberg's metadata tables to understand table internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "metadata-exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg Metadata Exploration:\n",
      "========================================\n",
      "\n",
      "1. Table History (Snapshots):\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id|is_current_ancestor|\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|2025-07-01 13:41:23.087|3824926247776142618|NULL     |true               |\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "\n",
      "Total snapshots: 1\n",
      "\n",
      "2. Manifest Files:\n",
      "+-------+---------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+------------------------------------------------------------------------+\n",
      "|content|path                                                                                   |length|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries                                                     |\n",
      "+-------+---------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+------------------------------------------------------------------------+\n",
      "|0      |s3://warehouse/play_iceberg/users/metadata/034ca984-2799-480e-81d4-2263f71f3be5-m0.avro|5272  |0                |3824926247776142618|1                     |0                        |0                       |0                       |0                          |0                         |[{false, false, 2025, 2025}, {false, false, 7, 7}, {false, false, 1, 1}]|\n",
      "+-------+---------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+------------------------------------------------------------------------+\n",
      "\n",
      "Total manifest files: 1\n",
      "\n",
      "3. Data Files:\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "|file_path                                                                                                                                  |file_format|record_count|file_size_in_bytes|file_size_mb|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "|s3://warehouse/play_iceberg/users/data/created_year=2025/created_month=7/created_day=1/00000-0-034ca984-2799-480e-81d4-2263f71f3be5.parquet|PARQUET    |5           |3185              |0.003       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "\n",
      "\n",
      "File Statistics:\n",
      "+-----------+-------------+----------------+-------------------+\n",
      "|total_files|total_records|total_size_bytes|avg_file_size_bytes|\n",
      "+-----------+-------------+----------------+-------------------+\n",
      "|          1|            5|            3185|             3185.0|\n",
      "+-----------+-------------+----------------+-------------------+\n",
      "\n",
      "\n",
      "Metadata Benefits:\n",
      "- Complete audit trail of all table changes\n",
      "- File-level statistics for query optimization\n",
      "- Time travel capabilities through snapshots\n",
      "- Schema evolution tracking\n"
     ]
    }
   ],
   "source": [
    "# Iceberg metadata exploration\n",
    "print(\"Iceberg Metadata Exploration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Explore table history\n",
    "    print(\"\\n1. Table History (Snapshots):\")\n",
    "    history_df = spark.sql(\"SELECT * FROM rest.`play_iceberg`.users.history\")\n",
    "    history_df.show(truncate=False)\n",
    "    \n",
    "    if history_df.count() > 0:\n",
    "        print(f\"Total snapshots: {history_df.count()}\")\n",
    "        \n",
    "        # Show manifest information\n",
    "        print(\"\\n2. Manifest Files:\")\n",
    "        try:\n",
    "            manifests_df = spark.sql(\"SELECT * FROM rest.`play_iceberg`.users.manifests\")\n",
    "            manifests_df.show(truncate=False)\n",
    "            print(f\"Total manifest files: {manifests_df.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not access manifests: {e}\")\n",
    "        \n",
    "        # Show data files information\n",
    "        print(\"\\n3. Data Files:\")\n",
    "        try:\n",
    "            files_df = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                file_path,\n",
    "                file_format,\n",
    "                record_count,\n",
    "                file_size_in_bytes,\n",
    "                ROUND(file_size_in_bytes / 1024.0 / 1024.0, 3) as file_size_mb\n",
    "            FROM rest.`play_iceberg`.users.files\n",
    "            \"\"\")\n",
    "            files_df.show(truncate=False)\n",
    "            \n",
    "            # File statistics\n",
    "            file_stats = files_df.agg(\n",
    "                F.count(\"*\").alias(\"total_files\"),\n",
    "                F.sum(\"record_count\").alias(\"total_records\"),\n",
    "                F.sum(\"file_size_in_bytes\").alias(\"total_size_bytes\"),\n",
    "                F.avg(\"file_size_in_bytes\").alias(\"avg_file_size_bytes\")\n",
    "            )\n",
    "            \n",
    "            print(\"\\nFile Statistics:\")\n",
    "            file_stats.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not access data files metadata: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No table history found - table may be empty\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error accessing metadata tables: {e}\")\n",
    "    print(\"This may be normal if the table has no data yet\")\n",
    "\n",
    "print(\"\\nMetadata Benefits:\")\n",
    "print(\"- Complete audit trail of all table changes\")\n",
    "print(\"- File-level statistics for query optimization\")\n",
    "print(\"- Time travel capabilities through snapshots\")\n",
    "print(\"- Schema evolution tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimization-tips-section",
   "metadata": {},
   "source": [
    "## Performance Optimization Tips\n",
    "\n",
    "Best practices for optimizing Spark queries on Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optimization-tips",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark-Iceberg Performance Optimization:\n",
      "==================================================\n",
      "\n",
      "1. Current Spark Configuration:\n",
      "   spark.sql.adaptive.enabled: true\n",
      "   spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "   spark.serializer: org.apache.spark.serializer.KryoSerializer\n",
      "   spark.sql.execution.arrow.pyspark.enabled: false\n",
      "\n",
      "2. Optimization Strategies:\n",
      "   ✓ Column Pruning: Select only needed columns\n",
      "   ✓ Predicate Pushdown: Apply filters early\n",
      "   ✓ Partition Pruning: Use partition columns in WHERE\n",
      "   ✓ Vectorized Execution: Enabled by default\n",
      "   ✓ Adaptive Query Execution: Dynamic optimization\n",
      "\n",
      "3. Optimization Examples:\n",
      "   Efficient Query (with optimizations):\n",
      "   SELECT username, email\n",
      "    FROM users\n",
      "    WHERE is_active = true\n",
      "      AND created_year = 2025\n",
      "    ORDER BY user_id\n",
      "    LIMIT 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|     username|               email|\n",
      "+-------------+--------------------+\n",
      "|     john_doe|john.doe@example.com|\n",
      "|   jane_smith|jane.smith@exampl...|\n",
      "|  bob_builder|bob.builder@examp...|\n",
      "|charlie_brown|charlie.brown@exa...|\n",
      "+-------------+--------------------+\n",
      "\n",
      "   Optimizations applied:\n",
      "   - Column projection (username, email only)\n",
      "   - Predicate pushdown (is_active, created_year)\n",
      "   - Result limiting (LIMIT 5)\n",
      "   - Partition pruning (created_year filter)\n",
      "\n",
      "4. Best Practices Summary:\n",
      "   1. Use partition columns in WHERE clauses\n",
      "   2. Select only required columns\n",
      "   3. Apply filters as early as possible\n",
      "   4. Use appropriate data types\n",
      "   5. Enable adaptive query execution\n",
      "   6. Monitor query plans with EXPLAIN\n",
      "   7. Cache frequently accessed data\n",
      "   8. Use columnar formats (Parquet)\n"
     ]
    }
   ],
   "source": [
    "# Performance optimization demonstrations\n",
    "print(\"Spark-Iceberg Performance Optimization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Current Spark Configuration:\")\n",
    "important_configs = [\n",
    "    'spark.sql.adaptive.enabled',\n",
    "    'spark.sql.adaptive.coalescePartitions.enabled',\n",
    "    'spark.serializer',\n",
    "    'spark.sql.execution.arrow.pyspark.enabled'\n",
    "]\n",
    "\n",
    "for config in important_configs:\n",
    "    try:\n",
    "        value = spark.conf.get(config)\n",
    "        print(f\"   {config}: {value}\")\n",
    "    except Exception:\n",
    "        print(f\"   {config}: Not configured\")\n",
    "\n",
    "print(\"\\n2. Optimization Strategies:\")\n",
    "print(\"   ✓ Column Pruning: Select only needed columns\")\n",
    "print(\"   ✓ Predicate Pushdown: Apply filters early\")\n",
    "print(\"   ✓ Partition Pruning: Use partition columns in WHERE\")\n",
    "print(\"   ✓ Vectorized Execution: Enabled by default\")\n",
    "print(\"   ✓ Adaptive Query Execution: Dynamic optimization\")\n",
    "\n",
    "if users_df.count() > 0:\n",
    "    print(\"\\n3. Optimization Examples:\")\n",
    "    \n",
    "    # Efficient query example\n",
    "    efficient_query = \"\"\"\n",
    "    SELECT username, email\n",
    "    FROM users\n",
    "    WHERE is_active = true\n",
    "      AND created_year = 2025\n",
    "    ORDER BY user_id\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"   Efficient Query (with optimizations):\")\n",
    "    print(f\"   {efficient_query.strip()}\")\n",
    "    \n",
    "    result = spark.sql(efficient_query)\n",
    "    result.show()\n",
    "    \n",
    "    print(\"   Optimizations applied:\")\n",
    "    print(\"   - Column projection (username, email only)\")\n",
    "    print(\"   - Predicate pushdown (is_active, created_year)\")\n",
    "    print(\"   - Result limiting (LIMIT 5)\")\n",
    "    print(\"   - Partition pruning (created_year filter)\")\n",
    "\n",
    "print(\"\\n4. Best Practices Summary:\")\n",
    "best_practices = [\n",
    "    \"Use partition columns in WHERE clauses\",\n",
    "    \"Select only required columns\",\n",
    "    \"Apply filters as early as possible\",\n",
    "    \"Use appropriate data types\",\n",
    "    \"Enable adaptive query execution\",\n",
    "    \"Monitor query plans with EXPLAIN\",\n",
    "    \"Cache frequently accessed data\",\n",
    "    \"Use columnar formats (Parquet)\"\n",
    "]\n",
    "\n",
    "for i, practice in enumerate(best_practices, 1):\n",
    "    print(f\"   {i}. {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive querying of Apache Iceberg tables using Apache Spark:\n",
    "\n",
    "### What We Accomplished:\n",
    "1. **Environment Setup**: Configured Spark with Iceberg extensions\n",
    "2. **Catalog Exploration**: Discovered namespaces and tables\n",
    "3. **Table Inspection**: Examined schema and metadata\n",
    "4. **SQL Querying**: Executed basic and advanced SQL operations\n",
    "5. **DataFrame API**: Demonstrated programmatic data operations\n",
    "6. **Advanced Analytics**: Complex analytical queries and aggregations\n",
    "7. **Performance Analysis**: Partition pruning and optimization\n",
    "8. **Metadata Exploration**: Iceberg's internal metadata tables\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "- **Native Integration**: Seamless Spark-Iceberg connectivity\n",
    "- **SQL Compatibility**: Standard SQL with Iceberg extensions\n",
    "- **Performance Optimization**: Partition pruning, predicate pushdown\n",
    "- **Metadata Access**: Complete table history and file information\n",
    "- **Analytical Capabilities**: Complex aggregations and window functions\n",
    "\n",
    "### Performance Benefits Observed:\n",
    "- **Partition Pruning**: Reduced data scanning through partition filters\n",
    "- **Column Projection**: Efficient columnar data access\n",
    "- **Vectorized Execution**: High-performance analytical processing\n",
    "- **Adaptive Optimization**: Dynamic query plan adjustments\n",
    "- **Metadata Utilization**: Fast query planning using table statistics\n",
    "\n",
    "### Use Cases for Spark-Iceberg:\n",
    "1. **Data Lake Analytics**: Large-scale analytical processing\n",
    "2. **ETL Pipelines**: Extract, transform, and load operations\n",
    "3. **Real-time Analytics**: Streaming data processing\n",
    "4. **Data Science**: Exploratory data analysis and ML feature engineering\n",
    "5. **Reporting**: Business intelligence and dashboard data preparation\n",
    "\n",
    "Apache Spark provides the most comprehensive and performant interface for working with Iceberg tables, making it the preferred choice for production data lake operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark-Iceberg querying demonstration completed!\n",
      "\n",
      "Key takeaways:\n",
      "- Spark provides native, high-performance Iceberg support\n",
      "- SQL and DataFrame APIs offer flexible query options\n",
      "- Advanced optimizations enable efficient large-scale analytics\n",
      "- Metadata tables provide deep insights into table internals\n",
      "\n",
      "Spark session remains active for further operations\n"
     ]
    }
   ],
   "source": [
    "# Session cleanup\n",
    "print(\"Spark-Iceberg querying demonstration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"- Spark provides native, high-performance Iceberg support\")\n",
    "print(\"- SQL and DataFrame APIs offer flexible query options\")\n",
    "print(\"- Advanced optimizations enable efficient large-scale analytics\")\n",
    "print(\"- Metadata tables provide deep insights into table internals\")\n",
    "print(\"\\nSpark session remains active for further operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
