{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Data Insertion with Polars and Apache Iceberg\n",
                "\n",
                "This notebook demonstrates how to insert data into an Apache Iceberg table using Polars DataFrame. We'll explore data preparation, schema alignment, and efficient data loading techniques.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you'll understand:\n",
                "- How to prepare data using Polars DataFrame\n",
                "- How to align DataFrame schema with Iceberg table schema\n",
                "- How to perform efficient data insertion into Iceberg tables\n",
                "- How to verify data insertion and query results\n",
                "- Best practices for data loading and type conversion\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- Completed notebook 1 (table creation)\n",
                "- Polars library installed\n",
                "- PyIceberg library available\n",
                "- Docker environment running\n",
                "\n",
                "## Why Polars with Iceberg?\n",
                "\n",
                "Polars is an excellent choice for Iceberg data operations because:\n",
                "- **Performance**: Fast, memory-efficient operations\n",
                "- **Arrow Integration**: Native Apache Arrow support\n",
                "- **Type Safety**: Strong typing with automatic conversions\n",
                "- **API Design**: Clean, intuitive DataFrame operations\n",
                "- **Lazy Evaluation**: Optimized query planning"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-section",
            "metadata": {},
            "source": [
                "## Environment Setup\n",
                "\n",
                "Import necessary libraries and establish connection to the Iceberg catalog."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Libraries imported successfully:\n",
                        "- Polars version: 1.31.0\n",
                        "- PyArrow version: 20.0.0\n",
                        "- PyIceberg: Available\n",
                        "\n",
                        "Ready for data insertion operations\n"
                    ]
                }
            ],
            "source": [
                "from pyiceberg.catalog import load_catalog\n",
                "from datetime import datetime, timezone\n",
                "import polars as pl\n",
                "import pyarrow as pa\n",
                "\n",
                "print(\"Libraries imported successfully:\")\n",
                "print(f\"- Polars version: {pl.__version__}\")\n",
                "print(f\"- PyArrow version: {pa.__version__}\")\n",
                "print(\"- PyIceberg: Available\")\n",
                "print(\"\\nReady for data insertion operations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "catalog-connection-section",
            "metadata": {},
            "source": [
                "## Catalog Connection and Table Loading\n",
                "\n",
                "Connect to the Iceberg catalog and load the table we created in notebook 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "catalog-connection",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Catalog connection established\n",
                        "Users table loaded successfully\n",
                        "\n",
                        "Table: ('play_iceberg', 'users')\n",
                        "Schema fields: 8\n",
                        "Partition fields: 3\n"
                    ]
                }
            ],
            "source": [
                "# Configure catalog connection\n",
                "catalog_config = {\n",
                "    \"uri\": \"http://localhost:8181\",\n",
                "    \"s3.endpoint\": \"http://localhost:9000\",\n",
                "    \"s3.access-key-id\": \"admin\",\n",
                "    \"s3.secret-access-key\": \"password\",\n",
                "    \"s3.path-style-access\": \"true\",\n",
                "}\n",
                "\n",
                "# Load catalog\n",
                "try:\n",
                "    catalog = load_catalog(\"rest\", **catalog_config)\n",
                "    print(\"Catalog connection established\")\n",
                "    \n",
                "    # Load the users table\n",
                "    users_table = catalog.load_table(\"play_iceberg.users\")\n",
                "    print(\"Users table loaded successfully\")\n",
                "    \n",
                "    # Display table information\n",
                "    print(f\"\\nTable: {users_table.name()}\")\n",
                "    print(f\"Schema fields: {len(users_table.schema().fields)}\")\n",
                "    print(f\"Partition fields: {len(users_table.spec().fields)}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error connecting to catalog or loading table: {e}\")\n",
                "    print(\"Please ensure notebook 1 has been completed and Docker services are running\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "schema-inspection-section",
            "metadata": {},
            "source": [
                "## Schema Inspection\n",
                "\n",
                "Before inserting data, let's examine the table schema to understand the expected data types and structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "schema-inspection",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Iceberg Table Schema:\n",
                        "==============================\n",
                        "table {\n",
                        "  1: user_id: required long\n",
                        "  2: username: required string\n",
                        "  3: email: required string\n",
                        "  4: is_active: required boolean\n",
                        "  5: created_year: required int\n",
                        "  6: created_month: required int\n",
                        "  7: created_day: required int\n",
                        "  8: updated_at: required timestamp\n",
                        "}\n",
                        "\n",
                        "Arrow Schema (for type conversion):\n",
                        "========================================\n",
                        "user_id: int64 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '1'\n",
                        "username: large_string not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '2'\n",
                        "email: large_string not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '3'\n",
                        "is_active: bool not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '4'\n",
                        "created_year: int32 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '5'\n",
                        "created_month: int32 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '6'\n",
                        "created_day: int32 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '7'\n",
                        "updated_at: timestamp[us] not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '8'\n",
                        "\n",
                        "Field Details:\n",
                        "====================\n",
                        "  1: user_id (long) - Required\n",
                        "  2: username (string) - Required\n",
                        "  3: email (string) - Required\n",
                        "  4: is_active (boolean) - Required\n",
                        "  5: created_year (int) - Required\n",
                        "  6: created_month (int) - Required\n",
                        "  7: created_day (int) - Required\n",
                        "  8: updated_at (timestamp) - Required\n",
                        "\n",
                        "This schema will guide our DataFrame creation\n"
                    ]
                }
            ],
            "source": [
                "# Examine table schema\n",
                "print(\"Iceberg Table Schema:\")\n",
                "print(\"=\" * 30)\n",
                "iceberg_schema = users_table.schema()\n",
                "print(iceberg_schema)\n",
                "\n",
                "# Get Arrow schema for data type alignment\n",
                "print(\"\\nArrow Schema (for type conversion):\")\n",
                "print(\"=\" * 40)\n",
                "arrow_schema = iceberg_schema.as_arrow()\n",
                "print(arrow_schema)\n",
                "\n",
                "# Display field information for reference\n",
                "print(\"\\nField Details:\")\n",
                "print(\"=\" * 20)\n",
                "for field in iceberg_schema.fields:\n",
                "    required = \"Required\" if field.required else \"Optional\"\n",
                "    print(f\"  {field.field_id}: {field.name} ({field.field_type}) - {required}\")\n",
                "\n",
                "print(\"\\nThis schema will guide our DataFrame creation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-preparation-section",
            "metadata": {},
            "source": [
                "## Data Preparation with Polars\n",
                "\n",
                "Create sample user data using Polars DataFrame. We'll demonstrate best practices for:\n",
                "- **Data Type Alignment**: Ensure types match Iceberg schema\n",
                "- **Partition Value Creation**: Generate partition key values\n",
                "- **Data Quality**: Include realistic sample data\n",
                "- **Timestamp Handling**: Proper datetime formatting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "data-preparation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating sample user data...\n",
                        "Using timestamp: 2025-07-01 14:13:24.144741+00:00\n",
                        "Prepared 5 user records\n",
                        "Data includes mix of active and inactive users\n"
                    ]
                }
            ],
            "source": [
                "# Create sample user data\n",
                "print(\"Creating sample user data...\")\n",
                "\n",
                "# Get current timestamp for audit fields\n",
                "current_time = datetime.now(timezone.utc)\n",
                "print(f\"Using timestamp: {current_time}\")\n",
                "\n",
                "# Sample user data\n",
                "sample_users = [\n",
                "    {\n",
                "        \"user_id\": 1,\n",
                "        \"username\": \"john_doe\",\n",
                "        \"email\": \"john.doe@example.com\",\n",
                "        \"is_active\": True\n",
                "    },\n",
                "    {\n",
                "        \"user_id\": 2,\n",
                "        \"username\": \"jane_smith\",\n",
                "        \"email\": \"jane.smith@example.com\",\n",
                "        \"is_active\": True\n",
                "    },\n",
                "    {\n",
                "        \"user_id\": 3,\n",
                "        \"username\": \"alice_wonder\",\n",
                "        \"email\": \"alice.wonder@example.com\",\n",
                "        \"is_active\": False\n",
                "    },\n",
                "    {\n",
                "        \"user_id\": 4,\n",
                "        \"username\": \"bob_builder\",\n",
                "        \"email\": \"bob.builder@example.com\",\n",
                "        \"is_active\": True\n",
                "    },\n",
                "    {\n",
                "        \"user_id\": 5,\n",
                "        \"username\": \"charlie_brown\",\n",
                "        \"email\": \"charlie.brown@example.com\",\n",
                "        \"is_active\": True\n",
                "    }\n",
                "]\n",
                "\n",
                "print(f\"Prepared {len(sample_users)} user records\")\n",
                "print(\"Data includes mix of active and inactive users\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dataframe-creation-section",
            "metadata": {},
            "source": [
                "## DataFrame Creation and Enhancement\n",
                "\n",
                "Create a Polars DataFrame and add the required partition and audit fields."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "dataframe-creation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating Polars DataFrame...\n",
                        "\n",
                        "DataFrame created with enhanced fields:\n",
                        "Shape: (5, 8)\n",
                        "Columns: ['user_id', 'username', 'email', 'is_active', 'created_year', 'created_month', 'created_day', 'updated_at']\n",
                        "\n",
                        "Sample Data Preview:\n",
                        "shape: (5, 8)\n",
                        "┌─────────┬────────────┬────────────┬───────────┬────────────┬────────────┬────────────┬───────────┐\n",
                        "│ user_id ┆ username   ┆ email      ┆ is_active ┆ created_ye ┆ created_mo ┆ created_da ┆ updated_a │\n",
                        "│ ---     ┆ ---        ┆ ---        ┆ ---       ┆ ar         ┆ nth        ┆ y          ┆ t         │\n",
                        "│ i64     ┆ str        ┆ str        ┆ bool      ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
                        "│         ┆            ┆            ┆           ┆ i32        ┆ i32        ┆ i32        ┆ datetime[ │\n",
                        "│         ┆            ┆            ┆           ┆            ┆            ┆            ┆ μs, UTC]  │\n",
                        "╞═════════╪════════════╪════════════╪═══════════╪════════════╪════════════╪════════════╪═══════════╡\n",
                        "│ 1       ┆ john_doe   ┆ john.doe@e ┆ true      ┆ 2025       ┆ 7          ┆ 1          ┆ 2025-07-0 │\n",
                        "│         ┆            ┆ xample.com ┆           ┆            ┆            ┆            ┆ 1 14:13:2 │\n",
                        "│         ┆            ┆            ┆           ┆            ┆            ┆            ┆ 4.144741  │\n",
                        "│         ┆            ┆            ┆           ┆            ┆            ┆            ┆ UTC       │\n",
                        "│ 2       ┆ jane_smith ┆ jane.smith ┆ true      ┆ 2025       ┆ 7          ┆ 1          ┆ 2025-07-0 │\n",
                        "│         ┆            ┆ @example.c ┆           ┆            ┆            ┆            ┆ 1 14:13:2 │\n",
                        "│         ┆            ┆ om         ┆           ┆            ┆            ┆            ┆ 4.144741  │\n",
                        "│         ┆            ┆            ┆           ┆            ┆            ┆            ┆ UTC       │\n",
                        "│ 3       ┆ alice_wond ┆ alice.wond ┆ false     ┆ 2025       ┆ 7          ┆ 1          ┆ 2025-07-0 │\n",
                        "│         ┆ er         ┆ er@example ┆           ┆            ┆            ┆            ┆ 1 14:13:2 │\n",
                        "│         ┆            ┆ .com       ┆           ┆            ┆            ┆            ┆ 4.144741  │\n",
                        "│         ┆            ┆            ┆           ┆            ┆            ┆            ┆ UTC       │\n",
                        "│ 4       ┆ bob_builde ┆ bob.builde ┆ true      ┆ 2025       ┆ 7          ┆ 1          ┆ 2025-07-0 │\n",
                        "│         ┆ r          ┆ r@example. ┆           ┆            ┆            ┆            ┆ 1 14:13:2 │\n",
                        "│         ┆            ┆ com        ┆           ┆            ┆            ┆            ┆ 4.144741  │\n",
                        "│         ┆            ┆            ┆           ┆            ┆            ┆            ┆ UTC       │\n",
                        "│ 5       ┆ charlie_br ┆ charlie.br ┆ true      ┆ 2025       ┆ 7          ┆ 1          ┆ 2025-07-0 │\n",
                        "│         ┆ own        ┆ own@exampl ┆           ┆            ┆            ┆            ┆ 1 14:13:2 │\n",
                        "│         ┆            ┆ e.com      ┆           ┆            ┆            ┆            ┆ 4.144741  │\n",
                        "│         ┆            ┆            ┆           ┆            ┆            ┆            ┆ UTC       │\n",
                        "└─────────┴────────────┴────────────┴───────────┴────────────┴────────────┴────────────┴───────────┘\n"
                    ]
                }
            ],
            "source": [
                "# Create Polars DataFrame\n",
                "print(\"Creating Polars DataFrame...\")\n",
                "\n",
                "# Base DataFrame from sample data\n",
                "df = pl.DataFrame(sample_users)\n",
                "\n",
                "# Add partition fields (date components)\n",
                "df = df.with_columns([\n",
                "    pl.lit(current_time.year).alias(\"created_year\").cast(pl.Int32),\n",
                "    pl.lit(current_time.month).alias(\"created_month\").cast(pl.Int32),\n",
                "    pl.lit(current_time.day).alias(\"created_day\").cast(pl.Int32),\n",
                "    pl.lit(current_time).alias(\"updated_at\")\n",
                "])\n",
                "\n",
                "# Ensure proper data types\n",
                "df = df.with_columns([\n",
                "    pl.col(\"user_id\").cast(pl.Int64),\n",
                "    pl.col(\"username\").cast(pl.Utf8),\n",
                "    pl.col(\"email\").cast(pl.Utf8),\n",
                "    pl.col(\"is_active\").cast(pl.Boolean)\n",
                "])\n",
                "\n",
                "print(\"\\nDataFrame created with enhanced fields:\")\n",
                "print(f\"Shape: {df.shape}\")\n",
                "print(f\"Columns: {df.columns}\")\n",
                "\n",
                "# Display the DataFrame\n",
                "print(\"\\nSample Data Preview:\")\n",
                "print(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "schema-alignment-section",
            "metadata": {},
            "source": [
                "## Schema Alignment and Type Conversion\n",
                "\n",
                "Convert the Polars DataFrame to Arrow format and align with the Iceberg table schema."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "schema-alignment",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Converting DataFrame to Arrow format...\n",
                        "\n",
                        "Original Arrow schema:\n",
                        "user_id: int64\n",
                        "username: large_string\n",
                        "email: large_string\n",
                        "is_active: bool\n",
                        "created_year: int32\n",
                        "created_month: int32\n",
                        "created_day: int32\n",
                        "updated_at: timestamp[us, tz=UTC]\n",
                        "\n",
                        "Target Iceberg Arrow schema:\n",
                        "user_id: int64 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '1'\n",
                        "username: large_string not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '2'\n",
                        "email: large_string not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '3'\n",
                        "is_active: bool not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '4'\n",
                        "created_year: int32 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '5'\n",
                        "created_month: int32 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '6'\n",
                        "created_day: int32 not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '7'\n",
                        "updated_at: timestamp[us] not null\n",
                        "  -- field metadata --\n",
                        "  PARQUET:field_id: '8'\n",
                        "\n",
                        "Schema alignment successful!\n",
                        "Final Arrow table shape: (5, 8)\n",
                        "\n",
                        "Schema verification:\n",
                        "  user_id: int64 ✓ int64\n",
                        "  username: large_string ✓ large_string\n",
                        "  email: large_string ✓ large_string\n",
                        "  is_active: bool ✓ bool\n",
                        "  created_year: int32 ✓ int32\n",
                        "  created_month: int32 ✓ int32\n",
                        "  created_day: int32 ✓ int32\n",
                        "  updated_at: timestamp[us, tz=UTC] → timestamp[us]\n"
                    ]
                }
            ],
            "source": [
                "# Convert to Arrow and align with Iceberg schema\n",
                "print(\"Converting DataFrame to Arrow format...\")\n",
                "\n",
                "# Convert Polars DataFrame to Arrow Table\n",
                "arrow_table = df.to_arrow()\n",
                "\n",
                "print(\"\\nOriginal Arrow schema:\")\n",
                "print(arrow_table.schema)\n",
                "\n",
                "# Cast to match Iceberg table schema\n",
                "print(\"\\nTarget Iceberg Arrow schema:\")\n",
                "target_schema = users_table.schema().as_arrow()\n",
                "print(target_schema)\n",
                "\n",
                "# Perform schema alignment\n",
                "try:\n",
                "    aligned_table = arrow_table.cast(target_schema)\n",
                "    print(\"\\nSchema alignment successful!\")\n",
                "    print(f\"Final Arrow table shape: {aligned_table.shape}\")\n",
                "    \n",
                "    # Verify schema match\n",
                "    print(\"\\nSchema verification:\")\n",
                "    for i, field in enumerate(target_schema):\n",
                "        original_type = arrow_table.schema.field(i).type\n",
                "        aligned_type = aligned_table.schema.field(i).type\n",
                "        match_status = \"✓\" if original_type == aligned_type else \"→\"\n",
                "        print(f\"  {field.name}: {original_type} {match_status} {aligned_type}\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Schema alignment failed: {e}\")\n",
                "    print(\"Check data types and field alignment\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-insertion-section",
            "metadata": {},
            "source": [
                "## Data Insertion\n",
                "\n",
                "Insert the prepared data into the Iceberg table. This operation is atomic and will create a new snapshot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "data-insertion",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Inserting data into Iceberg table...\n",
                        "\n",
                        "Insertion process:\n",
                        "1. Validate data against table schema\n",
                        "2. Write data files to object storage\n",
                        "3. Update manifest files\n",
                        "4. Create new table snapshot\n",
                        "5. Update catalog metadata\n",
                        "\n",
                        "Table state before insertion:\n",
                        "  No current snapshot (empty table)\n",
                        "\n",
                        "Data insertion completed successfully!\n",
                        "\n",
                        "Table state after insertion:\n",
                        "  New snapshot: 7988583305662433429\n",
                        "  Snapshot timestamp: 1751379211622\n",
                        "\n",
                        "Insertion summary:\n",
                        "  Records inserted: 5\n",
                        "  Table now contains data and is queryable\n"
                    ]
                }
            ],
            "source": [
                "# Insert data into Iceberg table\n",
                "print(\"Inserting data into Iceberg table...\")\n",
                "print(\"\\nInsertion process:\")\n",
                "print(\"1. Validate data against table schema\")\n",
                "print(\"2. Write data files to object storage\")\n",
                "print(\"3. Update manifest files\")\n",
                "print(\"4. Create new table snapshot\")\n",
                "print(\"5. Update catalog metadata\")\n",
                "\n",
                "try:\n",
                "    # Check table state before insertion\n",
                "    print(\"\\nTable state before insertion:\")\n",
                "    current_snapshot = users_table.current_snapshot()\n",
                "    if current_snapshot:\n",
                "        print(f\"  Current snapshot: {current_snapshot.snapshot_id}\")\n",
                "    else:\n",
                "        print(\"  No current snapshot (empty table)\")\n",
                "    \n",
                "    # Perform insertion\n",
                "    users_table.append(aligned_table)\n",
                "    print(\"\\nData insertion completed successfully!\")\n",
                "    \n",
                "    # Check table state after insertion\n",
                "    print(\"\\nTable state after insertion:\")\n",
                "    new_snapshot = users_table.current_snapshot()\n",
                "    if new_snapshot:\n",
                "        print(f\"  New snapshot: {new_snapshot.snapshot_id}\")\n",
                "        print(f\"  Snapshot timestamp: {new_snapshot.timestamp_ms}\")\n",
                "    \n",
                "    print(\"\\nInsertion summary:\")\n",
                "    print(f\"  Records inserted: {len(aligned_table)}\")\n",
                "    print(\"  Table now contains data and is queryable\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Data insertion failed: {e}\")\n",
                "    print(\"Check data format and table accessibility\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-verification-section",
            "metadata": {},
            "source": [
                "## Data Verification and Querying\n",
                "\n",
                "Verify the data was inserted correctly by querying the table and examining the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "data-verification",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Verifying data insertion...\n",
                        "\n",
                        "Query Results:\n",
                        "Total records: 5\n",
                        "Columns: ['user_id', 'username', 'email', 'is_active', 'created_year', 'created_month', 'created_day', 'updated_at']\n",
                        "\n",
                        "Data Summary:\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 5 entries, 0 to 4\n",
                        "Data columns (total 8 columns):\n",
                        " #   Column         Non-Null Count  Dtype         \n",
                        "---  ------         --------------  -----         \n",
                        " 0   user_id        5 non-null      int64         \n",
                        " 1   username       5 non-null      object        \n",
                        " 2   email          5 non-null      object        \n",
                        " 3   is_active      5 non-null      bool          \n",
                        " 4   created_year   5 non-null      int32         \n",
                        " 5   created_month  5 non-null      int32         \n",
                        " 6   created_day    5 non-null      int32         \n",
                        " 7   updated_at     5 non-null      datetime64[us]\n",
                        "dtypes: bool(1), datetime64[us](1), int32(3), int64(1), object(2)\n",
                        "memory usage: 357.0+ bytes\n",
                        "None\n",
                        "\n",
                        "Sample Records:\n",
                        "   user_id       username                      email  is_active  created_year  \\\n",
                        "0        1       john_doe       john.doe@example.com       True          2025   \n",
                        "1        2     jane_smith     jane.smith@example.com       True          2025   \n",
                        "2        3   alice_wonder   alice.wonder@example.com      False          2025   \n",
                        "3        4    bob_builder    bob.builder@example.com       True          2025   \n",
                        "4        5  charlie_brown  charlie.brown@example.com       True          2025   \n",
                        "\n",
                        "   created_month  created_day                 updated_at  \n",
                        "0              7            1 2025-07-01 14:13:24.144741  \n",
                        "1              7            1 2025-07-01 14:13:24.144741  \n",
                        "2              7            1 2025-07-01 14:13:24.144741  \n",
                        "3              7            1 2025-07-01 14:13:24.144741  \n",
                        "4              7            1 2025-07-01 14:13:24.144741  \n",
                        "\n",
                        "Data Quality Checks:\n",
                        "  Unique user IDs: 5\n",
                        "  Active users: 4\n",
                        "  Inactive users: 1\n",
                        "  Null values: 0\n",
                        "\n",
                        "Partitioning Verification:\n",
                        "  Unique partitions: 1\n",
                        "  Partition values: [[2025, 7, 1]]\n"
                    ]
                }
            ],
            "source": [
                "# Verify data insertion by querying the table\n",
                "print(\"Verifying data insertion...\")\n",
                "\n",
                "try:\n",
                "    # Scan all data from the table\n",
                "    scan_result = users_table.scan()\n",
                "    \n",
                "    # Convert to Pandas for easy viewing\n",
                "    result_df = scan_result.to_pandas()\n",
                "    \n",
                "    print(\"\\nQuery Results:\")\n",
                "    print(f\"Total records: {len(result_df)}\")\n",
                "    print(f\"Columns: {list(result_df.columns)}\")\n",
                "    \n",
                "    print(\"\\nData Summary:\")\n",
                "    print(result_df.info())\n",
                "    \n",
                "    print(\"\\nSample Records:\")\n",
                "    print(result_df.head())\n",
                "    \n",
                "    # Verify data quality\n",
                "    print(\"\\nData Quality Checks:\")\n",
                "    print(f\"  Unique user IDs: {result_df['user_id'].nunique()}\")\n",
                "    print(f\"  Active users: {result_df['is_active'].sum()}\")\n",
                "    print(f\"  Inactive users: {(~result_df['is_active']).sum()}\")\n",
                "    print(f\"  Null values: {result_df.isnull().sum().sum()}\")\n",
                "    \n",
                "    # Verify partitioning\n",
                "    partition_info = result_df[['created_year', 'created_month', 'created_day']].drop_duplicates()\n",
                "    print(\"\\nPartitioning Verification:\")\n",
                "    print(f\"  Unique partitions: {len(partition_info)}\")\n",
                "    print(f\"  Partition values: {partition_info.values.tolist()}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Data verification failed: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "performance-analysis-section",
            "metadata": {},
            "source": [
                "## Performance Analysis\n",
                "\n",
                "Analyze the insertion performance and examine the created data files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "performance-analysis",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Performance and Storage Analysis:\n",
                        "========================================\n",
                        "\n",
                        "Snapshot Information:\n",
                        "  Snapshot ID: 7988583305662433429\n",
                        "  Timestamp: 1751379211622\n",
                        "  Operation: append (data insertion)\n",
                        "  Manifest list: s3://warehouse/play_iceberg/users/metadata/snap-7988583305662433429-0-a0730b11-4f1d-4ae2-bb4e-c77bd509dc10.avro\n",
                        "\n",
                        "Performance Considerations:\n",
                        "  Small dataset (5 records) - suitable for testing\n",
                        "  For production: batch larger datasets for efficiency\n",
                        "\n",
                        "Best Practices Applied:\n",
                        "  ✓ Schema alignment before insertion\n",
                        "  ✓ Proper data type conversion\n",
                        "  ✓ Partition key population\n",
                        "  ✓ Atomic insertion operation\n",
                        "  ✓ Data verification after insertion\n"
                    ]
                }
            ],
            "source": [
                "# Analyze insertion performance and file structure\n",
                "print(\"Performance and Storage Analysis:\")\n",
                "print(\"=\" * 40)\n",
                "\n",
                "# Get table metadata\n",
                "current_snapshot = users_table.current_snapshot()\n",
                "if current_snapshot:\n",
                "    print(\"\\nSnapshot Information:\")\n",
                "    print(f\"  Snapshot ID: {current_snapshot.snapshot_id}\")\n",
                "    print(f\"  Timestamp: {current_snapshot.timestamp_ms}\")\n",
                "    print(\"  Operation: append (data insertion)\")\n",
                "    \n",
                "    # File information\n",
                "    if hasattr(current_snapshot, 'manifest_list'):\n",
                "        print(f\"  Manifest list: {current_snapshot.manifest_list}\")\n",
                "\n",
                "# Performance considerations\n",
                "print(\"\\nPerformance Considerations:\")\n",
                "record_count = len(result_df)\n",
                "if record_count < 1000:\n",
                "    print(f\"  Small dataset ({record_count} records) - suitable for testing\")\n",
                "    print(\"  For production: batch larger datasets for efficiency\")\n",
                "elif record_count < 100000:\n",
                "    print(f\"  Medium dataset ({record_count} records) - good batch size\")\n",
                "else:\n",
                "    print(f\"  Large dataset ({record_count} records) - consider partitioning\")\n",
                "\n",
                "print(\"\\nBest Practices Applied:\")\n",
                "print(\"  ✓ Schema alignment before insertion\")\n",
                "print(\"  ✓ Proper data type conversion\")\n",
                "print(\"  ✓ Partition key population\")\n",
                "print(\"  ✓ Atomic insertion operation\")\n",
                "print(\"  ✓ Data verification after insertion\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "querying-examples-section",
            "metadata": {},
            "source": [
                "## Advanced Querying Examples\n",
                "\n",
                "Demonstrate various querying capabilities with the inserted data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "querying-examples",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Advanced Querying Examples:\n",
                        "===================================\n",
                        "\n",
                        "1. Active Users Only:\n",
                        "   Found 4 active users\n",
                        "   Usernames: ['john_doe', 'jane_smith', 'bob_builder', 'charlie_brown']\n",
                        "\n",
                        "2. User Contact Information:\n",
                        "   Retrieved 3 columns for 5 users\n",
                        "       username                     email  is_active\n",
                        "0      john_doe      john.doe@example.com       True\n",
                        "1    jane_smith    jane.smith@example.com       True\n",
                        "2  alice_wonder  alice.wonder@example.com      False\n",
                        "\n",
                        "3. Active User Emails:\n",
                        "   Retrieved emails for 4 active users\n",
                        "   - john_doe: john.doe@example.com\n",
                        "   - jane_smith: jane.smith@example.com\n",
                        "   - bob_builder: bob.builder@example.com\n",
                        "   - charlie_brown: charlie.brown@example.com\n",
                        "\n",
                        "Querying Benefits:\n",
                        "  - Row filtering reduces data transfer\n",
                        "  - Column projection minimizes memory usage\n",
                        "  - Partition pruning improves performance\n",
                        "  - Predicate pushdown optimizes storage scans\n"
                    ]
                }
            ],
            "source": [
                "# Advanced querying examples\n",
                "print(\"Advanced Querying Examples:\")\n",
                "print(\"=\" * 35)\n",
                "\n",
                "# Example 1: Filter by active status\n",
                "print(\"\\n1. Active Users Only:\")\n",
                "active_users = users_table.scan(\n",
                "    row_filter=\"is_active == true\"\n",
                ").to_pandas()\n",
                "print(f\"   Found {len(active_users)} active users\")\n",
                "print(f\"   Usernames: {active_users['username'].tolist()}\")\n",
                "\n",
                "# Example 2: Column projection\n",
                "print(\"\\n2. User Contact Information:\")\n",
                "contact_info = users_table.scan(\n",
                "    selected_fields=[\"username\", \"email\", \"is_active\"]\n",
                ").to_pandas()\n",
                "print(f\"   Retrieved {len(contact_info.columns)} columns for {len(contact_info)} users\")\n",
                "print(contact_info.head(3))\n",
                "\n",
                "# Example 3: Combined filter and projection\n",
                "print(\"\\n3. Active User Emails:\")\n",
                "active_emails = users_table.scan(\n",
                "    selected_fields=[\"username\", \"email\"],\n",
                "    row_filter=\"is_active == true\"\n",
                ").to_pandas()\n",
                "print(f\"   Retrieved emails for {len(active_emails)} active users\")\n",
                "for _, user in active_emails.iterrows():\n",
                "    print(f\"   - {user['username']}: {user['email']}\")\n",
                "\n",
                "print(\"\\nQuerying Benefits:\")\n",
                "print(\"  - Row filtering reduces data transfer\")\n",
                "print(\"  - Column projection minimizes memory usage\")\n",
                "print(\"  - Partition pruning improves performance\")\n",
                "print(\"  - Predicate pushdown optimizes storage scans\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "next-steps-section",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "The data has been successfully inserted and verified. Here are recommended next steps:\n",
                "\n",
                "### Immediate Actions:\n",
                "1. **Explore Querying**: Try different filter and projection combinations\n",
                "2. **Data Updates**: Learn how to modify existing records\n",
                "3. **Batch Operations**: Insert larger datasets efficiently\n",
                "4. **Schema Evolution**: Add new columns to the table\n",
                "\n",
                "### Production Considerations:\n",
                "1. **Error Handling**: Implement robust error handling for data operations\n",
                "2. **Data Validation**: Add validation rules before insertion\n",
                "3. **Monitoring**: Track insertion performance and data quality\n",
                "4. **Optimization**: Tune batch sizes and partition strategies\n",
                "\n",
                "### Advanced Features:\n",
                "- **Upsert Operations**: Merge new data with existing records\n",
                "- **Time Travel**: Query historical versions of data\n",
                "- **Concurrent Writes**: Handle multiple writers safely\n",
                "- **Data Compaction**: Optimize storage layout over time"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary-section",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated the complete process of inserting data into Apache Iceberg using Polars:\n",
                "\n",
                "### What We Accomplished:\n",
                "1. **Data Preparation**: Created structured user data with Polars\n",
                "2. **Schema Alignment**: Matched DataFrame types with Iceberg schema\n",
                "3. **Type Conversion**: Properly converted Polars to Arrow format\n",
                "4. **Data Insertion**: Successfully loaded data into Iceberg table\n",
                "5. **Verification**: Confirmed data integrity and queryability\n",
                "6. **Performance Analysis**: Evaluated insertion efficiency\n",
                "\n",
                "### Key Concepts Learned:\n",
                "- **Schema Compatibility**: Importance of type alignment\n",
                "- **Arrow Integration**: How Polars works with Iceberg via Arrow\n",
                "- **Partition Handling**: Automatic partition creation during insertion\n",
                "- **Atomic Operations**: Insertion creates consistent snapshots\n",
                "- **Query Optimization**: How to efficiently read inserted data\n",
                "\n",
                "### Best Practices Applied:\n",
                "- **Type Safety**: Explicit type casting for data integrity\n",
                "- **Schema Validation**: Verified compatibility before insertion\n",
                "- **Data Quality**: Included realistic test data with edge cases\n",
                "- **Error Handling**: Proper exception handling for robust operations\n",
                "- **Verification**: Confirmed successful operations through querying\n",
                "\n",
                "The table now contains sample data and is ready for further operations including updates, deletes, and advanced querying scenarios."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
