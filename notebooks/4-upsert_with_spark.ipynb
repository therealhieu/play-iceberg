{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Future Steps\n\n### Immediate Actions\n- Implement automated upsert scheduling for regular data synchronization\n- Add comprehensive logging and monitoring for production deployments\n- Create performance benchmarks for different data volumes\n\n### Production Enhancements\n- Implement retry logic for failed merge operations\n- Add data quality monitoring and alerting\n- Optimize partition strategy based on merge patterns\n- Set up automated testing for merge operation validation\n\n### Advanced Features\n- Implement Change Data Capture (CDC) integration with merge operations\n- Add support for streaming upserts using Structured Streaming\n- Create custom merge strategies for domain-specific use cases\n- Integrate with data lineage tracking systems",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced Upsert Operations with Apache Iceberg and Spark\n\n## Objectives\n- Master MERGE INTO capabilities for efficient data synchronization\n- Implement conditional merge logic with complex business rules\n- Execute bulk upsert operations with performance optimization\n- Handle DELETE operations within MERGE statements for data cleanup",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, BooleanType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:45:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n",
      "Default Catalog: rest\n",
      "Adaptive Query Execution: true\n",
      "\n",
      "Catalog context set to: rest.play_iceberg\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session with optimized configuration for merge operations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced Iceberg Upsert Tutorial\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Default Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Adaptive Query Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Set catalog context\n",
    "spark.sql(\"USE rest.`play_iceberg`\")\n",
    "print(\"\\nCatalog context set to: rest.play_iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table state after basic merge:\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|user_id|username           |email                          |is_active|created_year|created_month|created_day|updated_at                |\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|1      |john_doe_updated   |john.doe.updated@example.com   |false    |2025        |7            |1          |2025-07-01 13:46:10.39113 |\n",
      "|2      |jane_smith         |jane.smith@example.com         |true     |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "|3      |alice_wonder_active|alice.wonder.active@example.com|true     |2025        |7            |1          |2025-07-01 13:46:10.39113 |\n",
      "|4      |bob_builder        |bob.builder@example.com        |true     |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "|5      |charlie_brown      |charlie.brown@example.com      |true     |2025        |7            |1          |2025-07-01 13:41:16.173663|\n",
      "|6      |marketing_user     |marketing@company.com          |true     |2025        |7            |1          |2025-07-01 13:46:10.39113 |\n",
      "|7      |support_user       |support@company.com            |true     |2025        |7            |1          |2025-07-01 13:46:10.39113 |\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "\n",
      "\n",
      "Post-merge statistics:\n",
      "+-------------+------------+--------------+\n",
      "|total_records|active_users|inactive_users|\n",
      "+-------------+------------+--------------+\n",
      "|            7|           6|             1|\n",
      "+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the merge results\n",
    "print(\"Table state after basic merge:\")\n",
    "result_df = spark.sql(\"SELECT * FROM users ORDER BY user_id\")\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Show statistics after merge\n",
    "print(\"\\nPost-merge statistics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(CASE WHEN is_active = true THEN 1 END) as active_users,\n",
    "        COUNT(CASE WHEN is_active = false THEN 1 END) as inactive_users\n",
    "    FROM users\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conditional Merge Operations\n",
    "\n",
    "Now let's explore more sophisticated merge patterns with conditional logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bulk Upsert Performance Optimization\n",
    "\n",
    "Let's demonstrate handling larger datasets and optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1007 records for bulk upsert\n",
      "- New users: 1000\n",
      "- Updated existing users: 7\n"
     ]
    }
   ],
   "source": [
    "# Generate bulk data for performance testing\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_bulk_users(start_id, count):\n",
    "    \"\"\"Generate bulk user data for testing.\"\"\"\n",
    "    bulk_data = []\n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    for i in range(count):\n",
    "        user_id = start_id + i\n",
    "        bulk_data.append({\n",
    "            'user_id': user_id,\n",
    "            'username': f'bulk_user_{user_id}',\n",
    "            'email': f'bulk.user.{user_id}@example.com',\n",
    "            'is_active': random.choice([True, False]),\n",
    "            'created_year': 2025,\n",
    "            'created_month': 6,\n",
    "            'created_day': 27,\n",
    "            'updated_at': base_time + timedelta(seconds=i)\n",
    "        })\n",
    "    \n",
    "    return bulk_data\n",
    "\n",
    "# Generate 1000 new users and 100 updates to existing users\n",
    "new_users = generate_bulk_users(100, 1000)\n",
    "existing_updates = []\n",
    "\n",
    "# Update some existing users (1-8)\n",
    "for user_id in range(1, 8):\n",
    "    existing_updates.append({\n",
    "        'user_id': user_id,\n",
    "        'username': f'updated_user_{user_id}',\n",
    "        'email': f'updated.user.{user_id}@example.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    })\n",
    "\n",
    "# Combine all data\n",
    "bulk_upsert_data = new_users + existing_updates\n",
    "print(f\"Generated {len(bulk_upsert_data)} records for bulk upsert\")\n",
    "print(f\"- New users: {len(new_users)}\")\n",
    "print(f\"- Updated existing users: {len(existing_updates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Delete Operations with MERGE\n",
    "\n",
    "Iceberg also supports DELETE operations within MERGE statements for data cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup actions to be performed:\n",
      "+-------+----------+\n",
      "|user_id|    action|\n",
      "+-------+----------+\n",
      "|    105|    delete|\n",
      "|    110|    delete|\n",
      "|    115|deactivate|\n",
      "|    120|deactivate|\n",
      "+-------+----------+\n",
      "\n",
      "\n",
      "Current state of affected users:\n",
      "+-------+-------------+--------------------+---------+\n",
      "|user_id|     username|               email|is_active|\n",
      "+-------+-------------+--------------------+---------+\n",
      "|    105|bulk_user_105|bulk.user.105@exa...|     true|\n",
      "|    110|bulk_user_110|bulk.user.110@exa...|     true|\n",
      "|    115|bulk_user_115|bulk.user.115@exa...|    false|\n",
      "|    120|bulk_user_120|bulk.user.120@exa...|    false|\n",
      "+-------+-------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a cleanup dataset - users to be deleted or deactivated\n",
    "cleanup_data = [\n",
    "    {'user_id': 105, 'action': 'delete'},\n",
    "    {'user_id': 110, 'action': 'delete'},\n",
    "    {'user_id': 115, 'action': 'deactivate'},\n",
    "    {'user_id': 120, 'action': 'deactivate'}\n",
    "]\n",
    "\n",
    "cleanup_schema = StructType([\n",
    "    StructField(\"user_id\", LongType(), False),\n",
    "    StructField(\"action\", StringType(), False)\n",
    "])\n",
    "\n",
    "cleanup_df = spark.createDataFrame(cleanup_data, cleanup_schema)\n",
    "cleanup_df.createOrReplaceTempView(\"cleanup_actions\")\n",
    "\n",
    "print(\"Cleanup actions to be performed:\")\n",
    "cleanup_df.show()\n",
    "\n",
    "# Check current state of these users\n",
    "print(\"\\nCurrent state of affected users:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, username, email, is_active \n",
    "    FROM users \n",
    "    WHERE user_id IN (105, 110, 115, 120)\n",
    "    ORDER BY user_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup merge operation completed\n"
     ]
    }
   ],
   "source": [
    "# Execute conditional delete/update merge\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO users AS target\n",
    "    USING cleanup_actions AS source\n",
    "    ON target.user_id = source.user_id\n",
    "    WHEN MATCHED AND source.action = 'delete' THEN\n",
    "        DELETE\n",
    "    WHEN MATCHED AND source.action = 'deactivate' THEN\n",
    "        UPDATE SET\n",
    "            target.is_active = false,\n",
    "            target.updated_at = current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "print(\"Cleanup merge operation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Merge Patterns\n",
    "\n",
    "Explore complex scenarios and edge cases in merge operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid records: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid records filtered out: 2\n",
      "\n",
      "Valid records to be merged:\n",
      "+-------+----------+-----------------+---------+------------+-------------+-----------+--------------------+\n",
      "|user_id|  username|            email|is_active|created_year|created_month|created_day|          updated_at|\n",
      "+-------+----------+-----------------+---------+------------+-------------+-----------+--------------------+\n",
      "|   1001|valid_user|valid@example.com|     true|        2025|            6|         27|2025-07-01 13:46:...|\n",
      "+-------+----------+-----------------+---------+------------+-------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate merge with data validation\n",
    "# Create source data with some invalid records\n",
    "validation_data = [\n",
    "    {\n",
    "        'user_id': 1001,\n",
    "        'username': 'valid_user',\n",
    "        'email': 'valid@example.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    },\n",
    "    {\n",
    "        'user_id': 1002,\n",
    "        'username': '',  # Invalid: empty username\n",
    "        'email': 'invalid@example.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    },\n",
    "    {\n",
    "        'user_id': 1003,\n",
    "        'username': 'another_valid_user',\n",
    "        'email': 'not-an-email',  # Invalid: bad email format\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    }\n",
    "]\n",
    "\n",
    "validation_df = spark.createDataFrame(validation_data, spark.table(\"users\").schema)\n",
    "\n",
    "# Add validation logic\n",
    "validated_df = validation_df.filter(\n",
    "    (col(\"username\") != \"\") & \n",
    "    (col(\"username\").isNotNull()) &\n",
    "    (col(\"email\").contains(\"@\")) &\n",
    "    (col(\"email\").contains(\".\"))\n",
    ")\n",
    "\n",
    "validated_df.createOrReplaceTempView(\"validated_updates\")\n",
    "\n",
    "print(f\"Original records: {validation_df.count()}\")\n",
    "print(f\"Valid records: {validated_df.count()}\")\n",
    "print(f\"Invalid records filtered out: {validation_df.count() - validated_df.count()}\")\n",
    "\n",
    "print(\"\\nValid records to be merged:\")\n",
    "validated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated merge completed\n",
      "+-------+--------------+--------------------+\n",
      "|user_id|      username|               email|\n",
      "+-------+--------------+--------------------+\n",
      "|   1001|bulk_user_1001|bulk.user.1001@ex...|\n",
      "|   1002|bulk_user_1002|bulk.user.1002@ex...|\n",
      "|   1003|bulk_user_1003|bulk.user.1003@ex...|\n",
      "+-------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute merge with validated data\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO users AS target\n",
    "    USING validated_updates AS source\n",
    "    ON target.user_id = source.user_id\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            user_id, username, email, is_active,\n",
    "            created_year, created_month, created_day, updated_at\n",
    "        )\n",
    "        VALUES (\n",
    "            source.user_id, source.username, source.email, source.is_active,\n",
    "            source.created_year, source.created_month, source.created_day, source.updated_at\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Validated merge completed\")\n",
    "\n",
    "# Verify only valid record was inserted\n",
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, username, email \n",
    "    FROM users \n",
    "    WHERE user_id IN (1001, 1002, 1003)\n",
    "    ORDER BY user_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Cleanup\n",
    "\n",
    "Let's summarize what we've learned and clean up our session."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}