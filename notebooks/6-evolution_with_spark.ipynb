{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema and Partition Evolution with Apache Iceberg\n",
    "\n",
    "This comprehensive tutorial explores Apache Iceberg's powerful evolution capabilities - schema evolution and partition evolution. These features allow you to adapt your data lake architecture over time without costly data rewrites or downtime.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master schema evolution techniques (add, rename, drop, reorder columns)\n",
    "- Understand partition evolution and optimization strategies\n",
    "- Learn to handle data type changes and compatibility\n",
    "- Explore backward and forward compatibility patterns\n",
    "- Practice evolution monitoring and best practices\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of notebooks 1-5\n",
    "- Understanding of data modeling concepts\n",
    "- Basic knowledge of partitioning strategies\n",
    "\n",
    "## Why Evolution Matters\n",
    "- **Business Requirements Change**: New fields, different analytics needs\n",
    "- **Performance Optimization**: Better partitioning for query patterns\n",
    "- **Cost Efficiency**: Avoid expensive data rewrites\n",
    "- **Zero Downtime**: Keep systems running during changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Current State Analysis\n",
    "\n",
    "Let's begin by establishing our environment and understanding the current table structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    BooleanType, TimestampType, LongType, DoubleType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, lit, count, \n",
    "    max as spark_max, min as spark_min, \n",
    "    coalesce, round as spark_round, avg\n",
    ")\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Configuration constants\n",
    "CATALOG_NAME = \"rest.`play_iceberg`\"\n",
    "TABLE_NAME = \"users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:48:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n",
      "Default Catalog: rest\n",
      "\n",
      "Working with catalog: rest.`play_iceberg`\n"
     ]
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Create Spark session with evolution-friendly settings.\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Iceberg Schema and Partition Evolution\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def print_session_info(spark):\n",
    "    \"\"\"Print session configuration information.\"\"\"\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Default Catalog: {spark.conf.get('spark.sql.defaultCatalog', 'Not set')}\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = create_spark_session()\n",
    "print_session_info(spark)\n",
    "\n",
    "# Set working catalog\n",
    "spark.sql(f\"USE {CATALOG_NAME}\")\n",
    "print(f\"\\nWorking with catalog: {CATALOG_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CURRENT TABLE ANALYSIS ===\n",
      "\n",
      "Current Schema and Partitioning:\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                                             |comment|\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|user_id                     |bigint                                                                                                                |NULL   |\n",
      "|username                    |string                                                                                                                |NULL   |\n",
      "|email                       |string                                                                                                                |NULL   |\n",
      "|is_active                   |boolean                                                                                                               |NULL   |\n",
      "|created_year                |int                                                                                                                   |NULL   |\n",
      "|created_month               |int                                                                                                                   |NULL   |\n",
      "|created_day                 |int                                                                                                                   |NULL   |\n",
      "|updated_at                  |timestamp_ntz                                                                                                         |NULL   |\n",
      "|# Partition Information     |                                                                                                                      |       |\n",
      "|# col_name                  |data_type                                                                                                             |comment|\n",
      "|created_year                |int                                                                                                                   |NULL   |\n",
      "|created_month               |int                                                                                                                   |NULL   |\n",
      "|created_day                 |int                                                                                                                   |NULL   |\n",
      "|                            |                                                                                                                      |       |\n",
      "|# Metadata Columns          |                                                                                                                      |       |\n",
      "|_spec_id                    |int                                                                                                                   |       |\n",
      "|_partition                  |struct<created_year:int,created_month:int,created_day:int>                                                            |       |\n",
      "|_file                       |string                                                                                                                |       |\n",
      "|_pos                        |bigint                                                                                                                |       |\n",
      "|_deleted                    |boolean                                                                                                               |       |\n",
      "|                            |                                                                                                                      |       |\n",
      "|# Detailed Table Information|                                                                                                                      |       |\n",
      "|Name                        |rest.play_iceberg.users                                                                                               |       |\n",
      "|Type                        |MANAGED                                                                                                               |       |\n",
      "|Location                    |s3://warehouse/play_iceberg/users                                                                                     |       |\n",
      "|Provider                    |iceberg                                                                                                               |       |\n",
      "|Table Properties            |[current-snapshot-id=8866811446601788626,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd]|       |\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n",
      "\n",
      "Current Data Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+---------------+-------------+------------+--------------+\n",
      "|total_records|distinct_years|distinct_months|distinct_days|active_users|inactive_users|\n",
      "+-------------+--------------+---------------+-------------+------------+--------------+\n",
      "|         1006|             1|              2|            2|         506|           500|\n",
      "+-------------+--------------+---------------+-------------+------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def analyze_table_structure(table_name):\n",
    "    \"\"\"Analyze and display current table structure and statistics.\"\"\"\n",
    "    print(\"=== CURRENT TABLE ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Get detailed table information\n",
    "    table_desc = spark.sql(f\"DESCRIBE EXTENDED {table_name}\")\n",
    "    print(\"Current Schema and Partitioning:\")\n",
    "    table_desc.show(50, truncate=False)\n",
    "    \n",
    "    # Get current data statistics\n",
    "    current_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT created_year) as distinct_years,\n",
    "            COUNT(DISTINCT created_month) as distinct_months,\n",
    "            COUNT(DISTINCT created_day) as distinct_days,\n",
    "            COUNT(CASE WHEN is_active = true THEN 1 END) as active_users,\n",
    "            COUNT(CASE WHEN is_active = false THEN 1 END) as inactive_users\n",
    "        FROM {table_name}\n",
    "    \"\"\")\n",
    "    print(\"\\nCurrent Data Statistics:\")\n",
    "    current_stats.show()\n",
    "\n",
    "# Analyze current table\n",
    "analyze_table_structure(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Partition Distribution:\n",
      "+------------+-------------+-----------+------------+------------+--------------+\n",
      "|created_year|created_month|created_day|record_count|active_count|inactive_count|\n",
      "+------------+-------------+-----------+------------+------------+--------------+\n",
      "|        2025|            6|         27|         999|         499|           500|\n",
      "|        2025|            7|          1|           7|           7|             0|\n",
      "+------------+-------------+-----------+------------+------------+--------------+\n",
      "\n",
      "\n",
      "Current Sample Data:\n",
      "+-------+----------------+--------------------+---------+\n",
      "|user_id|        username|               email|is_active|\n",
      "+-------+----------------+--------------------+---------+\n",
      "|      1|  updated_user_1|updated.user.1@ex...|     true|\n",
      "|      2|  updated_user_2|updated.user.2@ex...|     true|\n",
      "|      3|  updated_user_3|updated.user.3@ex...|     true|\n",
      "|      4|  updated_user_4|updated.user.4@ex...|     true|\n",
      "|      5|  updated_user_5|updated.user.5@ex...|     true|\n",
      "|      6|  updated_user_6|updated.user.6@ex...|     true|\n",
      "|      7|  updated_user_7|updated.user.7@ex...|     true|\n",
      "|      8|new_premium_user| premium@company.com|     true|\n",
      "|    100|   bulk_user_100|bulk.user.100@exa...|     true|\n",
      "|    101|   bulk_user_101|bulk.user.101@exa...|     true|\n",
      "+-------+----------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_partition_distribution(table_name):\n",
    "    \"\"\"Display current partition distribution and sample data.\"\"\"\n",
    "    print(\"Current Partition Distribution:\")\n",
    "    partition_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            created_year,\n",
    "            created_month,\n",
    "            created_day,\n",
    "            COUNT(*) as record_count,\n",
    "            COUNT(CASE WHEN is_active = true THEN 1 END) as active_count,\n",
    "            COUNT(CASE WHEN is_active = false THEN 1 END) as inactive_count\n",
    "        FROM {table_name} \n",
    "        GROUP BY created_year, created_month, created_day\n",
    "        ORDER BY created_year, created_month, created_day\n",
    "    \"\"\")\n",
    "    partition_stats.show()\n",
    "\n",
    "    print(\"\\nCurrent Sample Data:\")\n",
    "    sample_data = spark.sql(f\"\"\"\n",
    "        SELECT user_id, username, email, is_active \n",
    "        FROM {table_name} \n",
    "        ORDER BY user_id \n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    sample_data.show()\n",
    "\n",
    "# Show partition distribution\n",
    "show_partition_distribution(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Evolution - Adding Columns\n",
    "\n",
    "Let's start with the most common schema evolution: adding new columns. This is a metadata-only operation that doesn't require rewriting existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCHEMA EVOLUTION: ADDING COLUMNS ===\n",
      "\n",
      "1. Adding 'country' column...\n",
      "   ✓ country column added\n",
      "\n",
      "2. Adding 'registration_source' column...\n",
      "   ✓ registration_source column added\n",
      "\n",
      "3. Adding 'user_score' column...\n",
      "   ✓ user_score column added\n",
      "\n",
      "4. Adding 'last_login_at' column...\n",
      "   ✓ last_login_at column added\n"
     ]
    }
   ],
   "source": [
    "def add_schema_columns(table_name):\n",
    "    \"\"\"Add multiple new columns to demonstrate schema evolution.\"\"\"\n",
    "    print(\"=== SCHEMA EVOLUTION: ADDING COLUMNS ===\\n\")\n",
    "    \n",
    "    columns_to_add = [\n",
    "        (\"country\", \"STRING\", \"Geographic location\"),\n",
    "        (\"registration_source\", \"STRING\", \"Source of user registration (web, mobile, api)\"),\n",
    "        (\"user_score\", \"DOUBLE\", \"User engagement score\"),\n",
    "        (\"last_login_at\", \"TIMESTAMP\", \"Last login timestamp\")\n",
    "    ]\n",
    "    \n",
    "    for i, (col_name, col_type, description) in enumerate(columns_to_add, 1):\n",
    "        print(f\"{i}. Adding '{col_name}' column...\")\n",
    "        if description and \"Source of\" in description:\n",
    "            spark.sql(f\"ALTER TABLE {table_name} ADD COLUMN {col_name} {col_type} COMMENT '{description}'\")\n",
    "        else:\n",
    "            spark.sql(f\"ALTER TABLE {table_name} ADD COLUMN {col_name} {col_type}\")\n",
    "        print(f\"   ✓ {col_name} column added\")\n",
    "        if i < len(columns_to_add):\n",
    "            print()\n",
    "\n",
    "# Add new columns\n",
    "add_schema_columns(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Schema:\n",
      "+-----------------------+-------------+----------------------------------------------+\n",
      "|col_name               |data_type    |comment                                       |\n",
      "+-----------------------+-------------+----------------------------------------------+\n",
      "|user_id                |bigint       |NULL                                          |\n",
      "|username               |string       |NULL                                          |\n",
      "|email                  |string       |NULL                                          |\n",
      "|is_active              |boolean      |NULL                                          |\n",
      "|created_year           |int          |NULL                                          |\n",
      "|created_month          |int          |NULL                                          |\n",
      "|created_day            |int          |NULL                                          |\n",
      "|updated_at             |timestamp_ntz|NULL                                          |\n",
      "|country                |string       |NULL                                          |\n",
      "|registration_source    |string       |Source of user registration (web, mobile, api)|\n",
      "|user_score             |double       |NULL                                          |\n",
      "|last_login_at          |timestamp    |NULL                                          |\n",
      "|# Partition Information|             |                                              |\n",
      "|# col_name             |data_type    |comment                                       |\n",
      "|created_year           |int          |NULL                                          |\n",
      "|created_month          |int          |NULL                                          |\n",
      "|created_day            |int          |NULL                                          |\n",
      "+-----------------------+-------------+----------------------------------------------+\n",
      "\n",
      "\n",
      "Existing Data (new columns show as NULL):\n",
      "+-------+--------------+---------+-------+-------------------+----------+-------------+\n",
      "|user_id|username      |is_active|country|registration_source|user_score|last_login_at|\n",
      "+-------+--------------+---------+-------+-------------------+----------+-------------+\n",
      "|1      |updated_user_1|true     |NULL   |NULL               |NULL      |NULL         |\n",
      "|2      |updated_user_2|true     |NULL   |NULL               |NULL      |NULL         |\n",
      "|3      |updated_user_3|true     |NULL   |NULL               |NULL      |NULL         |\n",
      "|4      |updated_user_4|true     |NULL   |NULL               |NULL      |NULL         |\n",
      "|5      |updated_user_5|true     |NULL   |NULL               |NULL      |NULL         |\n",
      "+-------+--------------+---------+-------+-------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def verify_schema_evolution(table_name):\n",
    "    \"\"\"Verify the schema evolution and show updated structure.\"\"\"\n",
    "    print(\"Updated Schema:\")\n",
    "    spark.sql(f\"DESCRIBE TABLE {table_name}\").show(20, truncate=False)\n",
    "\n",
    "    print(\"\\nExisting Data (new columns show as NULL):\")\n",
    "    evolved_data = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id, username, is_active,\n",
    "            country, registration_source, user_score, last_login_at\n",
    "        FROM {table_name} \n",
    "        ORDER BY user_id \n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    evolved_data.show(truncate=False)\n",
    "\n",
    "# Verify schema changes\n",
    "verify_schema_evolution(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting new records with evolved schema:\n",
      "✓ New users inserted successfully\n"
     ]
    }
   ],
   "source": [
    "def insert_evolved_data(table_name):\n",
    "    \"\"\"Insert new data using the evolved schema.\"\"\"\n",
    "    print(\"Inserting new records with evolved schema:\")\n",
    "    \n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} \n",
    "        (user_id, username, email, is_active, created_year, created_month, created_day,\n",
    "         updated_at, country, registration_source, user_score, last_login_at)\n",
    "        VALUES \n",
    "        (2001, 'global_user_1', 'global1@company.com', true, 2025, 6, 27,\n",
    "         current_timestamp(), 'Canada', 'web', 85.5, current_timestamp()),\n",
    "        (2002, 'global_user_2', 'global2@company.com', true, 2025, 6, 27,\n",
    "         current_timestamp(), 'Germany', 'mobile', 92.0, current_timestamp()),\n",
    "        (2003, 'global_user_3', 'global3@company.com', false, 2025, 6, 27,\n",
    "         current_timestamp(), 'Japan', 'api', 78.3, null)\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(insert_query)\n",
    "    print(\"✓ New users inserted successfully\")\n",
    "\n",
    "# Insert evolved data\n",
    "insert_evolved_data(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with Mixed Schema Evolution:\n",
      "+-------+--------------+-------+-------------------+----------+---------------+\n",
      "|user_id|username      |country|registration_source|user_score|user_type      |\n",
      "+-------+--------------+-------+-------------------+----------+---------------+\n",
      "|1      |updated_user_1|NULL   |NULL               |NULL      |Legacy User    |\n",
      "|2      |updated_user_2|NULL   |NULL               |NULL      |Legacy User    |\n",
      "|3      |updated_user_3|NULL   |NULL               |NULL      |Legacy User    |\n",
      "|2001   |global_user_1 |Canada |web                |85.5      |New Schema User|\n",
      "|2002   |global_user_2 |Germany|mobile             |92.0      |New Schema User|\n",
      "|2003   |global_user_3 |Japan  |api                |78.3      |New Schema User|\n",
      "+-------+--------------+-------+-------------------+----------+---------------+\n",
      "\n",
      "\n",
      "Schema Version Statistics:\n",
      "+---------------+----------+------------+\n",
      "| schema_version|user_count|active_count|\n",
      "+---------------+----------+------------+\n",
      "| Evolved Schema|         3|           2|\n",
      "|Original Schema|      1006|         506|\n",
      "+---------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_mixed_schema_data(table_name):\n",
    "    \"\"\"Display data showing both old and new schema versions.\"\"\"\n",
    "    print(\"Data with Mixed Schema Evolution:\")\n",
    "    mixed_data = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            user_id, username, country, registration_source, user_score,\n",
    "            CASE \n",
    "                WHEN country IS NULL THEN 'Legacy User'\n",
    "                ELSE 'New Schema User'\n",
    "            END as user_type\n",
    "        FROM {table_name} \n",
    "        WHERE user_id IN (1, 2, 3, 2001, 2002, 2003)\n",
    "        ORDER BY user_id\n",
    "    \"\"\")\n",
    "    mixed_data.show(truncate=False)\n",
    "\n",
    "    print(\"\\nSchema Version Statistics:\")\n",
    "    version_stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN country IS NULL THEN 'Original Schema'\n",
    "                ELSE 'Evolved Schema'\n",
    "            END as schema_version,\n",
    "            COUNT(*) as user_count,\n",
    "            COUNT(CASE WHEN is_active = true THEN 1 END) as active_count\n",
    "        FROM {table_name} \n",
    "        GROUP BY CASE WHEN country IS NULL THEN 'Original Schema' ELSE 'Evolved Schema' END\n",
    "    \"\"\")\n",
    "    version_stats.show()\n",
    "\n",
    "# Show mixed schema data\n",
    "show_mixed_schema_data(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Evolution - Column Operations\n",
    "\n",
    "Explore additional schema evolution operations like renaming and reordering columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED SCHEMA OPERATIONS ===\n",
      "\n",
      "1. Renaming 'user_score' to 'engagement_score'...\n",
      "   ✓ Column renamed successfully\n",
      "\n",
      "2. Updating column comment for engagement_score...\n",
      "   ✓ Column comment updated\n",
      "\n",
      "Updated schema after column operations:\n",
      "+-----------------------+-------------+----------------------------------------------+\n",
      "|col_name               |data_type    |comment                                       |\n",
      "+-----------------------+-------------+----------------------------------------------+\n",
      "|user_id                |bigint       |NULL                                          |\n",
      "|username               |string       |NULL                                          |\n",
      "|email                  |string       |NULL                                          |\n",
      "|is_active              |boolean      |NULL                                          |\n",
      "|created_year           |int          |NULL                                          |\n",
      "|created_month          |int          |NULL                                          |\n",
      "|created_day            |int          |NULL                                          |\n",
      "|updated_at             |timestamp_ntz|NULL                                          |\n",
      "|country                |string       |NULL                                          |\n",
      "|registration_source    |string       |Source of user registration (web, mobile, api)|\n",
      "|engagement_score       |double       |User engagement score (0-100 scale)           |\n",
      "|last_login_at          |timestamp    |NULL                                          |\n",
      "|# Partition Information|             |                                              |\n",
      "|# col_name             |data_type    |comment                                       |\n",
      "|created_year           |int          |NULL                                          |\n",
      "|created_month          |int          |NULL                                          |\n",
      "|created_day            |int          |NULL                                          |\n",
      "+-----------------------+-------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def perform_column_operations(table_name):\n",
    "    \"\"\"Demonstrate advanced column operations like renaming.\"\"\"\n",
    "    print(\"=== ADVANCED SCHEMA OPERATIONS ===\\n\")\n",
    "    \n",
    "    # Check if user_score column exists before renaming\n",
    "    schema_df = spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "    column_names = [row['col_name'] for row in schema_df.collect()]\n",
    "    \n",
    "    if 'user_score' in column_names:\n",
    "        print(\"1. Renaming 'user_score' to 'engagement_score'...\")\n",
    "        spark.sql(f\"ALTER TABLE {table_name} RENAME COLUMN user_score TO engagement_score\")\n",
    "        print(\"   ✓ Column renamed successfully\")\n",
    "    elif 'engagement_score' in column_names:\n",
    "        print(\"1. Column 'user_score' already renamed to 'engagement_score'\")\n",
    "        print(\"   ✓ Column rename previously completed\")\n",
    "    else:\n",
    "        print(\"1. Neither 'user_score' nor 'engagement_score' found in schema\")\n",
    "\n",
    "    print(\"\\n2. Updating column comment for engagement_score...\")\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            ALTER TABLE {table_name} ALTER COLUMN engagement_score \n",
    "            COMMENT 'User engagement score (0-100 scale)'\n",
    "        \"\"\")\n",
    "        print(\"   ✓ Column comment updated\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Comment update failed: {str(e)}\")\n",
    "\n",
    "def show_updated_schema(table_name):\n",
    "    \"\"\"Show the updated schema after column operations.\"\"\"\n",
    "    print(\"\\nUpdated schema after column operations:\")\n",
    "    schema_info = spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "    schema_info.show(truncate=False)\n",
    "\n",
    "# Perform column operations\n",
    "perform_column_operations(TABLE_NAME)\n",
    "show_updated_schema(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partition Evolution - Adding New Partition Fields\n",
    "\n",
    "Now let's explore partition evolution, which allows us to optimize query performance by changing partitioning strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARTITION EVOLUTION ANALYSIS ===\n",
      "\n",
      "Current partitioning: created_year, created_month, created_day\n",
      "\n",
      "Analyzing query patterns that would benefit from additional partitioning...\n",
      "\n",
      "Distribution by is_active (common filter):\n",
      "+---------+----------+----------+\n",
      "|is_active|user_count|percentage|\n",
      "+---------+----------+----------+\n",
      "|    false|       501|     49.65|\n",
      "|     true|       508|     50.35|\n",
      "+---------+----------+----------+\n",
      "\n",
      "\n",
      "Distribution by country:\n",
      "+-------+----------+\n",
      "|country|user_count|\n",
      "+-------+----------+\n",
      "|Unknown|      1006|\n",
      "|Germany|         1|\n",
      "| Canada|         1|\n",
      "|  Japan|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_partition_patterns(table_name):\n",
    "    \"\"\"Analyze current query patterns for partition evolution decisions.\"\"\"\n",
    "    print(\"=== PARTITION EVOLUTION ANALYSIS ===\\n\")\n",
    "    print(\"Current partitioning: created_year, created_month, created_day\")\n",
    "    print(\"\\nAnalyzing query patterns that would benefit from additional partitioning...\")\n",
    "\n",
    "    # Distribution by is_active (common filter)\n",
    "    print(\"\\nDistribution by is_active (common filter):\")\n",
    "    active_distribution = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            is_active,\n",
    "            COUNT(*) as user_count,\n",
    "            ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM {table_name}), 2) as percentage\n",
    "        FROM {table_name} \n",
    "        GROUP BY is_active\n",
    "        ORDER BY is_active\n",
    "    \"\"\")\n",
    "    active_distribution.show()\n",
    "\n",
    "    # Distribution by country (for geographic queries)\n",
    "    print(\"\\nDistribution by country:\")\n",
    "    country_distribution = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COALESCE(country, 'Unknown') as country,\n",
    "            COUNT(*) as user_count\n",
    "        FROM {table_name} \n",
    "        GROUP BY country\n",
    "        ORDER BY user_count DESC\n",
    "    \"\"\")\n",
    "    country_distribution.show()\n",
    "\n",
    "# Analyze partition patterns\n",
    "analyze_partition_patterns(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding is_active as partition field...\n",
      "✓ is_active partition field added\n",
      "\n",
      "Updated partition information:\n",
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|is_active              |boolean  |NULL   |\n",
      "|created_year           |int      |NULL   |\n",
      "|created_month          |int      |NULL   |\n",
      "|created_day            |int      |NULL   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|created_year           |int      |NULL   |\n",
      "|created_month          |int      |NULL   |\n",
      "|created_day            |int      |NULL   |\n",
      "|is_active              |boolean  |NULL   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evolve_partitioning(table_name):\n",
    "    \"\"\"Add new partition fields for better query optimization.\"\"\"\n",
    "    print(\"Adding is_active as partition field...\")\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {table_name} ADD PARTITION FIELD is_active\")\n",
    "        print(\"✓ is_active partition field added\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e) or \"Partition field\" in str(e):\n",
    "            print(\"✓ is_active partition field already exists\")\n",
    "        else:\n",
    "            print(f\"⚠ Partition field addition failed: {str(e)}\")\n",
    "\n",
    "def show_partition_info(table_name):\n",
    "    \"\"\"Display updated partition information.\"\"\"\n",
    "    print(\"\\nUpdated partition information:\")\n",
    "    partition_info = spark.sql(f\"DESCRIBE EXTENDED {table_name}\")\n",
    "    \n",
    "    # Filter for partition-related information\n",
    "    relevant_info = partition_info.filter(\n",
    "        (col(\"col_name\").contains(\"Partition\")) |\n",
    "        (col(\"col_name\").isin([\"created_year\", \"created_month\", \"created_day\", \"is_active\"])) |\n",
    "        (col(\"col_name\") == \"# col_name\")\n",
    "    )\n",
    "    relevant_info.show(truncate=False)\n",
    "\n",
    "# Evolve partitioning\n",
    "evolve_partitioning(TABLE_NAME)\n",
    "show_partition_info(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data with new partition specification:\n",
      "✓ Test data inserted with new partitioning\n"
     ]
    }
   ],
   "source": [
    "# Insert data that will use the new partition specification\n",
    "print(\"Inserting data with new partition specification:\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO users \n",
    "    (user_id, username, email, is_active, created_year, created_month, created_day, \n",
    "     updated_at, country, registration_source, engagement_score, last_login_at)\n",
    "    VALUES \n",
    "    (3001, 'partition_test_active', 'active@test.com', true, 2025, 6, 28, \n",
    "     current_timestamp(), 'USA', 'web', 88.0, current_timestamp()),\n",
    "    (3002, 'partition_test_inactive', 'inactive@test.com', false, 2025, 6, 28, \n",
    "     current_timestamp(), 'UK', 'mobile', 45.0, null),\n",
    "    (3003, 'partition_test_active2', 'active2@test.com', true, 2025, 6, 28, \n",
    "     current_timestamp(), 'France', 'api', 91.5, current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Test data inserted with new partitioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding country-based partitioning for geographic optimization...\n",
      "✓ Country bucket partition field added (5 buckets)\n",
      "\n",
      "Final partition specification:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name      |data_type                                                                                        |comment|\n",
      "+--------------+-------------------------------------------------------------------------------------------------+-------+\n",
      "|is_active     |boolean                                                                                          |NULL   |\n",
      "|created_year  |int                                                                                              |NULL   |\n",
      "|created_month |int                                                                                              |NULL   |\n",
      "|created_day   |int                                                                                              |NULL   |\n",
      "|# Partitioning|                                                                                                 |       |\n",
      "|Part 4        |bucket(5, country)                                                                               |       |\n",
      "|_partition    |struct<created_year:int,created_month:int,created_day:int,is_active:boolean,country_bucket_5:int>|       |\n",
      "+--------------+-------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_geographic_partitioning(table_name):\n",
    "    \"\"\"Add geographic partitioning for optimization.\"\"\"\n",
    "    print(\"\\nAdding country-based partitioning for geographic optimization...\")\n",
    "    try:\n",
    "        spark.sql(f\"ALTER TABLE {table_name} ADD PARTITION FIELD bucket(5, country)\")\n",
    "        print(\"✓ Country bucket partition field added (5 buckets)\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e) or \"Partition field\" in str(e):\n",
    "            print(\"✓ Country bucket partition field already exists\")\n",
    "        else:\n",
    "            print(f\"⚠ Country bucket partition addition failed: {str(e)}\")\n",
    "\n",
    "def show_final_partition_spec(table_name):\n",
    "    \"\"\"Show final partition specification.\"\"\"\n",
    "    print(\"\\nFinal partition specification:\")\n",
    "    final_partition_info = spark.sql(f\"DESCRIBE EXTENDED {table_name}\")\n",
    "    \n",
    "    # Show partition-related rows\n",
    "    partition_rows = []\n",
    "    for row in final_partition_info.collect():\n",
    "        col_name = row['col_name']\n",
    "        if ('Partition' in col_name or \n",
    "            col_name in ['created_year', 'created_month', 'created_day', 'is_active'] or\n",
    "            col_name == '# col_name' or\n",
    "            'bucket' in str(row['data_type']).lower()):\n",
    "            partition_rows.append(row)\n",
    "    \n",
    "    if partition_rows:\n",
    "        partition_df = spark.createDataFrame(partition_rows)\n",
    "        partition_df.show(truncate=False)\n",
    "    else:\n",
    "        print(\"No partition information found\")\n",
    "\n",
    "# Add geographic partitioning\n",
    "add_geographic_partitioning(TABLE_NAME)\n",
    "show_final_partition_spec(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evolution Impact Analysis\n",
    "\n",
    "Let's analyze how evolution affects query performance and data organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVOLUTION IMPACT ANALYSIS ===\n",
      "\n",
      "Partition Distribution Analysis:\n",
      "+------------+-------------+-----------+---------+------------+------------------+\n",
      "|created_year|created_month|created_day|is_active|record_count|distinct_countries|\n",
      "+------------+-------------+-----------+---------+------------+------------------+\n",
      "|        2025|            6|         27|    false|         501|                 1|\n",
      "|        2025|            6|         27|     true|         501|                 2|\n",
      "|        2025|            6|         28|    false|           1|                 1|\n",
      "|        2025|            6|         28|     true|           2|                 2|\n",
      "|        2025|            7|          1|     true|           7|                 0|\n",
      "+------------+-------------+-----------+---------+------------+------------------+\n",
      "\n",
      "\n",
      "Schema Evolution Timeline:\n",
      "+---------------------------+----------+-----------+-----------+\n",
      "|evolution_phase            |user_count|min_user_id|max_user_id|\n",
      "+---------------------------+----------+-----------+-----------+\n",
      "|Phase 1: Original Schema   |1006      |1          |1099       |\n",
      "|Phase 2: Schema + Geography|3         |2001       |2003       |\n",
      "|Phase 3: Full Evolution    |3         |3001       |3003       |\n",
      "+---------------------------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze partition distribution after evolution\n",
    "print(\"=== EVOLUTION IMPACT ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "# Show how data is distributed across partitions\n",
    "partition_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        created_year,\n",
    "        created_month,\n",
    "        created_day,\n",
    "        is_active,\n",
    "        COUNT(*) as record_count,\n",
    "        COUNT(DISTINCT country) as distinct_countries\n",
    "    FROM users \n",
    "    GROUP BY created_year, created_month, created_day, is_active\n",
    "    ORDER BY created_year, created_month, created_day, is_active\n",
    "\"\"\")\n",
    "\n",
    "print(\"Partition Distribution Analysis:\")\n",
    "partition_analysis.show()\n",
    "\n",
    "# Show schema evolution timeline\n",
    "print(\"\\nSchema Evolution Timeline:\")\n",
    "evolution_timeline = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN country IS NULL THEN 'Phase 1: Original Schema'\n",
    "            WHEN user_id < 3000 THEN 'Phase 2: Schema + Geography'\n",
    "            ELSE 'Phase 3: Full Evolution'\n",
    "        END as evolution_phase,\n",
    "        COUNT(*) as user_count,\n",
    "        MIN(user_id) as min_user_id,\n",
    "        MAX(user_id) as max_user_id\n",
    "    FROM users \n",
    "    GROUP BY CASE \n",
    "        WHEN country IS NULL THEN 'Phase 1: Original Schema'\n",
    "        WHEN user_id < 3000 THEN 'Phase 2: Schema + Geography'\n",
    "        ELSE 'Phase 3: Full Evolution'\n",
    "    END\n",
    "    ORDER BY min_user_id\n",
    "\"\"\")\n",
    "evolution_timeline.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing queries optimized by partition evolution:\n",
      "\n",
      "1. Query filtering by is_active (leverages new partitioning):\n",
      "+-------+-------------------+------------+--------------+\n",
      "|country|registration_source|active_users|avg_engagement|\n",
      "+-------+-------------------+------------+--------------+\n",
      "|   NULL|               NULL|         506|          NULL|\n",
      "| France|                api|           1|          91.5|\n",
      "|    USA|                web|           1|          88.0|\n",
      "| Canada|                web|           1|          85.5|\n",
      "|Germany|             mobile|           1|          92.0|\n",
      "+-------+-------------------+------------+--------------+\n",
      "\n",
      "\n",
      "2. Geographic analysis (leverages country bucketing):\n",
      "+-------+-----------+------------+--------------+----------------+\n",
      "|country|total_users|active_users|avg_engagement|users_with_login|\n",
      "+-------+-----------+------------+--------------+----------------+\n",
      "|Unknown|       1006|         506|          NULL|               0|\n",
      "|Germany|          1|           1|          92.0|               1|\n",
      "| France|          1|           1|          91.5|               1|\n",
      "|    USA|          1|           1|          88.0|               1|\n",
      "|     UK|          1|           0|          45.0|               0|\n",
      "| Canada|          1|           1|          85.5|               1|\n",
      "|  Japan|          1|           0|          78.3|               0|\n",
      "+-------+-----------+------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test queries that benefit from partition evolution\n",
    "print(\"Testing queries optimized by partition evolution:\")\n",
    "print()\n",
    "\n",
    "# Query 1: Filter by is_active (now partitioned)\n",
    "print(\"1. Query filtering by is_active (leverages new partitioning):\")\n",
    "active_users_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        registration_source,\n",
    "        COUNT(*) as active_users,\n",
    "        AVG(engagement_score) as avg_engagement\n",
    "    FROM users \n",
    "    WHERE is_active = true\n",
    "    GROUP BY country, registration_source\n",
    "    ORDER BY active_users DESC\n",
    "\"\"\")\n",
    "active_users_query.show()\n",
    "\n",
    "# Query 2: Geographic analysis (benefits from country bucketing)\n",
    "print(\"\\n2. Geographic analysis (leverages country bucketing):\")\n",
    "geographic_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(country, 'Unknown') as country,\n",
    "        COUNT(*) as total_users,\n",
    "        COUNT(CASE WHEN is_active = true THEN 1 END) as active_users,\n",
    "        ROUND(AVG(engagement_score), 2) as avg_engagement,\n",
    "        COUNT(CASE WHEN last_login_at IS NOT NULL THEN 1 END) as users_with_login\n",
    "    FROM users \n",
    "    GROUP BY country\n",
    "    ORDER BY total_users DESC\n",
    "\"\"\")\n",
    "geographic_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evolution Best Practices and Monitoring\n",
    "\n",
    "Let's explore monitoring and best practices for managing evolving schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVOLUTION MONITORING AND BEST PRACTICES ===\n",
      "\n",
      "Schema Complexity Metrics:\n",
      "+-----------------+-----+\n",
      "|           metric|value|\n",
      "+-----------------+-----+\n",
      "|Schema Complexity|   17|\n",
      "| Nullable Columns|   17|\n",
      "+-----------------+-----+\n",
      "\n",
      "\n",
      "Data Quality Across Evolution:\n",
      "+-----------------------------+-----+\n",
      "|metric                       |value|\n",
      "+-----------------------------+-----+\n",
      "|Records with Country         |6    |\n",
      "|Records with Engagement Score|6    |\n",
      "|Records with Last Login      |4    |\n",
      "|Total Records                |1012 |\n",
      "+-----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def monitor_schema_complexity(table_name):\n",
    "    \"\"\"Monitor schema complexity and data quality metrics.\"\"\"\n",
    "    print(\"=== EVOLUTION MONITORING AND BEST PRACTICES ===\\n\")\n",
    "    \n",
    "    # Get schema information\n",
    "    schema_df = spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "    schema_rows = schema_df.collect()\n",
    "    \n",
    "    # Calculate complexity metrics\n",
    "    total_columns = len([row for row in schema_rows if row['col_name'] and not row['col_name'].startswith('#')])\n",
    "    nullable_columns = len([row for row in schema_rows if row['data_type'] and 'nullable' not in row['data_type']])\n",
    "    \n",
    "    # Create metrics DataFrame\n",
    "    metrics_data = [\n",
    "        ('Schema Complexity', str(total_columns)),\n",
    "        ('Nullable Columns', str(nullable_columns))\n",
    "    ]\n",
    "    \n",
    "    metrics_df = spark.createDataFrame(metrics_data, ['metric', 'value'])\n",
    "    print(\"Schema Complexity Metrics:\")\n",
    "    metrics_df.show()\n",
    "\n",
    "def monitor_data_quality(table_name):\n",
    "    \"\"\"Monitor data quality across evolution phases.\"\"\"\n",
    "    data_quality = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            'Total Records' as metric,\n",
    "            CAST(COUNT(*) as STRING) as value\n",
    "        FROM {table_name}\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Records with Country' as metric,\n",
    "            CAST(COUNT(*) as STRING) as value\n",
    "        FROM {table_name} WHERE country IS NOT NULL\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Records with Engagement Score' as metric,\n",
    "            CAST(COUNT(*) as STRING) as value\n",
    "        FROM {table_name} WHERE engagement_score IS NOT NULL\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Records with Last Login' as metric,\n",
    "            CAST(COUNT(*) as STRING) as value\n",
    "        FROM {table_name} WHERE last_login_at IS NOT NULL\n",
    "        \n",
    "        ORDER BY metric\n",
    "    \"\"\")\n",
    "    print(\"\\nData Quality Across Evolution:\")\n",
    "    data_quality.show(truncate=False)\n",
    "\n",
    "# Monitor evolution\n",
    "monitor_schema_complexity(TABLE_NAME)\n",
    "monitor_data_quality(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating Backward Compatibility:\n",
      "\n",
      "1. Original schema query (still functional):\n",
      "+-------+----------------+--------------------+---------+------------+-------------+-----------+\n",
      "|user_id|        username|               email|is_active|created_year|created_month|created_day|\n",
      "+-------+----------------+--------------------+---------+------------+-------------+-----------+\n",
      "|      8|new_premium_user| premium@company.com|     true|        2025|            6|         27|\n",
      "|    100|   bulk_user_100|bulk.user.100@exa...|     true|        2025|            6|         27|\n",
      "|    101|   bulk_user_101|bulk.user.101@exa...|     true|        2025|            6|         27|\n",
      "|    102|   bulk_user_102|bulk.user.102@exa...|    false|        2025|            6|         27|\n",
      "|    103|   bulk_user_103|bulk.user.103@exa...|     true|        2025|            6|         27|\n",
      "+-------+----------------+--------------------+---------+------------+-------------+-----------+\n",
      "\n",
      "\n",
      "2. Full evolved schema query:\n",
      "+-------+---------------------+-------+-------------------+----------------+---------+-------------+\n",
      "|user_id|username             |country|registration_source|engagement_score|is_active|user_category|\n",
      "+-------+---------------------+-------+-------------------+----------------+---------+-------------+\n",
      "|1      |updated_user_1       |NULL   |NULL               |NULL            |true     |Legacy User  |\n",
      "|2001   |global_user_1        |Canada |web                |85.5            |true     |Recent User  |\n",
      "|3001   |partition_test_active|USA    |web                |88.0            |true     |Recent User  |\n",
      "+-------+---------------------+-------+-------------------+----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate backward compatibility\n",
    "print(\"Demonstrating Backward Compatibility:\")\n",
    "print()\n",
    "\n",
    "# Old-style query (original schema) still works\n",
    "print(\"1. Original schema query (still functional):\")\n",
    "original_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        user_id, username, email, is_active, \n",
    "        created_year, created_month, created_day\n",
    "    FROM users \n",
    "    WHERE created_year = 2025 AND created_month = 6\n",
    "    ORDER BY user_id\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "original_query.show()\n",
    "\n",
    "# New schema query with all features\n",
    "print(\"\\n2. Full evolved schema query:\")\n",
    "evolved_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        user_id, username, country, registration_source,\n",
    "        engagement_score, is_active,\n",
    "        CASE \n",
    "            WHEN last_login_at IS NOT NULL THEN 'Recent User'\n",
    "            WHEN country IS NOT NULL THEN 'Geographic User'\n",
    "            ELSE 'Legacy User'\n",
    "        END as user_category\n",
    "    FROM users \n",
    "    WHERE user_id IN (1, 2001, 3001)\n",
    "    ORDER BY user_id\n",
    "\"\"\")\n",
    "evolved_query.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Evolution Scenarios\n",
    "\n",
    "Explore complex evolution scenarios and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADVANCED EVOLUTION SCENARIOS ===\n",
      "\n",
      "Current partition fields before optimization:\n",
      "+--------------------+-------+------------+----------+\n",
      "|           partition|spec_id|record_count|file_count|\n",
      "+--------------------+-------+------------+----------+\n",
      "|{2025, 6, 28, tru...|      1|           2|         1|\n",
      "|{2025, 7, 1, NULL...|      0|           7|         1|\n",
      "|{2025, 6, 28, fal...|      1|           1|         1|\n",
      "|{2025, 6, 27, NUL...|      0|        1002|         3|\n",
      "+--------------------+-------+------------+----------+\n",
      "\n",
      "\n",
      "Evolution Benefits Analysis:\n",
      "+-------------------+-----+--------+\n",
      "|metric             |value|coverage|\n",
      "+-------------------+-----+--------+\n",
      "|Total Users        |1012 |100%    |\n",
      "|Geographic Coverage|6    |0.6%    |\n",
      "|Engagement Scoring |6    |0.6%    |\n",
      "|Source Tracking    |6    |0.6%    |\n",
      "+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_advanced_scenarios():\n",
    "    \"\"\"Demonstrate advanced evolution scenarios and edge cases.\"\"\"\n",
    "    print(\"=== ADVANCED EVOLUTION SCENARIOS ===\\n\")\n",
    "    \n",
    "    print(\"Current partition fields before optimization:\")\n",
    "    try:\n",
    "        current_partitions = spark.sql(f\"\"\"\n",
    "            SELECT partition, spec_id, record_count, file_count \n",
    "            FROM rest.`play_iceberg`.{TABLE_NAME}.partitions\n",
    "        \"\"\")\n",
    "        current_partitions.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Partition metadata query not available in this environment: {e}\")\n",
    "        print(\"Partition fields can be viewed in DESCRIBE EXTENDED output\")\n",
    "\n",
    "def analyze_evolution_benefits(table_name):\n",
    "    \"\"\"Create a summary of evolution benefits.\"\"\"\n",
    "    print(\"\\nEvolution Benefits Analysis:\")\n",
    "    benefits_analysis = spark.sql(f\"\"\"\n",
    "        WITH evolution_stats AS (\n",
    "            SELECT \n",
    "                COUNT(*) as total_users,\n",
    "                COUNT(CASE WHEN country IS NOT NULL THEN 1 END) as users_with_geography,\n",
    "                COUNT(CASE WHEN engagement_score IS NOT NULL THEN 1 END) as users_with_scores,\n",
    "                COUNT(CASE WHEN registration_source IS NOT NULL THEN 1 END) as users_with_source,\n",
    "                COUNT(DISTINCT COALESCE(country, 'Unknown')) as distinct_countries,\n",
    "                COUNT(DISTINCT created_year || '-' || created_month || '-' || created_day) as distinct_date_partitions\n",
    "            FROM {table_name}\n",
    "        )\n",
    "        SELECT \n",
    "            'Total Users' as metric,\n",
    "            CAST(total_users as STRING) as value,\n",
    "            '100%' as coverage\n",
    "        FROM evolution_stats\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Geographic Coverage' as metric,\n",
    "            CAST(users_with_geography as STRING) as value,\n",
    "            CAST(ROUND(users_with_geography * 100.0 / total_users, 1) as STRING) || '%' as coverage\n",
    "        FROM evolution_stats\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Engagement Scoring' as metric,\n",
    "            CAST(users_with_scores as STRING) as value,\n",
    "            CAST(ROUND(users_with_scores * 100.0 / total_users, 1) as STRING) || '%' as coverage\n",
    "        FROM evolution_stats\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Source Tracking' as metric,\n",
    "            CAST(users_with_source as STRING) as value,\n",
    "            CAST(ROUND(users_with_source * 100.0 / total_users, 1) as STRING) || '%' as coverage\n",
    "        FROM evolution_stats\n",
    "    \"\"\")\n",
    "    benefits_analysis.show(truncate=False)\n",
    "\n",
    "# Demonstrate advanced scenarios\n",
    "demonstrate_advanced_scenarios()\n",
    "analyze_evolution_benefits(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evolution Strategy Recommendations\n",
    "\n",
    "Compile best practices and recommendations for managing schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVOLUTION STRATEGY RECOMMENDATIONS ===\n",
      "\n",
      "Evolution Best Practices:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------------------------------------+\n",
      "|category           |recommendation                                   |\n",
      "+-------------------+-------------------------------------------------+\n",
      "|Schema Evolution   |Add columns as nullable to maintain compatibility|\n",
      "|Schema Evolution   |Use meaningful column names and comments         |\n",
      "|Schema Evolution   |Plan for data type compatibility                 |\n",
      "|Partition Evolution|Add partition fields based on query patterns     |\n",
      "|Partition Evolution|Use bucketing for high-cardinality fields        |\n",
      "|Partition Evolution|Monitor partition distribution and skew          |\n",
      "|Data Quality       |Validate data during evolution phases            |\n",
      "|Data Quality       |Maintain backward compatibility                  |\n",
      "|Performance        |Test queries after evolution changes             |\n",
      "|Performance        |Monitor file count and compaction needs          |\n",
      "+-------------------+-------------------------------------------------+\n",
      "\n",
      "\n",
      "Final Evolved Table State:\n",
      "+-----------------+-----+\n",
      "|           aspect|count|\n",
      "+-----------------+-----+\n",
      "|   Schema Columns|   17|\n",
      "|Active Partitions|    5|\n",
      "|    Total Records| 1012|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_evolution_recommendations():\n",
    "    \"\"\"Create evolution strategy recommendations.\"\"\"\n",
    "    print(\"=== EVOLUTION STRATEGY RECOMMENDATIONS ===\\n\")\n",
    "    \n",
    "    recommendations = [\n",
    "        (\"Schema Evolution\", \"Add columns as nullable to maintain compatibility\"),\n",
    "        (\"Schema Evolution\", \"Use meaningful column names and comments\"),\n",
    "        (\"Schema Evolution\", \"Plan for data type compatibility\"),\n",
    "        (\"Partition Evolution\", \"Add partition fields based on query patterns\"),\n",
    "        (\"Partition Evolution\", \"Use bucketing for high-cardinality fields\"),\n",
    "        (\"Partition Evolution\", \"Monitor partition distribution and skew\"),\n",
    "        (\"Data Quality\", \"Validate data during evolution phases\"),\n",
    "        (\"Data Quality\", \"Maintain backward compatibility\"),\n",
    "        (\"Performance\", \"Test queries after evolution changes\"),\n",
    "        (\"Performance\", \"Monitor file count and compaction needs\")\n",
    "    ]\n",
    "\n",
    "    recommendations_df = spark.createDataFrame(\n",
    "        recommendations, \n",
    "        [\"category\", \"recommendation\"]\n",
    "    )\n",
    "    print(\"Evolution Best Practices:\")\n",
    "    recommendations_df.show(truncate=False)\n",
    "\n",
    "def show_final_table_state(table_name):\n",
    "    \"\"\"Show final evolved table state without using information_schema.\"\"\"\n",
    "    print(\"\\nFinal Evolved Table State:\")\n",
    "    \n",
    "    # Get schema column count\n",
    "    schema_df = spark.sql(f\"DESCRIBE TABLE {table_name}\")\n",
    "    schema_rows = [row for row in schema_df.collect() if row['col_name'] and not row['col_name'].startswith('#')]\n",
    "    column_count = len(schema_rows)\n",
    "    \n",
    "    # Get other metrics from the table\n",
    "    final_state = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            'Total Records' as aspect,\n",
    "            CAST(COUNT(*) as STRING) as count\n",
    "        FROM {table_name}\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Active Partitions' as aspect,\n",
    "            CAST(COUNT(DISTINCT created_year || '-' || created_month || '-' || created_day || '-' || is_active) as STRING) as count\n",
    "        FROM {table_name}\n",
    "        \n",
    "        ORDER BY aspect\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create final state with schema columns\n",
    "    final_data = [('Schema Columns', str(column_count))]\n",
    "    final_data.extend([(row['aspect'], row['count']) for row in final_state.collect()])\n",
    "    \n",
    "    final_df = spark.createDataFrame(final_data, ['aspect', 'count'])\n",
    "    final_df.show()\n",
    "\n",
    "# Create recommendations and show final state\n",
    "create_evolution_recommendations()\n",
    "show_final_table_state(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Cleanup\n",
    "\n",
    "Summarize the evolution journey and clean up the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCHEMA AND PARTITION EVOLUTION TUTORIAL SUMMARY ===\n",
      "\n",
      "EVOLUTION JOURNEY COMPLETED:\n",
      "==================================================\n",
      "\n",
      "📋 SCHEMA EVOLUTION ACHIEVEMENTS:\n",
      "   ✓ Added geographic information (country)\n",
      "   ✓ Added user registration tracking (registration_source)\n",
      "   ✓ Added engagement scoring (engagement_score)\n",
      "   ✓ Added user activity tracking (last_login_at)\n",
      "   ✓ Renamed columns for clarity\n",
      "   ✓ Added meaningful column comments\n",
      "\n",
      "🗂️ PARTITION EVOLUTION ACHIEVEMENTS:\n",
      "   ✓ Added is_active partitioning for query optimization\n",
      "   ✓ Added country bucketing for geographic queries\n",
      "   ✓ Maintained backward compatibility\n",
      "   ✓ Optimized for common query patterns\n",
      "\n",
      "📊 DATA QUALITY MAINTENANCE:\n",
      "   ✓ Preserved all existing data during evolution\n",
      "   ✓ Maintained data type consistency\n",
      "   ✓ Handled NULL values appropriately\n",
      "   ✓ Enabled gradual schema adoption\n",
      "\n",
      "FINAL TABLE STATISTICS:\n",
      "+-------------+----------------------+--------------------+---------------------+---------------+\n",
      "|total_records|records_with_geography|records_with_scoring|countries_represented|highest_user_id|\n",
      "+-------------+----------------------+--------------------+---------------------+---------------+\n",
      "|         1012|                     6|                   6|                    7|           3003|\n",
      "+-------------+----------------------+--------------------+---------------------+---------------+\n",
      "\n",
      "\n",
      "KEY LEARNINGS:\n",
      "• Schema evolution enables zero-downtime table modifications\n",
      "• Partition evolution optimizes query performance over time\n",
      "• Iceberg handles compatibility across different schema versions\n",
      "• Evolution should be driven by actual usage patterns\n",
      "• Monitoring and validation are crucial during evolution\n",
      "\n",
      "🎉 Tutorial completed successfully!\n",
      "Your table is now evolved and optimized for modern analytics workloads.\n"
     ]
    }
   ],
   "source": [
    "def print_evolution_summary():\n",
    "    \"\"\"Create comprehensive evolution summary.\"\"\"\n",
    "    print(\"=== SCHEMA AND PARTITION EVOLUTION TUTORIAL SUMMARY ===\\n\")\n",
    "    print(\"EVOLUTION JOURNEY COMPLETED:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    achievements = {\n",
    "        \"📋 SCHEMA EVOLUTION ACHIEVEMENTS\": [\n",
    "            \"Added geographic information (country)\",\n",
    "            \"Added user registration tracking (registration_source)\", \n",
    "            \"Added engagement scoring (engagement_score)\",\n",
    "            \"Added user activity tracking (last_login_at)\",\n",
    "            \"Renamed columns for clarity\",\n",
    "            \"Added meaningful column comments\"\n",
    "        ],\n",
    "        \"🗂️ PARTITION EVOLUTION ACHIEVEMENTS\": [\n",
    "            \"Added is_active partitioning for query optimization\",\n",
    "            \"Added country bucketing for geographic queries\", \n",
    "            \"Maintained backward compatibility\",\n",
    "            \"Optimized for common query patterns\"\n",
    "        ],\n",
    "        \"📊 DATA QUALITY MAINTENANCE\": [\n",
    "            \"Preserved all existing data during evolution\",\n",
    "            \"Maintained data type consistency\",\n",
    "            \"Handled NULL values appropriately\", \n",
    "            \"Enabled gradual schema adoption\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in achievements.items():\n",
    "        print(category + \":\")\n",
    "        for item in items:\n",
    "            print(f\"   ✓ {item}\")\n",
    "        print()\n",
    "\n",
    "def show_final_statistics(table_name):\n",
    "    \"\"\"Display final table statistics.\"\"\"\n",
    "    final_summary = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(CASE WHEN country IS NOT NULL THEN 1 END) as records_with_geography,\n",
    "            COUNT(CASE WHEN engagement_score IS NOT NULL THEN 1 END) as records_with_scoring,\n",
    "            COUNT(DISTINCT COALESCE(country, 'Unknown')) as countries_represented,\n",
    "            MAX(user_id) as highest_user_id\n",
    "        FROM {table_name}\n",
    "    \"\"\")\n",
    "    print(\"FINAL TABLE STATISTICS:\")\n",
    "    final_summary.show()\n",
    "\n",
    "def print_key_learnings():\n",
    "    \"\"\"Print key learnings from the evolution process.\"\"\"\n",
    "    learnings = [\n",
    "        \"Schema evolution enables zero-downtime table modifications\",\n",
    "        \"Partition evolution optimizes query performance over time\", \n",
    "        \"Iceberg handles compatibility across different schema versions\",\n",
    "        \"Evolution should be driven by actual usage patterns\",\n",
    "        \"Monitoring and validation are crucial during evolution\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nKEY LEARNINGS:\")\n",
    "    for learning in learnings:\n",
    "        print(f\"• {learning}\")\n",
    "    \n",
    "    print(\"\\n🎉 Tutorial completed successfully!\")\n",
    "    print(\"Your table is now evolved and optimized for modern analytics workloads.\")\n",
    "\n",
    "# Generate summary\n",
    "print_evolution_summary()\n",
    "show_final_statistics(TABLE_NAME)\n",
    "print_key_learnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing session cleanup...\n",
      "✓ Spark catalog cache cleared\n",
      "\n",
      "Session Information:\n",
      "Application: PySparkShell\n",
      "Catalog: rest\n",
      "\n",
      "✅ Evolution tutorial completed successfully!\n",
      "Your Iceberg table now supports modern analytics with evolved schema and optimized partitioning.\n",
      "\n",
      "============================================================\n",
      "TUTORIAL COMPLETE - READY FOR PRODUCTION WORKLOADS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def cleanup_session():\n",
    "    \"\"\"Perform comprehensive session cleanup.\"\"\"\n",
    "    print(\"Performing session cleanup...\")\n",
    "    \n",
    "    try:\n",
    "        # Clear any cached data\n",
    "        spark.catalog.clearCache()\n",
    "        print(\"✓ Spark catalog cache cleared\")\n",
    "        \n",
    "        # Show final session information\n",
    "        print(\"\\nSession Information:\")\n",
    "        print(f\"Application: {spark.sparkContext.getConf().get('spark.app.name')}\")\n",
    "        print(f\"Catalog: {spark.conf.get('spark.sql.defaultCatalog', 'Not set')}\")\n",
    "        \n",
    "        print(\"\\n✅ Evolution tutorial completed successfully!\")\n",
    "        print(\"Your Iceberg table now supports modern analytics with evolved schema and optimized partitioning.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup completed with minor warnings: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TUTORIAL COMPLETE - READY FOR PRODUCTION WORKLOADS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Perform cleanup\n",
    "cleanup_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
