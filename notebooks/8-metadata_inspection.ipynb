{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Apache Iceberg Metadata Inspection\n",
    "\n",
    "This comprehensive notebook demonstrates how to inspect and analyze Apache Iceberg table metadata. Understanding Iceberg's metadata structure is crucial for:\n",
    "\n",
    "- **Performance Optimization**: Query planning and partition pruning\n",
    "- **Storage Management**: Understanding file organization and sizes\n",
    "- **Data Evolution**: Tracking schema changes and table history\n",
    "- **Debugging**: Investigating data issues and performance bottlenecks\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "1. **Table History**: Complete timeline of all table snapshots and operations\n",
    "2. **Manifest Files**: Index files that track data file locations and statistics\n",
    "3. **Data Files**: Physical Parquet files containing the actual data\n",
    "4. **File-Level Analysis**: Deep dive into storage patterns and efficiency\n",
    "5. **Column Statistics**: Data distribution and quality insights\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Apache Iceberg table with data (run notebooks 1-7 first)\n",
    "- Spark session configured with Iceberg extensions\n",
    "- REST catalog connection to MinIO storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Initialize our Spark session with Iceberg configuration and verify the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:51:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Initialized\n",
      "   Spark Version: 3.5.5\n",
      "   Default Catalog: rest\n",
      "   Iceberg Extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users table found and accessible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, min as spark_min, max as spark_max, sum as spark_sum\n",
    ")\n",
    "\n",
    "# Create Spark session with Iceberg extensions\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Metadata Deep Dive\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Initialized\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Default Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"   Iceberg Extensions: {spark.conf.get('spark.sql.extensions')}\")\n",
    "\n",
    "# Verify table exists\n",
    "try:\n",
    "    table_exists = (\n",
    "        spark.sql(\"SHOW TABLES IN rest.`play_iceberg`\")\n",
    "        .filter(\"tableName = 'users'\")\n",
    "        .count() > 0\n",
    "    )\n",
    "    if table_exists:\n",
    "        print(\"Users table found and accessible\")\n",
    "    else:\n",
    "        print(\"Users table not found - run notebooks 1-7 first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing catalog: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-section",
   "metadata": {},
   "source": [
    "## 1. Table History Analysis\n",
    "\n",
    "The table history is Iceberg's version control system. Each snapshot represents a consistent state of the table after an operation (INSERT, UPDATE, DELETE, etc.).\n",
    "\n",
    "### Key Concepts:\n",
    "- **Snapshot ID**: Unique identifier for each table version\n",
    "- **Parent ID**: Points to the previous snapshot, forming a lineage chain\n",
    "- **made_current_at**: Timestamp when this snapshot became the current version\n",
    "- **is_current_ancestor**: Whether this snapshot is in the current table's lineage\n",
    "\n",
    "This information is crucial for:\n",
    "- Time travel queries\n",
    "- Understanding table evolution\n",
    "- Debugging data changes\n",
    "- Planning retention policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "history-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table History Timeline:\n",
      "==================================================\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|operation_type|\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "|2025-07-01 13:41:23.087|3824926247776142618|NULL               |true               |INITIAL       |\n",
      "|2025-07-01 13:46:22.566|3682366184055111348|3824926247776142618|true               |UPDATE        |\n",
      "|2025-07-01 13:46:29.92 |5474445044599496672|3682366184055111348|true               |UPDATE        |\n",
      "|2025-07-01 13:46:43.276|703749108260326071 |5474445044599496672|true               |UPDATE        |\n",
      "|2025-07-01 13:46:51.537|1680807781020980082|703749108260326071 |true               |UPDATE        |\n",
      "|2025-07-01 13:47:01.334|8866811446601788626|1680807781020980082|true               |UPDATE        |\n",
      "|2025-07-01 13:48:29.944|5929459840228022626|8866811446601788626|true               |UPDATE        |\n",
      "|2025-07-01 13:48:37.868|1926503896228031236|5929459840228022626|true               |UPDATE        |\n",
      "|2025-07-01 13:50:45.161|7274330971587238251|1926503896228031236|false              |UPDATE        |\n",
      "|2025-07-01 13:50:46.609|841024702459819741 |7274330971587238251|false              |UPDATE        |\n",
      "|2025-07-01 13:50:48.992|1926503896228031236|5929459840228022626|true               |UPDATE        |\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "\n",
      "\n",
      "History Statistics:\n",
      "   Total Snapshots: 11\n",
      "   Current Lineage: 9\n",
      "   Orphaned Snapshots: 2\n",
      "\n",
      "Timeline:\n",
      "   First Snapshot: 2025-07-01 13:41:23.087000\n",
      "   Latest Snapshot: 2025-07-01 13:50:48.992000\n",
      "   Table Age: 10 operations\n"
     ]
    }
   ],
   "source": [
    "# Analyze table history with detailed metrics\n",
    "history_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor,\n",
    "    CASE \n",
    "        WHEN parent_id IS NULL THEN 'INITIAL'\n",
    "        ELSE 'UPDATE'\n",
    "    END as operation_type\n",
    "FROM rest.`play_iceberg`.users.history\n",
    "ORDER BY made_current_at\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table History Timeline:\")\n",
    "print(\"=\" * 50)\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "# Calculate history statistics\n",
    "total_snapshots = history_df.count()\n",
    "current_ancestors = history_df.filter(\"is_current_ancestor = true\").count()\n",
    "orphaned_snapshots = total_snapshots - current_ancestors\n",
    "\n",
    "print(\"\\nHistory Statistics:\")\n",
    "print(f\"   Total Snapshots: {total_snapshots}\")\n",
    "print(f\"   Current Lineage: {current_ancestors}\")\n",
    "print(f\"   Orphaned Snapshots: {orphaned_snapshots}\")\n",
    "\n",
    "if total_snapshots > 1:\n",
    "    first_snapshot = history_df.orderBy(\"made_current_at\").first()\n",
    "    latest_snapshot = history_df.orderBy(col(\"made_current_at\").desc()).first()\n",
    "    \n",
    "    print(\"\\nTimeline:\")\n",
    "    print(f\"   First Snapshot: {first_snapshot['made_current_at']}\")\n",
    "    print(f\"   Latest Snapshot: {latest_snapshot['made_current_at']}\")\n",
    "    print(f\"   Table Age: {total_snapshots - 1} operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manifests-section",
   "metadata": {},
   "source": [
    "## 2. Manifest File Analysis\n",
    "\n",
    "Manifest files are Iceberg's indexing system. They contain metadata about data files and enable efficient query planning.\n",
    "\n",
    "### Understanding Manifests:\n",
    "- **Content Type**: 0 = data files, 1 = delete files\n",
    "- **Partition Spec**: Defines how data is partitioned\n",
    "- **File Counts**: Track additions, deletions, and existing files\n",
    "- **Partition Summaries**: Min/max values for partition pruning\n",
    "\n",
    "### Why This Matters:\n",
    "- Query engines use manifests to skip irrelevant files\n",
    "- Partition summaries enable efficient filtering\n",
    "- File counts help understand table operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manifests-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest Files Overview:\n",
      "============================================================\n",
      "+------------+-----------------+-------------------+----------------------+-------------------------+----------------+\n",
      "|content_type|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|manifest_size_kb|\n",
      "+------------+-----------------+-------------------+----------------------+-------------------------+----------------+\n",
      "|DATA_FILES  |0                |1926503896228031236|0                     |4                        |8.86            |\n",
      "|DATA_FILES  |1                |1926503896228031236|2                     |0                        |8.76            |\n",
      "+------------+-----------------+-------------------+----------------------+-------------------------+----------------+\n",
      "\n",
      "\n",
      "Manifest Statistics:\n",
      "   Total Manifests: 2\n",
      "   Files per Manifest: 0 - 2\n",
      "   Manifest Sizes: 8.76 - 8.86 KB\n",
      "\n",
      "Partition Summary Example:\n",
      "   Partition Spec ID: 0\n",
      "   Partition 1: 2025 - 2025 (nulls: False)\n",
      "   Partition 2: 6 - 7 (nulls: False)\n",
      "   Partition 3: 1 - 27 (nulls: False)\n"
     ]
    }
   ],
   "source": [
    "# Detailed manifest analysis\n",
    "manifests_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    content,\n",
    "    CASE content \n",
    "        WHEN 0 THEN 'DATA_FILES'\n",
    "        WHEN 1 THEN 'DELETE_FILES'\n",
    "        ELSE 'UNKNOWN'\n",
    "    END as content_type,\n",
    "    partition_spec_id,\n",
    "    added_snapshot_id,\n",
    "    added_data_files_count,\n",
    "    existing_data_files_count,\n",
    "    deleted_data_files_count,\n",
    "    ROUND(length / 1024.0, 2) as manifest_size_kb,\n",
    "    partition_summaries\n",
    "FROM rest.`play_iceberg`.users.manifests\n",
    "ORDER BY added_snapshot_id, partition_spec_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"Manifest Files Overview:\")\n",
    "print(\"=\" * 60)\n",
    "manifests_df.select(\n",
    "    \"content_type\", \"partition_spec_id\", \"added_snapshot_id\", \n",
    "    \"added_data_files_count\", \"existing_data_files_count\", \"manifest_size_kb\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Analyze manifest statistics\n",
    "manifest_stats = manifests_df.agg(\n",
    "    count(\"*\").alias(\"total_manifests\"),\n",
    "    spark_min(\"added_data_files_count\").alias(\"min_files_per_manifest\"),\n",
    "    spark_max(\"added_data_files_count\").alias(\"max_files_per_manifest\"),\n",
    "    spark_min(\"manifest_size_kb\").alias(\"min_manifest_size_kb\"),\n",
    "    spark_max(\"manifest_size_kb\").alias(\"max_manifest_size_kb\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nManifest Statistics:\")\n",
    "print(f\"   Total Manifests: {manifest_stats['total_manifests']}\")\n",
    "print(\n",
    "    f\"   Files per Manifest: {manifest_stats['min_files_per_manifest']} - \"\n",
    "    f\"{manifest_stats['max_files_per_manifest']}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Manifest Sizes: {manifest_stats['min_manifest_size_kb']:.2f} - \"\n",
    "    f\"{manifest_stats['max_manifest_size_kb']:.2f} KB\"\n",
    ")\n",
    "\n",
    "# Show partition summaries for the first manifest\n",
    "first_manifest = manifests_df.filter(\"partition_summaries IS NOT NULL\").first()\n",
    "if first_manifest and first_manifest['partition_summaries']:\n",
    "    print(\"\\nPartition Summary Example:\")\n",
    "    print(f\"   Partition Spec ID: {first_manifest['partition_spec_id']}\")\n",
    "    for i, summary in enumerate(first_manifest['partition_summaries']):\n",
    "        print(\n",
    "            f\"   Partition {i+1}: {summary['lower_bound']} - \"\n",
    "            f\"{summary['upper_bound']} (nulls: {summary['contains_null']})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datafiles-section",
   "metadata": {},
   "source": [
    "## 3. Data Files Deep Dive\n",
    "\n",
    "Data files are the actual Parquet files containing your table data. Understanding their characteristics is essential for performance optimization.\n",
    "\n",
    "### Key Metrics:\n",
    "- **File Size**: Impacts query performance (too small = overhead, too large = slow)\n",
    "- **Record Count**: Number of rows per file\n",
    "- **Compression Ratio**: Storage efficiency\n",
    "- **Partition Layout**: How data is organized on disk\n",
    "\n",
    "### Optimization Insights:\n",
    "- Optimal file sizes: 128MB - 1GB for most workloads\n",
    "- Consistent record counts indicate good partitioning\n",
    "- File paths reveal partition strategy effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "datafiles-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Files Analysis:\n",
      "================================================================================\n",
      "+-----------+------------+------------+----------------+---------------+\n",
      "|file_format|record_count|file_size_mb|bytes_per_record|partition_type |\n",
      "+-----------+------------+------------+----------------+---------------+\n",
      "|PARQUET    |998         |0.012       |13.07           |mixed_partition|\n",
      "|PARQUET    |2           |0.004       |1944.50         |active_users   |\n",
      "|PARQUET    |3           |0.004       |1282.33         |mixed_partition|\n",
      "|PARQUET    |1           |0.004       |3681.00         |inactive_users |\n",
      "|PARQUET    |7           |0.003       |375.29          |mixed_partition|\n",
      "|PARQUET    |1           |0.002       |2442.00         |mixed_partition|\n",
      "+-----------+------------+------------+----------------+---------------+\n",
      "\n",
      "\n",
      "Storage Statistics:\n",
      "   Total Files: 6\n",
      "   Total Records: 1,012\n",
      "   Total Storage: 29,528 bytes (0.028 MB)\n",
      "   File Size Range: 2442 - 13042 bytes\n",
      "   Records per File: 1 - 998\n",
      "   Average File Size: 4921 bytes\n",
      "   Average Records per File: 168.7\n",
      "\n",
      "Partition Distribution:\n",
      "+---------------+----------+-------------+----------+\n",
      "|partition_type |file_count|total_records|total_size|\n",
      "+---------------+----------+-------------+----------+\n",
      "|inactive_users |1         |1            |3681      |\n",
      "|active_users   |1         |2            |3889      |\n",
      "|mixed_partition|4         |1009         |21958     |\n",
      "+---------------+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data files analysis\n",
    "files_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    file_path,\n",
    "    file_format,\n",
    "    record_count,\n",
    "    file_size_in_bytes,\n",
    "    ROUND(file_size_in_bytes / 1024.0 / 1024.0, 3) as file_size_mb,\n",
    "    ROUND(file_size_in_bytes * 1.0 / record_count, 2) as bytes_per_record,\n",
    "    -- Extract partition information from file path\n",
    "    CASE \n",
    "        WHEN file_path LIKE '%is_active=true%' THEN 'active_users'\n",
    "        WHEN file_path LIKE '%is_active=false%' THEN 'inactive_users'\n",
    "        ELSE 'mixed_partition'\n",
    "    END as partition_type\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "ORDER BY file_size_in_bytes DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data Files Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "files_df.select(\n",
    "    \"file_format\", \"record_count\", \"file_size_mb\", \n",
    "    \"bytes_per_record\", \"partition_type\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Calculate storage statistics using Spark SQL functions\n",
    "file_stats = files_df.agg(\n",
    "    count(\"*\").alias(\"total_files\"),\n",
    "    spark_sum(\"record_count\").alias(\"total_records\"),\n",
    "    spark_sum(\"file_size_in_bytes\").alias(\"total_size_bytes\"),\n",
    "    spark_min(\"file_size_in_bytes\").alias(\"min_file_size\"),\n",
    "    spark_max(\"file_size_in_bytes\").alias(\"max_file_size\"),\n",
    "    spark_min(\"record_count\").alias(\"min_records\"),\n",
    "    spark_max(\"record_count\").alias(\"max_records\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nStorage Statistics:\")\n",
    "print(f\"   Total Files: {file_stats['total_files']}\")\n",
    "print(f\"   Total Records: {file_stats['total_records']:,}\")\n",
    "total_size_mb = file_stats['total_size_bytes']/1024/1024\n",
    "print(\n",
    "    f\"   Total Storage: {file_stats['total_size_bytes']:,} bytes \"\n",
    "    f\"({total_size_mb:.3f} MB)\"\n",
    ")\n",
    "print(\n",
    "    f\"   File Size Range: {file_stats['min_file_size']} - \"\n",
    "    f\"{file_stats['max_file_size']} bytes\"\n",
    ")\n",
    "print(\n",
    "    f\"   Records per File: {file_stats['min_records']} - \"\n",
    "    f\"{file_stats['max_records']}\"\n",
    ")\n",
    "avg_file_size = file_stats['total_size_bytes']/file_stats['total_files']\n",
    "print(f\"   Average File Size: {avg_file_size:.0f} bytes\")\n",
    "avg_records = file_stats['total_records']/file_stats['total_files']\n",
    "print(f\"   Average Records per File: {avg_records:.1f}\")\n",
    "\n",
    "# Analyze partition distribution\n",
    "partition_stats = files_df.groupBy(\"partition_type\").agg(\n",
    "    count(\"*\").alias(\"file_count\"),\n",
    "    spark_sum(\"record_count\").alias(\"total_records\"),\n",
    "    spark_sum(\"file_size_in_bytes\").alias(\"total_size\")\n",
    ").orderBy(\"file_count\")\n",
    "\n",
    "print(\"\\nPartition Distribution:\")\n",
    "partition_stats.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file-metadata-section",
   "metadata": {},
   "source": [
    "## 4. File-Level Metadata Deep Dive\n",
    "\n",
    "Iceberg tracks detailed statistics for each data file, enabling efficient query planning and optimization.\n",
    "\n",
    "### File Statistics Include:\n",
    "- **Column Sizes**: Bytes used by each column (compression effectiveness)\n",
    "- **Value Counts**: Number of values per column\n",
    "- **Null Counts**: Null values per column per file\n",
    "- **Bounds**: Min/max values for predicate pushdown\n",
    "\n",
    "### Performance Impact:\n",
    "- Query engines use bounds for file pruning\n",
    "- Column sizes help estimate scan costs\n",
    "- Statistics enable better join ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "file-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-Level Metadata Analysis:\n",
      "==================================================\n",
      "+--------------------------------------------------------------+------------------+------------+----------------+\n",
      "|file_name                                                     |file_size_in_bytes|record_count|bytes_per_record|\n",
      "+--------------------------------------------------------------+------------------+------------+----------------+\n",
      "|00000-839-759a0fd4-25db-41bd-8e21-a70f9cb4dc84-0-00001.parquet|13042             |998         |13.07           |\n",
      "|00000-31-5faa8614-eeb6-445d-b377-644a41656b38-0-00001.parquet |3889              |2           |1944.50         |\n",
      "|00000-10-106f15bf-6293-4ee2-a926-7f4807b22ec3-0-00001.parquet |3847              |3           |1282.33         |\n",
      "|00000-31-5faa8614-eeb6-445d-b377-644a41656b38-0-00002.parquet |3681              |1           |3681.00         |\n",
      "|00000-804-a1e7a35f-d545-4f9d-9c96-2e90163f309c-0-00002.parquet|2627              |7           |375.29          |\n",
      "|00000-65-cc0575ab-588e-4474-ae8e-9ba2720524f5-0-00001.parquet |2442              |1           |2442.00         |\n",
      "+--------------------------------------------------------------+------------------+------------+----------------+\n",
      "\n",
      "\n",
      "Detailed Analysis - Largest File:\n",
      "   File: 00000-839-759a0fd4-25db-41bd-8e21-a70f9cb4dc84-0-00001.parquet\n",
      "   Size: 13,042 bytes\n",
      "   Records: 998\n",
      "   Efficiency: 13.07 bytes/record\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:53:33 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/07/01 13:53:33 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/07/01 13:53:35 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/07/01 13:53:35 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.23.0.6\n",
      "25/07/01 13:53:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Storage Breakdown:\n",
      "   updated_at: 3773 bytes (34.6%)\n",
      "   email: 2698 bytes (24.8%)\n",
      "   username: 2348 bytes (21.6%)\n",
      "   user_id: 1691 bytes (15.5%)\n",
      "   is_active: 163 bytes (1.5%)\n",
      "   created_month: 74 bytes (0.7%)\n",
      "   created_day: 74 bytes (0.7%)\n",
      "   created_year: 73 bytes (0.7%)\n",
      "\n",
      "Data Quality per Column:\n",
      "   user_id: 998 values, 0 nulls (0.0% null rate)\n",
      "   username: 998 values, 0 nulls (0.0% null rate)\n",
      "   email: 998 values, 0 nulls (0.0% null rate)\n",
      "   is_active: 998 values, 0 nulls (0.0% null rate)\n",
      "   created_year: 998 values, 0 nulls (0.0% null rate)\n",
      "   created_month: 998 values, 0 nulls (0.0% null rate)\n",
      "   created_day: 998 values, 0 nulls (0.0% null rate)\n",
      "   updated_at: 998 values, 0 nulls (0.0% null rate)\n",
      "\n",
      "Storage Efficiency Summary:\n",
      "   Total Files: 6\n",
      "   Total Storage: 29,528 bytes (0.028 MB)\n",
      "   Total Records: 1,012\n",
      "   Overall Efficiency: 29.18 bytes/record\n",
      "   Average File Size: 4,921 bytes\n",
      "   File Size Range: 2442 - 13042 bytes\n"
     ]
    }
   ],
   "source": [
    "# Detailed file-level metadata analysis\n",
    "file_metadata = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    SPLIT(file_path, '/')[SIZE(SPLIT(file_path, '/')) - 1] as file_name,\n",
    "    file_size_in_bytes,\n",
    "    record_count,\n",
    "    column_sizes,\n",
    "    value_counts,\n",
    "    null_value_counts,\n",
    "    lower_bounds,\n",
    "    upper_bounds,\n",
    "    ROUND(file_size_in_bytes * 1.0 / record_count, 2) as bytes_per_record\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "ORDER BY file_size_in_bytes DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"File-Level Metadata Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show basic file info\n",
    "file_metadata.select(\n",
    "    \"file_name\", \"file_size_in_bytes\", \"record_count\", \"bytes_per_record\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Analyze the largest file in detail\n",
    "largest_file = file_metadata.collect()[0]\n",
    "print(\"\\nDetailed Analysis - Largest File:\")\n",
    "print(f\"   File: {largest_file['file_name']}\")\n",
    "print(f\"   Size: {largest_file['file_size_in_bytes']:,} bytes\")\n",
    "print(f\"   Records: {largest_file['record_count']:,}\")\n",
    "print(f\"   Efficiency: {largest_file['bytes_per_record']} bytes/record\")\n",
    "\n",
    "users_df = spark.table(\"play_iceberg.users\")\n",
    "\n",
    "# Column size analysis\n",
    "if largest_file['column_sizes']:\n",
    "    print(\"\\nColumn Storage Breakdown:\")\n",
    "    column_sizes = largest_file['column_sizes']\n",
    "    total_column_bytes = sum(column_sizes.values())\n",
    "    \n",
    "    # Get column names for mapping\n",
    "    column_names = users_df.columns\n",
    "    \n",
    "    for col_id, size in sorted(column_sizes.items(), key=lambda x: x[1], reverse=True):\n",
    "        col_name = (\n",
    "            column_names[col_id - 1] \n",
    "            if col_id <= len(column_names) \n",
    "            else f\"column_{col_id}\"\n",
    "        )\n",
    "        percentage = (size / total_column_bytes) * 100\n",
    "        print(f\"   {col_name}: {size} bytes ({percentage:.1f}%)\")\n",
    "\n",
    "# Value and null count analysis\n",
    "if largest_file['value_counts'] and largest_file['null_value_counts']:\n",
    "    print(\"\\nData Quality per Column:\")\n",
    "    value_counts = largest_file['value_counts']\n",
    "    null_counts = largest_file['null_value_counts']\n",
    "    \n",
    "    for col_id in sorted(value_counts.keys()):\n",
    "        col_name = (\n",
    "            column_names[col_id - 1] \n",
    "            if col_id <= len(column_names) \n",
    "            else f\"column_{col_id}\"\n",
    "        )\n",
    "        values = value_counts.get(col_id, 0)\n",
    "        nulls = null_counts.get(col_id, 0)\n",
    "        null_rate = (nulls / values) * 100 if values > 0 else 0\n",
    "        print(f\"   {col_name}: {values} values, {nulls} nulls ({null_rate:.1f}% null rate)\")\n",
    "\n",
    "# Storage efficiency analysis\n",
    "all_files = file_metadata.collect()\n",
    "total_size = sum(f['file_size_in_bytes'] for f in all_files)\n",
    "total_records = sum(f['record_count'] for f in all_files)\n",
    "\n",
    "print(\"\\nStorage Efficiency Summary:\")\n",
    "print(f\"   Total Files: {len(all_files)}\")\n",
    "print(f\"   Total Storage: {total_size:,} bytes ({total_size/1024/1024:.3f} MB)\")\n",
    "print(f\"   Total Records: {total_records:,}\")\n",
    "print(f\"   Overall Efficiency: {total_size/total_records:.2f} bytes/record\")\n",
    "print(f\"   Average File Size: {total_size/len(all_files):,.0f} bytes\")\n",
    "min_file_size = min(f['file_size_in_bytes'] for f in all_files)\n",
    "max_file_size = max(f['file_size_in_bytes'] for f in all_files)\n",
    "print(f\"   File Size Range: {min_file_size} - {max_file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"Cleaning up Spark session...\")\n",
    "spark.stop()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
