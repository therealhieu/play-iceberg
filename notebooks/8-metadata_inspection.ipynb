{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Apache Iceberg Metadata Inspection\n",
    "\n",
    "This comprehensive notebook demonstrates how to inspect and analyze Apache Iceberg table metadata. Understanding Iceberg's metadata structure is crucial for:\n",
    "\n",
    "- **Performance Optimization**: Query planning and partition pruning\n",
    "- **Storage Management**: Understanding file organization and sizes\n",
    "- **Data Evolution**: Tracking schema changes and table history\n",
    "- **Debugging**: Investigating data issues and performance bottlenecks\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "1. **Table History**: Complete timeline of all table snapshots and operations\n",
    "2. **Manifest Files**: Index files that track data file locations and statistics\n",
    "3. **Data Files**: Physical Parquet files containing the actual data\n",
    "4. **File-Level Analysis**: Deep dive into storage patterns and efficiency\n",
    "5. **Column Statistics**: Data distribution and quality insights\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Apache Iceberg table with data (run notebooks 1-7 first)\n",
    "- Spark session configured with Iceberg extensions\n",
    "- REST catalog connection to MinIO storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Initialize our Spark session with Iceberg configuration and verify the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Initialized\n",
      "   Spark Version: 3.5.5\n",
      "   Default Catalog: rest\n",
      "   Iceberg Extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "Users table found and accessible\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, min as spark_min, max as spark_max, sum as spark_sum\n",
    ")\n",
    "\n",
    "# Create Spark session with Iceberg extensions\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Metadata Deep Dive\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Initialized\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Default Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"   Iceberg Extensions: {spark.conf.get('spark.sql.extensions')}\")\n",
    "\n",
    "# Verify table exists\n",
    "try:\n",
    "    table_exists = (\n",
    "        spark.sql(\"SHOW TABLES IN rest.`play-iceberg`\")\n",
    "        .filter(\"tableName = 'users'\")\n",
    "        .count() > 0\n",
    "    )\n",
    "    if table_exists:\n",
    "        print(\"Users table found and accessible\")\n",
    "    else:\n",
    "        print(\"Users table not found - run notebooks 1-7 first\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing catalog: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-section",
   "metadata": {},
   "source": [
    "## 1. Table History Analysis\n",
    "\n",
    "The table history is Iceberg's version control system. Each snapshot represents a consistent state of the table after an operation (INSERT, UPDATE, DELETE, etc.).\n",
    "\n",
    "### Key Concepts:\n",
    "- **Snapshot ID**: Unique identifier for each table version\n",
    "- **Parent ID**: Points to the previous snapshot, forming a lineage chain\n",
    "- **made_current_at**: Timestamp when this snapshot became the current version\n",
    "- **is_current_ancestor**: Whether this snapshot is in the current table's lineage\n",
    "\n",
    "This information is crucial for:\n",
    "- Time travel queries\n",
    "- Understanding table evolution\n",
    "- Debugging data changes\n",
    "- Planning retention policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "history-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table History Timeline:\n",
      "==================================================\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|operation_type|\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "|2025-06-27 05:27:19.364|7177909142708658903|NULL               |true               |INITIAL       |\n",
      "|2025-06-27 05:27:56.364|537167494850070313 |7177909142708658903|true               |UPDATE        |\n",
      "|2025-06-27 05:28:25.466|8898508412055667294|537167494850070313 |true               |UPDATE        |\n",
      "|2025-06-27 05:28:28.65 |1404250745301458840|8898508412055667294|true               |UPDATE        |\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "\n",
      "\n",
      "History Statistics:\n",
      "   Total Snapshots: 4\n",
      "   Current Lineage: 4\n",
      "   Orphaned Snapshots: 0\n",
      "\n",
      "Timeline:\n",
      "   First Snapshot: 2025-06-27 05:27:19.364000\n",
      "   Latest Snapshot: 2025-06-27 05:28:28.650000\n",
      "   Table Age: 3 operations\n"
     ]
    }
   ],
   "source": [
    "# Analyze table history with detailed metrics\n",
    "history_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor,\n",
    "    CASE \n",
    "        WHEN parent_id IS NULL THEN 'INITIAL'\n",
    "        ELSE 'UPDATE'\n",
    "    END as operation_type\n",
    "FROM rest.`play-iceberg`.users.history\n",
    "ORDER BY made_current_at\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table History Timeline:\")\n",
    "print(\"=\" * 50)\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "# Calculate history statistics\n",
    "total_snapshots = history_df.count()\n",
    "current_ancestors = history_df.filter(\"is_current_ancestor = true\").count()\n",
    "orphaned_snapshots = total_snapshots - current_ancestors\n",
    "\n",
    "print(\"\\nHistory Statistics:\")\n",
    "print(f\"   Total Snapshots: {total_snapshots}\")\n",
    "print(f\"   Current Lineage: {current_ancestors}\")\n",
    "print(f\"   Orphaned Snapshots: {orphaned_snapshots}\")\n",
    "\n",
    "if total_snapshots > 1:\n",
    "    first_snapshot = history_df.orderBy(\"made_current_at\").first()\n",
    "    latest_snapshot = history_df.orderBy(col(\"made_current_at\").desc()).first()\n",
    "    \n",
    "    print(\"\\nTimeline:\")\n",
    "    print(f\"   First Snapshot: {first_snapshot['made_current_at']}\")\n",
    "    print(f\"   Latest Snapshot: {latest_snapshot['made_current_at']}\")\n",
    "    print(f\"   Table Age: {total_snapshots - 1} operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manifests-section",
   "metadata": {},
   "source": [
    "## 2. Manifest File Analysis\n",
    "\n",
    "Manifest files are Iceberg's indexing system. They contain metadata about data files and enable efficient query planning.\n",
    "\n",
    "### Understanding Manifests:\n",
    "- **Content Type**: 0 = data files, 1 = delete files\n",
    "- **Partition Spec**: Defines how data is partitioned\n",
    "- **File Counts**: Track additions, deletions, and existing files\n",
    "- **Partition Summaries**: Min/max values for partition pruning\n",
    "\n",
    "### Why This Matters:\n",
    "- Query engines use manifests to skip irrelevant files\n",
    "- Partition summaries enable efficient filtering\n",
    "- File counts help understand table operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "manifests-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest Files Overview:\n",
      "============================================================\n",
      "+------------+-----------------+-------------------+----------------------+-------------------------+----------------+\n",
      "|content_type|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|manifest_size_kb|\n",
      "+------------+-----------------+-------------------+----------------------+-------------------------+----------------+\n",
      "|DATA_FILES  |0                |1404250745301458840|0                     |2                        |8.22            |\n",
      "|DATA_FILES  |1                |1404250745301458840|2                     |0                        |8.34            |\n",
      "+------------+-----------------+-------------------+----------------------+-------------------------+----------------+\n",
      "\n",
      "\n",
      "Manifest Statistics:\n",
      "   Total Manifests: 2\n",
      "   Files per Manifest: 0 - 2\n",
      "   Manifest Sizes: 8.22 - 8.34 KB\n",
      "\n",
      "Partition Summary Example:\n",
      "   Partition Spec ID: 0\n",
      "   Partition 1: 2025 - 2025 (nulls: False)\n",
      "   Partition 2: 6 - 6 (nulls: False)\n",
      "   Partition 3: 27 - 27 (nulls: False)\n"
     ]
    }
   ],
   "source": [
    "# Detailed manifest analysis\n",
    "manifests_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    content,\n",
    "    CASE content \n",
    "        WHEN 0 THEN 'DATA_FILES'\n",
    "        WHEN 1 THEN 'DELETE_FILES'\n",
    "        ELSE 'UNKNOWN'\n",
    "    END as content_type,\n",
    "    partition_spec_id,\n",
    "    added_snapshot_id,\n",
    "    added_data_files_count,\n",
    "    existing_data_files_count,\n",
    "    deleted_data_files_count,\n",
    "    ROUND(length / 1024.0, 2) as manifest_size_kb,\n",
    "    partition_summaries\n",
    "FROM rest.`play-iceberg`.users.manifests\n",
    "ORDER BY added_snapshot_id, partition_spec_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"Manifest Files Overview:\")\n",
    "print(\"=\" * 60)\n",
    "manifests_df.select(\n",
    "    \"content_type\", \"partition_spec_id\", \"added_snapshot_id\", \n",
    "    \"added_data_files_count\", \"existing_data_files_count\", \"manifest_size_kb\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Analyze manifest statistics\n",
    "manifest_stats = manifests_df.agg(\n",
    "    count(\"*\").alias(\"total_manifests\"),\n",
    "    spark_min(\"added_data_files_count\").alias(\"min_files_per_manifest\"),\n",
    "    spark_max(\"added_data_files_count\").alias(\"max_files_per_manifest\"),\n",
    "    spark_min(\"manifest_size_kb\").alias(\"min_manifest_size_kb\"),\n",
    "    spark_max(\"manifest_size_kb\").alias(\"max_manifest_size_kb\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nManifest Statistics:\")\n",
    "print(f\"   Total Manifests: {manifest_stats['total_manifests']}\")\n",
    "print(\n",
    "    f\"   Files per Manifest: {manifest_stats['min_files_per_manifest']} - \"\n",
    "    f\"{manifest_stats['max_files_per_manifest']}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Manifest Sizes: {manifest_stats['min_manifest_size_kb']:.2f} - \"\n",
    "    f\"{manifest_stats['max_manifest_size_kb']:.2f} KB\"\n",
    ")\n",
    "\n",
    "# Show partition summaries for the first manifest\n",
    "first_manifest = manifests_df.filter(\"partition_summaries IS NOT NULL\").first()\n",
    "if first_manifest and first_manifest['partition_summaries']:\n",
    "    print(\"\\nPartition Summary Example:\")\n",
    "    print(f\"   Partition Spec ID: {first_manifest['partition_spec_id']}\")\n",
    "    for i, summary in enumerate(first_manifest['partition_summaries']):\n",
    "        print(\n",
    "            f\"   Partition {i+1}: {summary['lower_bound']} - \"\n",
    "            f\"{summary['upper_bound']} (nulls: {summary['contains_null']})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datafiles-section",
   "metadata": {},
   "source": [
    "## 3. Data Files Deep Dive\n",
    "\n",
    "Data files are the actual Parquet files containing your table data. Understanding their characteristics is essential for performance optimization.\n",
    "\n",
    "### Key Metrics:\n",
    "- **File Size**: Impacts query performance (too small = overhead, too large = slow)\n",
    "- **Record Count**: Number of rows per file\n",
    "- **Compression Ratio**: Storage efficiency\n",
    "- **Partition Layout**: How data is organized on disk\n",
    "\n",
    "### Optimization Insights:\n",
    "- Optimal file sizes: 128MB - 1GB for most workloads\n",
    "- Consistent record counts indicate good partitioning\n",
    "- File paths reveal partition strategy effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "datafiles-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Files Analysis:\n",
      "================================================================================\n",
      "+-----------+------------+------------+----------------+---------------+\n",
      "|file_format|record_count|file_size_mb|bytes_per_record|partition_type |\n",
      "+-----------+------------+------------+----------------+---------------+\n",
      "|PARQUET    |7           |0.003       |390.14          |mixed_partition|\n",
      "|PARQUET    |1           |0.003       |2701.00         |inactive_users |\n",
      "|PARQUET    |1           |0.003       |2686.00         |active_users   |\n",
      "|PARQUET    |1           |0.003       |2677.00         |mixed_partition|\n",
      "+-----------+------------+------------+----------------+---------------+\n",
      "\n",
      "\n",
      "Storage Statistics:\n",
      "   Total Files: 4\n",
      "   Total Records: 10\n",
      "   Total Storage: 10,795 bytes (0.010 MB)\n",
      "   File Size Range: 2677 - 2731 bytes\n",
      "   Records per File: 1 - 7\n",
      "   Average File Size: 2699 bytes\n",
      "   Average Records per File: 2.5\n",
      "\n",
      "Partition Distribution:\n",
      "+---------------+----------+-------------+----------+\n",
      "|partition_type |file_count|total_records|total_size|\n",
      "+---------------+----------+-------------+----------+\n",
      "|inactive_users |1         |1            |2701      |\n",
      "|active_users   |1         |1            |2686      |\n",
      "|mixed_partition|2         |8            |5408      |\n",
      "+---------------+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data files analysis\n",
    "files_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    file_path,\n",
    "    file_format,\n",
    "    record_count,\n",
    "    file_size_in_bytes,\n",
    "    ROUND(file_size_in_bytes / 1024.0 / 1024.0, 3) as file_size_mb,\n",
    "    ROUND(file_size_in_bytes * 1.0 / record_count, 2) as bytes_per_record,\n",
    "    -- Extract partition information from file path\n",
    "    CASE \n",
    "        WHEN file_path LIKE '%is_active=true%' THEN 'active_users'\n",
    "        WHEN file_path LIKE '%is_active=false%' THEN 'inactive_users'\n",
    "        ELSE 'mixed_partition'\n",
    "    END as partition_type\n",
    "FROM rest.`play-iceberg`.users.files\n",
    "ORDER BY file_size_in_bytes DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data Files Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "files_df.select(\n",
    "    \"file_format\", \"record_count\", \"file_size_mb\", \n",
    "    \"bytes_per_record\", \"partition_type\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Calculate storage statistics using Spark SQL functions\n",
    "file_stats = files_df.agg(\n",
    "    count(\"*\").alias(\"total_files\"),\n",
    "    spark_sum(\"record_count\").alias(\"total_records\"),\n",
    "    spark_sum(\"file_size_in_bytes\").alias(\"total_size_bytes\"),\n",
    "    spark_min(\"file_size_in_bytes\").alias(\"min_file_size\"),\n",
    "    spark_max(\"file_size_in_bytes\").alias(\"max_file_size\"),\n",
    "    spark_min(\"record_count\").alias(\"min_records\"),\n",
    "    spark_max(\"record_count\").alias(\"max_records\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nStorage Statistics:\")\n",
    "print(f\"   Total Files: {file_stats['total_files']}\")\n",
    "print(f\"   Total Records: {file_stats['total_records']:,}\")\n",
    "total_size_mb = file_stats['total_size_bytes']/1024/1024\n",
    "print(\n",
    "    f\"   Total Storage: {file_stats['total_size_bytes']:,} bytes \"\n",
    "    f\"({total_size_mb:.3f} MB)\"\n",
    ")\n",
    "print(\n",
    "    f\"   File Size Range: {file_stats['min_file_size']} - \"\n",
    "    f\"{file_stats['max_file_size']} bytes\"\n",
    ")\n",
    "print(\n",
    "    f\"   Records per File: {file_stats['min_records']} - \"\n",
    "    f\"{file_stats['max_records']}\"\n",
    ")\n",
    "avg_file_size = file_stats['total_size_bytes']/file_stats['total_files']\n",
    "print(f\"   Average File Size: {avg_file_size:.0f} bytes\")\n",
    "avg_records = file_stats['total_records']/file_stats['total_files']\n",
    "print(f\"   Average Records per File: {avg_records:.1f}\")\n",
    "\n",
    "# Analyze partition distribution\n",
    "partition_stats = files_df.groupBy(\"partition_type\").agg(\n",
    "    count(\"*\").alias(\"file_count\"),\n",
    "    spark_sum(\"record_count\").alias(\"total_records\"),\n",
    "    spark_sum(\"file_size_in_bytes\").alias(\"total_size\")\n",
    ").orderBy(\"file_count\")\n",
    "\n",
    "print(\"\\nPartition Distribution:\")\n",
    "partition_stats.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file-metadata-section",
   "metadata": {},
   "source": [
    "## 4. File-Level Metadata Deep Dive\n",
    "\n",
    "Iceberg tracks detailed statistics for each data file, enabling efficient query planning and optimization.\n",
    "\n",
    "### File Statistics Include:\n",
    "- **Column Sizes**: Bytes used by each column (compression effectiveness)\n",
    "- **Value Counts**: Number of values per column\n",
    "- **Null Counts**: Null values per column per file\n",
    "- **Bounds**: Min/max values for predicate pushdown\n",
    "\n",
    "### Performance Impact:\n",
    "- Query engines use bounds for file pruning\n",
    "- Column sizes help estimate scan costs\n",
    "- Statistics enable better join ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "file-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-Level Metadata Analysis:\n",
      "==================================================\n",
      "+-------------------------------------------------------------+------------------+------------+----------------+\n",
      "|file_name                                                    |file_size_in_bytes|record_count|bytes_per_record|\n",
      "+-------------------------------------------------------------+------------------+------------+----------------+\n",
      "|00000-30-9b1d49e8-ff68-4fe6-a27e-3452c998c2f3-0-00001.parquet|2731              |7           |390.14          |\n",
      "|00000-5-b897449e-ee70-4899-9ded-977c1e2d3038-0-00002.parquet |2701              |1           |2701.00         |\n",
      "|00000-5-b897449e-ee70-4899-9ded-977c1e2d3038-0-00001.parquet |2686              |1           |2686.00         |\n",
      "|00000-1-59198145-e908-4bd7-80d9-93add1ac6e70-0-00001.parquet |2677              |1           |2677.00         |\n",
      "+-------------------------------------------------------------+------------------+------------+----------------+\n",
      "\n",
      "\n",
      "Detailed Analysis - Largest File:\n",
      "   File: 00000-30-9b1d49e8-ff68-4fe6-a27e-3452c998c2f3-0-00001.parquet\n",
      "   Size: 2,731 bytes\n",
      "   Records: 7\n",
      "   Efficiency: 390.14 bytes/record\n",
      "\n",
      "Column Storage Breakdown:\n",
      "   email: 142 bytes (21.4%)\n",
      "   username: 124 bytes (18.7%)\n",
      "   updated_at: 85 bytes (12.8%)\n",
      "   created_month: 72 bytes (10.8%)\n",
      "   created_day: 72 bytes (10.8%)\n",
      "   created_year: 71 bytes (10.7%)\n",
      "   user_id: 62 bytes (9.3%)\n",
      "   is_active: 36 bytes (5.4%)\n",
      "\n",
      "Data Quality per Column:\n",
      "   user_id: 7 values, 0 nulls (0.0% null rate)\n",
      "   username: 7 values, 0 nulls (0.0% null rate)\n",
      "   email: 7 values, 0 nulls (0.0% null rate)\n",
      "   is_active: 7 values, 0 nulls (0.0% null rate)\n",
      "   created_year: 7 values, 0 nulls (0.0% null rate)\n",
      "   created_month: 7 values, 0 nulls (0.0% null rate)\n",
      "   created_day: 7 values, 0 nulls (0.0% null rate)\n",
      "   updated_at: 7 values, 0 nulls (0.0% null rate)\n",
      "\n",
      "Storage Efficiency Summary:\n",
      "   Total Files: 4\n",
      "   Total Storage: 10,795 bytes (0.010 MB)\n",
      "   Total Records: 10\n",
      "   Overall Efficiency: 1079.50 bytes/record\n",
      "   Average File Size: 2,699 bytes\n",
      "   File Size Range: 2677 - 2731 bytes\n"
     ]
    }
   ],
   "source": [
    "# Detailed file-level metadata analysis\n",
    "file_metadata = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    SPLIT(file_path, '/')[SIZE(SPLIT(file_path, '/')) - 1] as file_name,\n",
    "    file_size_in_bytes,\n",
    "    record_count,\n",
    "    column_sizes,\n",
    "    value_counts,\n",
    "    null_value_counts,\n",
    "    lower_bounds,\n",
    "    upper_bounds,\n",
    "    ROUND(file_size_in_bytes * 1.0 / record_count, 2) as bytes_per_record\n",
    "FROM rest.`play-iceberg`.users.files\n",
    "ORDER BY file_size_in_bytes DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"File-Level Metadata Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show basic file info\n",
    "file_metadata.select(\n",
    "    \"file_name\", \"file_size_in_bytes\", \"record_count\", \"bytes_per_record\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Analyze the largest file in detail\n",
    "largest_file = file_metadata.collect()[0]\n",
    "print(\"\\nDetailed Analysis - Largest File:\")\n",
    "print(f\"   File: {largest_file['file_name']}\")\n",
    "print(f\"   Size: {largest_file['file_size_in_bytes']:,} bytes\")\n",
    "print(f\"   Records: {largest_file['record_count']:,}\")\n",
    "print(f\"   Efficiency: {largest_file['bytes_per_record']} bytes/record\")\n",
    "\n",
    "# Column size analysis\n",
    "if largest_file['column_sizes']:\n",
    "    print(\"\\nColumn Storage Breakdown:\")\n",
    "    column_sizes = largest_file['column_sizes']\n",
    "    total_column_bytes = sum(column_sizes.values())\n",
    "    \n",
    "    # Get column names for mapping\n",
    "    column_names = users_df.columns\n",
    "    \n",
    "    for col_id, size in sorted(column_sizes.items(), key=lambda x: x[1], reverse=True):\n",
    "        col_name = (\n",
    "            column_names[col_id - 1] \n",
    "            if col_id <= len(column_names) \n",
    "            else f\"column_{col_id}\"\n",
    "        )\n",
    "        percentage = (size / total_column_bytes) * 100\n",
    "        print(f\"   {col_name}: {size} bytes ({percentage:.1f}%)\")\n",
    "\n",
    "# Value and null count analysis\n",
    "if largest_file['value_counts'] and largest_file['null_value_counts']:\n",
    "    print(\"\\nData Quality per Column:\")\n",
    "    value_counts = largest_file['value_counts']\n",
    "    null_counts = largest_file['null_value_counts']\n",
    "    \n",
    "    for col_id in sorted(value_counts.keys()):\n",
    "        col_name = (\n",
    "            column_names[col_id - 1] \n",
    "            if col_id <= len(column_names) \n",
    "            else f\"column_{col_id}\"\n",
    "        )\n",
    "        values = value_counts.get(col_id, 0)\n",
    "        nulls = null_counts.get(col_id, 0)\n",
    "        null_rate = (nulls / values) * 100 if values > 0 else 0\n",
    "        print(f\"   {col_name}: {values} values, {nulls} nulls ({null_rate:.1f}% null rate)\")\n",
    "\n",
    "# Storage efficiency analysis\n",
    "all_files = file_metadata.collect()\n",
    "total_size = sum(f['file_size_in_bytes'] for f in all_files)\n",
    "total_records = sum(f['record_count'] for f in all_files)\n",
    "\n",
    "print(\"\\nStorage Efficiency Summary:\")\n",
    "print(f\"   Total Files: {len(all_files)}\")\n",
    "print(f\"   Total Storage: {total_size:,} bytes ({total_size/1024/1024:.3f} MB)\")\n",
    "print(f\"   Total Records: {total_records:,}\")\n",
    "print(f\"   Overall Efficiency: {total_size/total_records:.2f} bytes/record\")\n",
    "print(f\"   Average File Size: {total_size/len(all_files):,.0f} bytes\")\n",
    "min_file_size = min(f['file_size_in_bytes'] for f in all_files)\n",
    "max_file_size = max(f['file_size_in_bytes'] for f in all_files)\n",
    "print(f\"   File Size Range: {min_file_size} - {max_file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "print(\"Cleaning up Spark session...\")\n",
    "spark.stop()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
