{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Apache Iceberg Architecture: Understanding the Three Layers\n",
    "\n",
    "This notebook provides a comprehensive exploration of Apache Iceberg's three-layer architecture. Understanding these layers is crucial for working effectively with Iceberg and optimizing its performance.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Apache Iceberg is built on a three-layer architecture that separates concerns and provides flexibility:\n",
    "\n",
    "### 1. Catalog Layer\n",
    "- **Purpose**: Table discovery and metadata location\n",
    "- **Function**: Maps table names to metadata file locations\n",
    "- **Examples**: Hive Metastore, AWS Glue, REST Catalog, JDBC Catalog\n",
    "\n",
    "### 2. Metadata Layer\n",
    "- **Purpose**: Table schema, partitioning, and file tracking\n",
    "- **Components**: Metadata files, manifest lists, manifest files\n",
    "- **Function**: Enables ACID transactions and time travel\n",
    "\n",
    "### 3. Data Layer\n",
    "- **Purpose**: Actual data storage\n",
    "- **Format**: Parquet, ORC, or Avro files\n",
    "- **Organization**: Partitioned and optimized for query performance\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "1. **Catalog Layer Deep Dive**: How table discovery works\n",
    "2. **Metadata Layer Analysis**: Examine metadata files and manifests\n",
    "3. **Data Layer Inspection**: Understand data file organization\n",
    "4. **Layer Interactions**: How the layers work together\n",
    "5. **Performance Implications**: Impact on query planning and execution\n",
    "6. **Best Practices**: Optimizing each layer\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of data lakes and table formats\n",
    "- Completed previous notebooks to have sample data\n",
    "- Access to Iceberg table with REST catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Initialize our environment and prepare for architecture exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:54:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n",
      "Architecture exploration session initialized\n",
      "Using namespace: rest.play_iceberg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target table 'users' found with 1012 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, size\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Architecture Layers\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Architecture exploration session initialized\")\n",
    "\n",
    "# Set default database\n",
    "spark.sql(\"USE rest.`play_iceberg`\")\n",
    "print(\"Using namespace: rest.play_iceberg\")\n",
    "\n",
    "# Verify table exists\n",
    "try:\n",
    "    table_count = spark.sql(\"SELECT COUNT(*) as count FROM users\").collect()[0]['count']\n",
    "    print(f\"Target table 'users' found with {table_count} records\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing table: {e}\")\n",
    "    print(\"Please run previous notebooks to create sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog-layer-section",
   "metadata": {},
   "source": [
    "## 1. Catalog Layer Deep Dive\n",
    "\n",
    "The catalog layer is the entry point to Iceberg tables. It provides:\n",
    "- **Table Discovery**: Maps table names to metadata locations\n",
    "- **Namespace Management**: Organizes tables into logical groups\n",
    "- **Authentication**: Controls access to tables\n",
    "- **Metadata Location**: Points to the current metadata file\n",
    "\n",
    "### 1.1 Catalog Configuration\n",
    "Let's examine our catalog configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "catalog-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog Configuration:\n",
      "========================================\n",
      "Default Catalog: rest\n",
      "Catalog Type: rest\n",
      "Catalog URI: http://iceberg-rest:8181/\n",
      "Warehouse Location: s3://warehouse/\n",
      "S3 Endpoint: http://minio:9000\n",
      "\n",
      "Catalog Type: REST Catalog\n",
      "- Provides HTTP API for table operations\n",
      "- Centralized metadata management\n",
      "- Supports multiple storage backends\n"
     ]
    }
   ],
   "source": [
    "# Examine Spark catalog configuration\n",
    "print(\"Catalog Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "catalog_configs = {\n",
    "    'Default Catalog': spark.conf.get('spark.sql.defaultCatalog', 'Not set'),\n",
    "    'Catalog Type': spark.conf.get('spark.sql.catalog.rest.type', 'Not set'),\n",
    "    'Catalog URI': spark.conf.get('spark.sql.catalog.rest.uri', 'Not set'),\n",
    "    'Warehouse Location': spark.conf.get('spark.sql.catalog.rest.warehouse', 'Not set'),\n",
    "    'S3 Endpoint': spark.conf.get('spark.sql.catalog.rest.s3.endpoint', 'Not set')\n",
    "}\n",
    "\n",
    "for key, value in catalog_configs.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nCatalog Type: REST Catalog\")\n",
    "print(\"- Provides HTTP API for table operations\")\n",
    "print(\"- Centralized metadata management\")\n",
    "print(\"- Supports multiple storage backends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog-operations-section",
   "metadata": {},
   "source": [
    "### 1.2 Catalog Operations\n",
    "Explore what the catalog layer provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "catalog-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Catalogs:\n",
      "==============================\n",
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|         rest|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "Namespaces in 'rest' catalog:\n",
      "===================================\n",
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|play_iceberg|\n",
      "+------------+\n",
      "\n",
      "Tables in 'rest.play_iceberg' namespace:\n",
      "=============================================\n",
      "+------------+---------+-----------+\n",
      "|   namespace|tableName|isTemporary|\n",
      "+------------+---------+-----------+\n",
      "|play_iceberg|    users|      false|\n",
      "+------------+---------+-----------+\n",
      "\n",
      "Total tables found: 1\n"
     ]
    }
   ],
   "source": [
    "# List all catalogs\n",
    "print(\"Available Catalogs:\")\n",
    "print(\"=\" * 30)\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "# List namespaces in our catalog\n",
    "print(\"Namespaces in 'rest' catalog:\")\n",
    "print(\"=\" * 35)\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES IN rest\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Using direct namespace access\")\n",
    "\n",
    "# List tables in our namespace\n",
    "print(\"Tables in 'rest.play_iceberg' namespace:\")\n",
    "print(\"=\" * 45)\n",
    "tables_df = spark.sql(\"SHOW TABLES IN rest.`play_iceberg`\")\n",
    "tables_df.show()\n",
    "\n",
    "# Get table count\n",
    "table_count = tables_df.count()\n",
    "print(f\"Total tables found: {table_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-metadata-location-section",
   "metadata": {},
   "source": [
    "### 1.3 Table Metadata Location\n",
    "The catalog stores the location of the current metadata file for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "table-metadata-location",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Metadata Information:\n",
      "========================================\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
      "|col_name                    |data_type                                                                                                             |comment                                       |\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
      "|user_id                     |bigint                                                                                                                |NULL                                          |\n",
      "|username                    |string                                                                                                                |NULL                                          |\n",
      "|email                       |string                                                                                                                |NULL                                          |\n",
      "|is_active                   |boolean                                                                                                               |NULL                                          |\n",
      "|created_year                |int                                                                                                                   |NULL                                          |\n",
      "|created_month               |int                                                                                                                   |NULL                                          |\n",
      "|created_day                 |int                                                                                                                   |NULL                                          |\n",
      "|updated_at                  |timestamp_ntz                                                                                                         |NULL                                          |\n",
      "|country                     |string                                                                                                                |NULL                                          |\n",
      "|registration_source         |string                                                                                                                |Source of user registration (web, mobile, api)|\n",
      "|engagement_score            |double                                                                                                                |User engagement score (0-100 scale)           |\n",
      "|last_login_at               |timestamp                                                                                                             |NULL                                          |\n",
      "|                            |                                                                                                                      |                                              |\n",
      "|# Partitioning              |                                                                                                                      |                                              |\n",
      "|Part 0                      |created_year                                                                                                          |                                              |\n",
      "|Part 1                      |created_month                                                                                                         |                                              |\n",
      "|Part 2                      |created_day                                                                                                           |                                              |\n",
      "|Part 3                      |is_active                                                                                                             |                                              |\n",
      "|Part 4                      |bucket(5, country)                                                                                                    |                                              |\n",
      "|                            |                                                                                                                      |                                              |\n",
      "|# Metadata Columns          |                                                                                                                      |                                              |\n",
      "|_spec_id                    |int                                                                                                                   |                                              |\n",
      "|_partition                  |struct<created_year:int,created_month:int,created_day:int,is_active:boolean,country_bucket_5:int>                     |                                              |\n",
      "|_file                       |string                                                                                                                |                                              |\n",
      "|_pos                        |bigint                                                                                                                |                                              |\n",
      "|_deleted                    |boolean                                                                                                               |                                              |\n",
      "|                            |                                                                                                                      |                                              |\n",
      "|# Detailed Table Information|                                                                                                                      |                                              |\n",
      "|Name                        |rest.play_iceberg.users                                                                                               |                                              |\n",
      "|Type                        |MANAGED                                                                                                               |                                              |\n",
      "|Location                    |s3://warehouse/play_iceberg/users                                                                                     |                                              |\n",
      "|Provider                    |iceberg                                                                                                               |                                              |\n",
      "|Table Properties            |[current-snapshot-id=1926503896228031236,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd]|                                              |\n",
      "+----------------------------+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
      "\n",
      "\n",
      "Catalog Layer Functions:\n",
      "1. Maps 'users' table name to metadata file location\n",
      "2. Provides table schema and properties\n",
      "3. Manages table lifecycle (create, drop, rename)\n",
      "4. Handles concurrent access and locking\n"
     ]
    }
   ],
   "source": [
    "# Get detailed table information\n",
    "print(\"Table Metadata Information:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show table details\n",
    "table_details = spark.sql(\"DESCRIBE EXTENDED users\")\n",
    "table_details.show(50, truncate=False)\n",
    "\n",
    "print(\"\\nCatalog Layer Functions:\")\n",
    "print(\"1. Maps 'users' table name to metadata file location\")\n",
    "print(\"2. Provides table schema and properties\")\n",
    "print(\"3. Manages table lifecycle (create, drop, rename)\")\n",
    "print(\"4. Handles concurrent access and locking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-layer-section",
   "metadata": {},
   "source": [
    "## 2. Metadata Layer Deep Dive\n",
    "\n",
    "The metadata layer is the heart of Iceberg's functionality. It consists of:\n",
    "\n",
    "### Metadata Hierarchy:\n",
    "1. **Metadata File** (JSON): Root of metadata tree, contains schema and current snapshot\n",
    "2. **Manifest List** (Avro): Lists all manifest files for a snapshot\n",
    "3. **Manifest Files** (Avro): Lists data files with statistics\n",
    "4. **Data Files** (Parquet/ORC/Avro): Actual data storage\n",
    "\n",
    "### 2.1 Table Metadata Files\n",
    "Let's explore the metadata files structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metadata-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Table Schema and Metadata:\n",
      "=============================================\n",
      "Table Schema:\n",
      "root\n",
      " |-- user_id: long (nullable = false)\n",
      " |-- username: string (nullable = false)\n",
      " |-- email: string (nullable = false)\n",
      " |-- is_active: boolean (nullable = false)\n",
      " |-- created_year: integer (nullable = false)\n",
      " |-- created_month: integer (nullable = false)\n",
      " |-- created_day: integer (nullable = false)\n",
      " |-- updated_at: timestamp_ntz (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- registration_source: string (nullable = true)\n",
      " |-- engagement_score: double (nullable = true)\n",
      " |-- last_login_at: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Table Properties from Metadata Layer:\n",
      "+--------------------------+----------+--------------------+\n",
      "|query_time                |table_name|current_record_count|\n",
      "+--------------------------+----------+--------------------+\n",
      "|2025-07-01 13:54:14.663151|users     |1012                |\n",
      "+--------------------------+----------+--------------------+\n",
      "\n",
      "Partitioning Strategy (from metadata):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----------+--------------------+\n",
      "|created_year|created_month|created_day|records_in_partition|\n",
      "+------------+-------------+-----------+--------------------+\n",
      "|        2025|            6|         27|                1002|\n",
      "|        2025|            6|         28|                   3|\n",
      "|        2025|            7|          1|                   7|\n",
      "+------------+-------------+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get current table metadata\n",
    "print(\"Current Table Schema and Metadata:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Show table schema\n",
    "users_df = spark.table(\"users\")\n",
    "print(\"Table Schema:\")\n",
    "users_df.printSchema()\n",
    "\n",
    "# Get table properties\n",
    "print(\"\\nTable Properties from Metadata Layer:\")\n",
    "properties = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    current_timestamp() as query_time,\n",
    "    'users' as table_name,\n",
    "    COUNT(*) as current_record_count\n",
    "FROM users\n",
    "\"\"\")\n",
    "properties.show(truncate=False)\n",
    "\n",
    "# Show partitioning information\n",
    "print(\"Partitioning Strategy (from metadata):\")\n",
    "partition_info = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    created_year,\n",
    "    created_month, \n",
    "    created_day,\n",
    "    COUNT(*) as records_in_partition\n",
    "FROM users \n",
    "GROUP BY created_year, created_month, created_day\n",
    "ORDER BY created_year, created_month, created_day\n",
    "\"\"\")\n",
    "partition_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snapshots-section",
   "metadata": {},
   "source": [
    "### 2.2 Snapshots and History\n",
    "Each change to the table creates a new snapshot in the metadata layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "snapshots",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Snapshots (Metadata Layer History):\n",
      "==================================================\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-07-01 13:41:23.087|3824926247776142618|NULL               |true               |\n",
      "|2025-07-01 13:46:22.566|3682366184055111348|3824926247776142618|true               |\n",
      "|2025-07-01 13:46:29.92 |5474445044599496672|3682366184055111348|true               |\n",
      "|2025-07-01 13:46:43.276|703749108260326071 |5474445044599496672|true               |\n",
      "|2025-07-01 13:46:51.537|1680807781020980082|703749108260326071 |true               |\n",
      "|2025-07-01 13:47:01.334|8866811446601788626|1680807781020980082|true               |\n",
      "|2025-07-01 13:48:29.944|5929459840228022626|8866811446601788626|true               |\n",
      "|2025-07-01 13:48:37.868|1926503896228031236|5929459840228022626|true               |\n",
      "|2025-07-01 13:50:45.161|7274330971587238251|1926503896228031236|false              |\n",
      "|2025-07-01 13:50:46.609|841024702459819741 |7274330971587238251|false              |\n",
      "|2025-07-01 13:50:48.992|1926503896228031236|5929459840228022626|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Snapshot Analysis:\n",
      "Total Snapshots: 11\n",
      "Current Lineage: 9\n",
      "Orphaned Snapshots: 2\n",
      "\n",
      "Metadata Layer Benefits:\n",
      "- ACID transactions through atomic metadata updates\n",
      "- Time travel via snapshot history\n",
      "- Schema evolution without data rewrite\n",
      "- Concurrent readers during writes\n"
     ]
    }
   ],
   "source": [
    "# Examine table snapshots\n",
    "print(\"Table Snapshots (Metadata Layer History):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "snapshots_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor\n",
    "FROM rest.`play_iceberg`.users.history\n",
    "ORDER BY made_current_at\n",
    "\"\"\")\n",
    "\n",
    "snapshots_df.show(truncate=False)\n",
    "\n",
    "# Count snapshots\n",
    "snapshot_count = snapshots_df.count()\n",
    "current_snapshots = snapshots_df.filter(\"is_current_ancestor = true\").count()\n",
    "orphaned_snapshots = snapshot_count - current_snapshots\n",
    "\n",
    "print(\"\\nSnapshot Analysis:\")\n",
    "print(f\"Total Snapshots: {snapshot_count}\")\n",
    "print(f\"Current Lineage: {current_snapshots}\")\n",
    "print(f\"Orphaned Snapshots: {orphaned_snapshots}\")\n",
    "\n",
    "print(\"\\nMetadata Layer Benefits:\")\n",
    "print(\"- ACID transactions through atomic metadata updates\")\n",
    "print(\"- Time travel via snapshot history\")\n",
    "print(\"- Schema evolution without data rewrite\")\n",
    "print(\"- Concurrent readers during writes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manifests-section",
   "metadata": {},
   "source": [
    "### 2.3 Manifest Files\n",
    "Manifest files are the index that maps partitions to data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "manifests",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest Files (Metadata Layer Index):\n",
      "==================================================\n",
      "+-------+------------+-----------------+-------------------+----------------------+-------------------------+------------------------+----------------+\n",
      "|content|content_type|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|deleted_data_files_count|manifest_size_kb|\n",
      "+-------+------------+-----------------+-------------------+----------------------+-------------------------+------------------------+----------------+\n",
      "|0      |DATA_FILES  |0                |1926503896228031236|0                     |4                        |0                       |8.86            |\n",
      "|0      |DATA_FILES  |1                |1926503896228031236|2                     |0                        |0                       |8.76            |\n",
      "+-------+------------+-----------------+-------------------+----------------------+-------------------------+------------------------+----------------+\n",
      "\n",
      "\n",
      "Manifest Statistics:\n",
      "+---------------+------------------------+-------------------+--------------------+\n",
      "|total_manifests|total_data_files_tracked|total_deleted_files|avg_manifest_size_kb|\n",
      "+---------------+------------------------+-------------------+--------------------+\n",
      "|              2|                       2|                  0|                8.81|\n",
      "+---------------+------------------------+-------------------+--------------------+\n",
      "\n",
      "Manifest File Functions:\n",
      "- Index data files by partition\n",
      "- Store file-level statistics (row counts, column bounds)\n",
      "- Enable partition pruning and file skipping\n",
      "- Support efficient query planning\n"
     ]
    }
   ],
   "source": [
    "# Examine manifest files\n",
    "print(\"Manifest Files (Metadata Layer Index):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "manifests_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    content,\n",
    "    CASE content\n",
    "        WHEN 0 THEN 'DATA_FILES'\n",
    "        WHEN 1 THEN 'DELETE_FILES'\n",
    "        ELSE 'UNKNOWN'\n",
    "    END as content_type,\n",
    "    partition_spec_id,\n",
    "    added_snapshot_id,\n",
    "    added_data_files_count,\n",
    "    existing_data_files_count,\n",
    "    deleted_data_files_count,\n",
    "    ROUND(length / 1024.0, 2) as manifest_size_kb\n",
    "FROM rest.`play_iceberg`.users.manifests\n",
    "ORDER BY added_snapshot_id, partition_spec_id\n",
    "\"\"\")\n",
    "\n",
    "manifests_df.show(truncate=False)\n",
    "\n",
    "# Manifest statistics\n",
    "manifest_stats = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_manifests,\n",
    "    SUM(added_data_files_count) as total_data_files_tracked,\n",
    "    SUM(deleted_data_files_count) as total_deleted_files,\n",
    "    ROUND(AVG(length / 1024.0), 2) as avg_manifest_size_kb\n",
    "FROM rest.`play_iceberg`.users.manifests\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nManifest Statistics:\")\n",
    "manifest_stats.show()\n",
    "\n",
    "print(\"Manifest File Functions:\")\n",
    "print(\"- Index data files by partition\")\n",
    "print(\"- Store file-level statistics (row counts, column bounds)\")\n",
    "print(\"- Enable partition pruning and file skipping\")\n",
    "print(\"- Support efficient query planning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partition-summaries-section",
   "metadata": {},
   "source": [
    "### 2.4 Partition Summaries\n",
    "Manifests contain partition summaries that enable efficient query pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partition-summaries",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Summaries (Query Optimization Metadata):\n",
      "============================================================\n",
      "Partition Spec ID: 1\n",
      "Snapshot ID: 1926503896228031236\n",
      "\n",
      "Partition Summary Details:\n",
      "  Partition 1:\n",
      "    Lower Bound: 2025\n",
      "    Upper Bound: 2025\n",
      "    Contains Null: False\n",
      "    Contains NaN: False\n",
      "\n",
      "  Partition 2:\n",
      "    Lower Bound: 6\n",
      "    Upper Bound: 6\n",
      "    Contains Null: False\n",
      "    Contains NaN: False\n",
      "\n",
      "  Partition 3:\n",
      "    Lower Bound: 28\n",
      "    Upper Bound: 28\n",
      "    Contains Null: False\n",
      "    Contains NaN: False\n",
      "\n",
      "  Partition 4:\n",
      "    Lower Bound: false\n",
      "    Upper Bound: true\n",
      "    Contains Null: False\n",
      "    Contains NaN: False\n",
      "\n",
      "Partition Summary Benefits:\n",
      "- Enable partition pruning during query planning\n",
      "- Reduce data scan requirements\n",
      "- Support predicate pushdown optimizations\n",
      "- Improve query performance significantly\n"
     ]
    }
   ],
   "source": [
    "# Examine partition summaries\n",
    "print(\"Partition Summaries (Query Optimization Metadata):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get manifest with partition summaries\n",
    "manifest_with_summaries = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    partition_spec_id,\n",
    "    added_snapshot_id,\n",
    "    partition_summaries\n",
    "FROM rest.`play_iceberg`.users.manifests\n",
    "WHERE partition_summaries IS NOT NULL\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# Show partition summary structure\n",
    "summaries = manifest_with_summaries.collect()\n",
    "if summaries:\n",
    "    summary = summaries[0]\n",
    "    print(f\"Partition Spec ID: {summary['partition_spec_id']}\")\n",
    "    print(f\"Snapshot ID: {summary['added_snapshot_id']}\")\n",
    "    print(\"\\nPartition Summary Details:\")\n",
    "    \n",
    "    if summary['partition_summaries']:\n",
    "        for i, part_summary in enumerate(summary['partition_summaries']):\n",
    "            print(f\"  Partition {i+1}:\")\n",
    "            print(f\"    Lower Bound: {part_summary['lower_bound']}\")\n",
    "            print(f\"    Upper Bound: {part_summary['upper_bound']}\")\n",
    "            print(f\"    Contains Null: {part_summary['contains_null']}\")\n",
    "            print(f\"    Contains NaN: {part_summary['contains_nan']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No partition summaries found\")\n",
    "\n",
    "print(\"Partition Summary Benefits:\")\n",
    "print(\"- Enable partition pruning during query planning\")\n",
    "print(\"- Reduce data scan requirements\")\n",
    "print(\"- Support predicate pushdown optimizations\")\n",
    "print(\"- Improve query performance significantly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-layer-section",
   "metadata": {},
   "source": [
    "## 3. Data Layer Deep Dive\n",
    "\n",
    "The data layer contains the actual table data stored in optimized file formats.\n",
    "\n",
    "### Data Layer Characteristics:\n",
    "- **File Formats**: Parquet (default), ORC, Avro\n",
    "- **Partitioning**: Physical data organization\n",
    "- **Compression**: Storage optimization\n",
    "- **Statistics**: Column-level min/max values\n",
    "\n",
    "### 3.1 Data File Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "data-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Layer File Organization:\n",
      "========================================\n",
      "+-----------+------------+------------+----------------+\n",
      "|file_format|record_count|file_size_mb|bytes_per_record|\n",
      "+-----------+------------+------------+----------------+\n",
      "|PARQUET    |998         |0.012       |13.07           |\n",
      "|PARQUET    |2           |0.004       |1944.50         |\n",
      "|PARQUET    |3           |0.004       |1282.33         |\n",
      "|PARQUET    |1           |0.004       |3681.00         |\n",
      "|PARQUET    |7           |0.003       |375.29          |\n",
      "|PARQUET    |1           |0.002       |2442.00         |\n",
      "+-----------+------------+------------+----------------+\n",
      "\n",
      "\n",
      "Data Layer Statistics:\n",
      "+-----------+-------------+----------------+-------------+-------------------+--------------------+\n",
      "|total_files|total_records|total_size_bytes|total_size_mb|avg_file_size_bytes|avg_records_per_file|\n",
      "+-----------+-------------+----------------+-------------+-------------------+--------------------+\n",
      "|6          |1012         |29528           |0.028        |4921.0             |168.7               |\n",
      "+-----------+-------------+----------------+-------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine data files\n",
    "print(\"Data Layer File Organization:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "data_files_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    file_path,\n",
    "    file_format,\n",
    "    record_count,\n",
    "    file_size_in_bytes,\n",
    "    ROUND(file_size_in_bytes / 1024.0 / 1024.0, 3) as file_size_mb,\n",
    "    ROUND(file_size_in_bytes * 1.0 / record_count, 2) as bytes_per_record\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "ORDER BY file_size_in_bytes DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show file overview\n",
    "data_files_df.select(\n",
    "    \"file_format\", \"record_count\", \"file_size_mb\", \"bytes_per_record\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Data layer statistics\n",
    "data_stats = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_files,\n",
    "    SUM(record_count) as total_records,\n",
    "    SUM(file_size_in_bytes) as total_size_bytes,\n",
    "    ROUND(SUM(file_size_in_bytes) / 1024.0 / 1024.0, 3) as total_size_mb,\n",
    "    ROUND(AVG(file_size_in_bytes), 0) as avg_file_size_bytes,\n",
    "    ROUND(AVG(record_count), 1) as avg_records_per_file\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nData Layer Statistics:\")\n",
    "data_stats.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partition-layout-section",
   "metadata": {},
   "source": [
    "### 3.2 Partition Layout Analysis\n",
    "Examine how data is physically organized by partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "partition-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Partition Layout:\n",
      "===================================\n",
      "+--------------+------------------+----------+-------------+-------------+\n",
      "|date_partition|activity_partition|file_count|total_records|total_size_kb|\n",
      "+--------------+------------------+----------+-------------+-------------+\n",
      "|2025/06/27    |MIXED             |3         |1002         |18.88        |\n",
      "|OTHER         |ACTIVE            |1         |2            |3.80         |\n",
      "|OTHER         |INACTIVE          |1         |1            |3.59         |\n",
      "|OTHER         |MIXED             |1         |7            |2.57         |\n",
      "+--------------+------------------+----------+-------------+-------------+\n",
      "\n",
      "\n",
      "Partition Benefits:\n",
      "- Enables partition pruning during queries\n",
      "- Improves data locality and scan performance\n",
      "- Supports efficient data lifecycle management\n",
      "- Reduces I/O for time-based queries\n"
     ]
    }
   ],
   "source": [
    "# Analyze partition layout from file paths\n",
    "print(\"Physical Partition Layout:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Extract partition information from file paths\n",
    "partition_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    file_path,\n",
    "    record_count,\n",
    "    file_size_in_bytes,\n",
    "    CASE \n",
    "        WHEN file_path LIKE '%created_year=2025/created_month=6/created_day=27%' THEN '2025/06/27'\n",
    "        ELSE 'OTHER'\n",
    "    END as partition_path,\n",
    "    CASE\n",
    "        WHEN file_path LIKE '%is_active=true%' THEN 'ACTIVE'\n",
    "        WHEN file_path LIKE '%is_active=false%' THEN 'INACTIVE'\n",
    "        ELSE 'MIXED'\n",
    "    END as activity_partition\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "\"\"\")\n",
    "\n",
    "# Show partition distribution\n",
    "partition_summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN file_path LIKE '%created_year=2025/created_month=6/created_day=27%' THEN '2025/06/27'\n",
    "        ELSE 'OTHER'\n",
    "    END as date_partition,\n",
    "    CASE\n",
    "        WHEN file_path LIKE '%is_active=true%' THEN 'ACTIVE'\n",
    "        WHEN file_path LIKE '%is_active=false%' THEN 'INACTIVE'\n",
    "        ELSE 'MIXED'\n",
    "    END as activity_partition,\n",
    "    COUNT(*) as file_count,\n",
    "    SUM(record_count) as total_records,\n",
    "    ROUND(SUM(file_size_in_bytes) / 1024.0, 2) as total_size_kb\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN file_path LIKE '%created_year=2025/created_month=6/created_day=27%' THEN '2025/06/27'\n",
    "        ELSE 'OTHER'\n",
    "    END,\n",
    "    CASE\n",
    "        WHEN file_path LIKE '%is_active=true%' THEN 'ACTIVE'\n",
    "        WHEN file_path LIKE '%is_active=false%' THEN 'INACTIVE'\n",
    "        ELSE 'MIXED'\n",
    "    END\n",
    "ORDER BY date_partition, activity_partition\n",
    "\"\"\")\n",
    "\n",
    "partition_summary.show(truncate=False)\n",
    "\n",
    "print(\"\\nPartition Benefits:\")\n",
    "print(\"- Enables partition pruning during queries\")\n",
    "print(\"- Improves data locality and scan performance\")\n",
    "print(\"- Supports efficient data lifecycle management\")\n",
    "print(\"- Reduces I/O for time-based queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file-statistics-section",
   "metadata": {},
   "source": [
    "### 3.3 File-Level Statistics\n",
    "Each data file includes detailed statistics stored in the metadata layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "file-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File-Level Statistics (for Query Optimization):\n",
      "=======================================================\n",
      "+--------------------------------------------------------------+------------+------------------+\n",
      "|file_name                                                     |record_count|file_size_in_bytes|\n",
      "+--------------------------------------------------------------+------------+------------------+\n",
      "|00000-839-759a0fd4-25db-41bd-8e21-a70f9cb4dc84-0-00001.parquet|998         |13042             |\n",
      "+--------------------------------------------------------------+------------+------------------+\n",
      "\n",
      "\n",
      "Detailed Statistics for: 00000-839-759a0fd4-25db-41bd-8e21-a70f9cb4dc84-0-00001.parquet\n",
      "============================================================\n",
      "Column Storage Sizes (bytes):\n",
      "  updated_at: 3773 bytes\n",
      "  email: 2698 bytes\n",
      "  username: 2348 bytes\n",
      "  user_id: 1691 bytes\n",
      "  is_active: 163 bytes\n",
      "  created_month: 74 bytes\n",
      "  created_day: 74 bytes\n",
      "  created_year: 73 bytes\n",
      "\n",
      "Value Counts per Column:\n",
      "  user_id: 998 values, 0 nulls\n",
      "  username: 998 values, 0 nulls\n",
      "  email: 998 values, 0 nulls\n",
      "  is_active: 998 values, 0 nulls\n",
      "  created_year: 998 values, 0 nulls\n",
      "  created_month: 998 values, 0 nulls\n",
      "  created_day: 998 values, 0 nulls\n",
      "  updated_at: 998 values, 0 nulls\n",
      "\n",
      "Statistics Usage:\n",
      "- Enable predicate pushdown and file skipping\n",
      "- Support bloom filter creation\n",
      "- Optimize join ordering and execution\n",
      "- Provide cost estimates for query planning\n"
     ]
    }
   ],
   "source": [
    "# Examine file-level statistics\n",
    "print(\"File-Level Statistics (for Query Optimization):\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Get detailed file statistics\n",
    "file_stats_detailed = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    SPLIT(file_path, '/')[SIZE(SPLIT(file_path, '/')) - 1] as file_name,\n",
    "    record_count,\n",
    "    file_size_in_bytes,\n",
    "    column_sizes,\n",
    "    value_counts,\n",
    "    null_value_counts,\n",
    "    lower_bounds,\n",
    "    upper_bounds\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "ORDER BY file_size_in_bytes DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# Show basic file info\n",
    "basic_info = file_stats_detailed.select(\"file_name\", \"record_count\", \"file_size_in_bytes\")\n",
    "basic_info.show(truncate=False)\n",
    "\n",
    "# Analyze statistics from the largest file\n",
    "largest_file_stats = file_stats_detailed.collect()[0]\n",
    "\n",
    "print(f\"\\nDetailed Statistics for: {largest_file_stats['file_name']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Column sizes (storage footprint per column)\n",
    "if largest_file_stats['column_sizes']:\n",
    "    print(\"Column Storage Sizes (bytes):\")\n",
    "    column_sizes = largest_file_stats['column_sizes']\n",
    "    # Map column IDs to names\n",
    "    column_names = spark.table(\"users\").columns\n",
    "    \n",
    "    for col_id, size in sorted(column_sizes.items(), key=lambda x: x[1], reverse=True):\n",
    "        col_name = column_names[col_id - 1] if col_id <= len(column_names) else f\"column_{col_id}\"\n",
    "        print(f\"  {col_name}: {size} bytes\")\n",
    "\n",
    "# Value counts\n",
    "if largest_file_stats['value_counts']:\n",
    "    print(\"\\nValue Counts per Column:\")\n",
    "    value_counts = largest_file_stats['value_counts']\n",
    "    null_counts = largest_file_stats['null_value_counts'] or {}\n",
    "    \n",
    "    for col_id, count in value_counts.items():\n",
    "        col_name = column_names[col_id - 1] if col_id <= len(column_names) else f\"column_{col_id}\"\n",
    "        nulls = null_counts.get(col_id, 0)\n",
    "        print(f\"  {col_name}: {count} values, {nulls} nulls\")\n",
    "\n",
    "print(\"\\nStatistics Usage:\")\n",
    "print(\"- Enable predicate pushdown and file skipping\")\n",
    "print(\"- Support bloom filter creation\")\n",
    "print(\"- Optimize join ordering and execution\")\n",
    "print(\"- Provide cost estimates for query planning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layer-interactions-section",
   "metadata": {},
   "source": [
    "## 4. Layer Interactions and Query Flow\n",
    "\n",
    "Let's trace how a query flows through all three layers.\n",
    "\n",
    "### 4.1 Query Planning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "query-flow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Flow Through Iceberg Layers:\n",
      "=============================================\n",
      "Example Query: SELECT COUNT(*) as active_users FROM users WHERE is_active = true\n",
      "\n",
      "Step 1: CATALOG LAYER\n",
      "- Resolve 'users' table name to metadata location\n",
      "- Authenticate and authorize access\n",
      "- Return current metadata file pointer\n",
      "\n",
      "Step 2: METADATA LAYER\n",
      "- Read current snapshot metadata\n",
      "- Get manifest list for current snapshot\n",
      "- Read relevant manifest files\n",
      "- Apply partition pruning using partition summaries\n",
      "- Filter data files based on predicates (is_active = true)\n",
      "\n",
      "Step 3: DATA LAYER\n",
      "- Read identified Parquet files\n",
      "- Apply column pruning and predicate pushdown\n",
      "- Use file statistics for further optimization\n",
      "- Execute query on actual data\n",
      "\n",
      "Query Result:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|active_users|\n",
      "+------------+\n",
      "|         510|\n",
      "+------------+\n",
      "\n",
      "\n",
      "Query Execution Plan (simplified):\n",
      "==================================================\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['COUNT(1) AS active_users#790]\n",
      "+- 'Filter ('is_active = true)\n",
      "   +- 'UnresolvedRelation [users], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "active_users: bigint\n",
      "Aggregate [count(1) AS active_users#790L]\n",
      "+- Filter (is_active#794 = true)\n",
      "   +- SubqueryAlias rest.play_iceberg.users\n",
      "      +- RelationV2[user_id#791L, username#792, email#793, is_active#794, created_year#795, created_month#796, created_day#797, updated_at#798, country#799, registration_source#800, engagement_score#801, last_login_at#802] rest.play_iceberg.users rest.play_iceberg.users\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [count(1) AS active_users#790L]\n",
      "+- Project\n",
      "   +- Filter is_active#794: boolean\n",
      "      +- RelationV2[is_active#794] rest.play_iceberg.users\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(1)], output=[active_users#790L])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=520]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#815L])\n",
      "         +- Project\n",
      "            +- Filter is_active#794: boolean\n",
      "               +- BatchScan rest.play_iceberg.users[is_active#794] rest.play_iceberg.users (branch=null) [filters=is_active = true, groupedBy=] RuntimeFilters: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Demonstrate query flow through layers\n",
    "print(\"Query Flow Through Iceberg Layers:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Example query: Find active users\n",
    "query = \"SELECT COUNT(*) as active_users FROM users WHERE is_active = true\"\n",
    "print(f\"Example Query: {query}\")\n",
    "print()\n",
    "\n",
    "print(\"Step 1: CATALOG LAYER\")\n",
    "print(\"- Resolve 'users' table name to metadata location\")\n",
    "print(\"- Authenticate and authorize access\")\n",
    "print(\"- Return current metadata file pointer\")\n",
    "print()\n",
    "\n",
    "print(\"Step 2: METADATA LAYER\")\n",
    "print(\"- Read current snapshot metadata\")\n",
    "print(\"- Get manifest list for current snapshot\")\n",
    "print(\"- Read relevant manifest files\")\n",
    "print(\"- Apply partition pruning using partition summaries\")\n",
    "print(\"- Filter data files based on predicates (is_active = true)\")\n",
    "print()\n",
    "\n",
    "print(\"Step 3: DATA LAYER\")\n",
    "print(\"- Read identified Parquet files\")\n",
    "print(\"- Apply column pruning and predicate pushdown\")\n",
    "print(\"- Use file statistics for further optimization\")\n",
    "print(\"- Execute query on actual data\")\n",
    "print()\n",
    "\n",
    "# Execute the example query\n",
    "result = spark.sql(query)\n",
    "print(\"Query Result:\")\n",
    "result.show()\n",
    "\n",
    "# Show query execution plan\n",
    "print(\"\\nQuery Execution Plan (simplified):\")\n",
    "print(\"=\" * 50)\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-analysis-section",
   "metadata": {},
   "source": [
    "### 4.2 Performance Impact Analysis\n",
    "Understand how each layer affects query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "performance-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Impact by Layer:\n",
      "===================================\n",
      "+--------------+----------------+-------------------------+----------------------------------+\n",
      "|layer         |operation       |complexity               |bottleneck                        |\n",
      "+--------------+----------------+-------------------------+----------------------------------+\n",
      "|CATALOG_LAYER |Table Resolution|O(1)                     |Network latency to catalog service|\n",
      "|METADATA_LAYER|File Planning   |O(manifests * partitions)|Number of manifest files          |\n",
      "|DATA_LAYER    |Data Scanning   |O(data_files * file_size)|File count and size               |\n",
      "+--------------+----------------+-------------------------+----------------------------------+\n",
      "\n",
      "\n",
      "Actual Metrics for 'users' Table:\n",
      "========================================\n",
      "Metadata Layer Metrics:\n",
      "+--------------+----------------+----------------------+\n",
      "|manifest_files|total_data_files|total_manifest_size_kb|\n",
      "+--------------+----------------+----------------------+\n",
      "|             2|               2|                 17.62|\n",
      "+--------------+----------------+----------------------+\n",
      "\n",
      "Data Layer Metrics:\n",
      "+----------+-------------+------------------+----------------+\n",
      "|data_files|total_records|total_data_size_mb|avg_file_size_kb|\n",
      "+----------+-------------+------------------+----------------+\n",
      "|         6|         1012|              0.03|            4.81|\n",
      "+----------+-------------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze performance characteristics of each layer\n",
    "print(\"Performance Impact by Layer:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Catalog layer performance\n",
    "catalog_perf = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    'CATALOG_LAYER' as layer,\n",
    "    'Table Resolution' as operation,\n",
    "    'O(1)' as complexity,\n",
    "    'Network latency to catalog service' as bottleneck\n",
    "\"\"\")\n",
    "\n",
    "# Metadata layer performance\n",
    "metadata_perf = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    'METADATA_LAYER' as layer,\n",
    "    'File Planning' as operation,\n",
    "    'O(manifests * partitions)' as complexity,\n",
    "    'Number of manifest files' as bottleneck\n",
    "\"\"\")\n",
    "\n",
    "# Data layer performance\n",
    "data_perf = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    'DATA_LAYER' as layer,\n",
    "    'Data Scanning' as operation,\n",
    "    'O(data_files * file_size)' as complexity,\n",
    "    'File count and size' as bottleneck\n",
    "\"\"\")\n",
    "\n",
    "# Combine performance analysis\n",
    "performance_df = catalog_perf.union(metadata_perf).union(data_perf)\n",
    "performance_df.show(truncate=False)\n",
    "\n",
    "# Get actual metrics for our table\n",
    "print(\"\\nActual Metrics for 'users' Table:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Metadata layer metrics\n",
    "metadata_metrics = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as manifest_files,\n",
    "    SUM(added_data_files_count) as total_data_files,\n",
    "    ROUND(SUM(length) / 1024.0, 2) as total_manifest_size_kb\n",
    "FROM rest.`play_iceberg`.users.manifests\n",
    "\"\"\")\n",
    "\n",
    "print(\"Metadata Layer Metrics:\")\n",
    "metadata_metrics.show()\n",
    "\n",
    "# Data layer metrics\n",
    "data_metrics = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as data_files,\n",
    "    SUM(record_count) as total_records,\n",
    "    ROUND(SUM(file_size_in_bytes) / 1024.0 / 1024.0, 2) as total_data_size_mb,\n",
    "    ROUND(AVG(file_size_in_bytes) / 1024.0, 2) as avg_file_size_kb\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data Layer Metrics:\")\n",
    "data_metrics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimization-section",
   "metadata": {},
   "source": [
    "## 5. Layer-Specific Optimizations\n",
    "\n",
    "Each layer has specific optimization strategies.\n",
    "\n",
    "### 5.1 Catalog Layer Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "catalog-optimizations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog Layer Optimization Strategies:\n",
      "==================================================\n",
      "1. Connection Pooling: Reuse catalog connections\n",
      "2. Caching: Cache table metadata locations\n",
      "3. Batch Operations: Group multiple table operations\n",
      "4. Geographic Distribution: Use regional catalog endpoints\n",
      "5. Authentication Optimization: Use service accounts with long-lived tokens\n",
      "\n",
      "Current Catalog Configuration Review:\n",
      "=============================================\n",
      "Catalog Response: SUCCESS\n",
      "Table Resolution: FAST\n",
      "Recommendation: Current catalog configuration is performing well\n"
     ]
    }
   ],
   "source": [
    "# Catalog layer optimization strategies\n",
    "print(\"Catalog Layer Optimization Strategies:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "catalog_optimizations = [\n",
    "    \"1. Connection Pooling: Reuse catalog connections\",\n",
    "    \"2. Caching: Cache table metadata locations\",\n",
    "    \"3. Batch Operations: Group multiple table operations\",\n",
    "    \"4. Geographic Distribution: Use regional catalog endpoints\",\n",
    "    \"5. Authentication Optimization: Use service accounts with long-lived tokens\"\n",
    "]\n",
    "\n",
    "for optimization in catalog_optimizations:\n",
    "    print(optimization)\n",
    "\n",
    "# Check current catalog settings\n",
    "print(\"\\nCurrent Catalog Configuration Review:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Simulate catalog health check\n",
    "try:\n",
    "    start_time = spark.sql(\"SELECT current_timestamp() as start_time\").collect()[0]['start_time']\n",
    "    table_check = spark.sql(\"SELECT COUNT(*) as count FROM users LIMIT 1\").collect()[0]['count']\n",
    "    end_time = spark.sql(\"SELECT current_timestamp() as end_time\").collect()[0]['end_time']\n",
    "    \n",
    "    print(\"Catalog Response: SUCCESS\")\n",
    "    print(\"Table Resolution: FAST\")\n",
    "    print(\"Recommendation: Current catalog configuration is performing well\")\n",
    "except Exception as e:\n",
    "    print(f\"Catalog Issue: {e}\")\n",
    "    print(\"Recommendation: Check catalog connectivity and authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-optimizations-section",
   "metadata": {},
   "source": [
    "### 5.2 Metadata Layer Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "metadata-optimizations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata Layer Optimization Analysis:\n",
      "==================================================\n",
      "Manifest Health Check:\n",
      "+--------------+----------------------+----------------------+----------------------+--------------------+\n",
      "|manifest_count|avg_files_per_manifest|max_files_per_manifest|min_files_per_manifest|avg_manifest_size_kb|\n",
      "+--------------+----------------------+----------------------+----------------------+--------------------+\n",
      "|             1|                   2.0|                     2|                     2|                8.76|\n",
      "+--------------+----------------------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Metadata Layer Recommendations:\n",
      "========================================\n",
      "GOOD: Manifest count is healthy\n",
      "WARNING: Low files per manifest ratio\n",
      "Recommendation: Increase target file size or batch writes\n",
      "\n",
      "Metadata Optimization Strategies:\n",
      "1. Manifest Compaction: Merge small manifests\n",
      "2. Partition Strategy: Align with query patterns\n",
      "3. File Sizing: Target 128MB-1GB per file\n",
      "4. Snapshot Cleanup: Remove old snapshots regularly\n",
      "5. Statistics Refresh: Update column statistics\n"
     ]
    }
   ],
   "source": [
    "# Metadata layer optimization analysis\n",
    "print(\"Metadata Layer Optimization Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze manifest file health\n",
    "manifest_health = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as manifest_count,\n",
    "    AVG(added_data_files_count) as avg_files_per_manifest,\n",
    "    MAX(added_data_files_count) as max_files_per_manifest,\n",
    "    MIN(added_data_files_count) as min_files_per_manifest,\n",
    "    ROUND(AVG(length / 1024.0), 2) as avg_manifest_size_kb\n",
    "FROM rest.`play_iceberg`.users.manifests\n",
    "WHERE added_data_files_count > 0\n",
    "\"\"\")\n",
    "\n",
    "print(\"Manifest Health Check:\")\n",
    "manifest_health.show()\n",
    "\n",
    "# Get manifest statistics for recommendations\n",
    "manifest_stats = manifest_health.collect()[0]\n",
    "manifest_count = manifest_stats['manifest_count']\n",
    "avg_files_per_manifest = manifest_stats['avg_files_per_manifest']\n",
    "\n",
    "print(\"\\nMetadata Layer Recommendations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if manifest_count > 20:\n",
    "    print(\"WARNING: High manifest count detected\")\n",
    "    print(\"Recommendation: Consider manifest compaction\")\n",
    "elif manifest_count > 10:\n",
    "    print(\"CAUTION: Moderate manifest count\")\n",
    "    print(\"Recommendation: Monitor manifest growth\")\n",
    "else:\n",
    "    print(\"GOOD: Manifest count is healthy\")\n",
    "\n",
    "if avg_files_per_manifest and avg_files_per_manifest < 10:\n",
    "    print(\"WARNING: Low files per manifest ratio\")\n",
    "    print(\"Recommendation: Increase target file size or batch writes\")\n",
    "else:\n",
    "    print(\"GOOD: Files per manifest ratio is healthy\")\n",
    "\n",
    "print(\"\\nMetadata Optimization Strategies:\")\n",
    "print(\"1. Manifest Compaction: Merge small manifests\")\n",
    "print(\"2. Partition Strategy: Align with query patterns\")\n",
    "print(\"3. File Sizing: Target 128MB-1GB per file\")\n",
    "print(\"4. Snapshot Cleanup: Remove old snapshots regularly\")\n",
    "print(\"5. Statistics Refresh: Update column statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-optimizations-section",
   "metadata": {},
   "source": [
    "### 5.3 Data Layer Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "data-optimizations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Layer Optimization Analysis:\n",
      "=============================================\n",
      "File Size Distribution:\n",
      "+-----------+-------------+-------------+-------------+----------------+---------------+-----------------+\n",
      "|total_files|min_file_size|max_file_size|avg_file_size|stddev_file_size|small_files_1mb|large_files_128mb|\n",
      "+-----------+-------------+-------------+-------------+----------------+---------------+-----------------+\n",
      "|6          |2442         |13042        |4921.0       |4028.0          |6              |0                |\n",
      "+-----------+-------------+-------------+-------------+----------------+---------------+-----------------+\n",
      "\n",
      "\n",
      "Data Layer Health Assessment:\n",
      "========================================\n",
      "WARNING: 6 small files (<1MB) detected\n",
      "Recommendation: Run compaction to merge small files\n",
      "GOOD: No excessively large files\n",
      "WARNING: Average file size (0.0MB) is very small\n",
      "Recommendation: Increase write batch size or run compaction\n",
      "\n",
      "Data Layer Optimization Strategies:\n",
      "1. File Compaction: Merge small files regularly\n",
      "2. Partition Tuning: Align partitions with query patterns\n",
      "3. Compression: Use appropriate compression codecs\n",
      "4. Column Layout: Optimize for read patterns\n",
      "5. File Format: Consider Parquet optimizations (bloom filters, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Data layer optimization analysis\n",
    "print(\"Data Layer Optimization Analysis:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Analyze file size distribution\n",
    "file_size_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_files,\n",
    "    MIN(file_size_in_bytes) as min_file_size,\n",
    "    MAX(file_size_in_bytes) as max_file_size,\n",
    "    ROUND(AVG(file_size_in_bytes), 0) as avg_file_size,\n",
    "    ROUND(STDDEV(file_size_in_bytes), 0) as stddev_file_size,\n",
    "    SUM(CASE WHEN file_size_in_bytes < 1024*1024 THEN 1 ELSE 0 END) as small_files_1mb,\n",
    "    SUM(CASE WHEN file_size_in_bytes > 128*1024*1024 THEN 1 ELSE 0 END) as large_files_128mb\n",
    "FROM rest.`play_iceberg`.users.files\n",
    "\"\"\")\n",
    "\n",
    "print(\"File Size Distribution:\")\n",
    "file_size_analysis.show(truncate=False)\n",
    "\n",
    "# Get file size statistics for recommendations\n",
    "file_stats = file_size_analysis.collect()[0]\n",
    "total_files = file_stats['total_files']\n",
    "small_files = file_stats['small_files_1mb']\n",
    "large_files = file_stats['large_files_128mb']\n",
    "avg_file_size = file_stats['avg_file_size']\n",
    "\n",
    "print(\"\\nData Layer Health Assessment:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Small files check\n",
    "if small_files > total_files * 0.3:\n",
    "    print(f\"WARNING: {small_files} small files (<1MB) detected\")\n",
    "    print(\"Recommendation: Run compaction to merge small files\")\n",
    "elif small_files > 0:\n",
    "    print(f\"CAUTION: {small_files} small files detected\")\n",
    "    print(\"Recommendation: Monitor file growth patterns\")\n",
    "else:\n",
    "    print(\"GOOD: No small files detected\")\n",
    "\n",
    "# Large files check\n",
    "if large_files > 0:\n",
    "    print(f\"WARNING: {large_files} large files (>128MB) detected\")\n",
    "    print(\"Recommendation: Consider better partitioning or file splitting\")\n",
    "else:\n",
    "    print(\"GOOD: No excessively large files\")\n",
    "\n",
    "# Average file size assessment\n",
    "optimal_size = 64 * 1024 * 1024  # 64MB target\n",
    "if avg_file_size < optimal_size * 0.1:\n",
    "    print(f\"WARNING: Average file size ({avg_file_size/1024/1024:.1f}MB) is very small\")\n",
    "    print(\"Recommendation: Increase write batch size or run compaction\")\n",
    "elif avg_file_size > optimal_size * 4:\n",
    "    print(f\"WARNING: Average file size ({avg_file_size/1024/1024:.1f}MB) is very large\")\n",
    "    print(\"Recommendation: Improve partitioning strategy\")\n",
    "else:\n",
    "    print(f\"GOOD: Average file size ({avg_file_size/1024/1024:.1f}MB) is reasonable\")\n",
    "\n",
    "print(\"\\nData Layer Optimization Strategies:\")\n",
    "print(\"1. File Compaction: Merge small files regularly\")\n",
    "print(\"2. Partition Tuning: Align partitions with query patterns\")\n",
    "print(\"3. Compression: Use appropriate compression codecs\")\n",
    "print(\"4. Column Layout: Optimize for read patterns\")\n",
    "print(\"5. File Format: Consider Parquet optimizations (bloom filters, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-concepts-section",
   "metadata": {},
   "source": [
    "## 6. Advanced Architecture Concepts\n",
    "\n",
    "### 6.1 Concurrent Operations and ACID Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "concurrent-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACID Properties in Iceberg's Three-Layer Architecture:\n",
      "============================================================\n",
      "\n",
      "ATOMICITY:\n",
      "- Catalog Layer: Atomic pointer updates to metadata files\n",
      "- Metadata Layer: All-or-nothing snapshot creation\n",
      "- Data Layer: Immutable files ensure consistency\n",
      "\n",
      "CONSISTENCY:\n",
      "- Catalog Layer: Validates table schema constraints\n",
      "- Metadata Layer: Maintains referential integrity\n",
      "- Data Layer: Schema evolution without data rewrite\n",
      "\n",
      "ISOLATION:\n",
      "- Catalog Layer: Concurrent readers see consistent snapshots\n",
      "- Metadata Layer: MVCC through snapshot isolation\n",
      "- Data Layer: Immutable files prevent read/write conflicts\n",
      "\n",
      "DURABILITY:\n",
      "- Catalog Layer: Persisted metadata locations\n",
      "- Metadata Layer: Immutable manifest and metadata files\n",
      "- Data Layer: Durable storage with replication\n",
      "\n",
      "\n",
      "Current Table State (Transaction View):\n",
      "==================================================\n",
      "Current Snapshot ID: 1926503896228031236\n",
      "All concurrent readers will see this exact snapshot until a new transaction commits.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate ACID properties through the three-layer architecture\n",
    "print(\"ACID Properties in Iceberg's Three-Layer Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nATOMICITY:\")\n",
    "print(\"- Catalog Layer: Atomic pointer updates to metadata files\")\n",
    "print(\"- Metadata Layer: All-or-nothing snapshot creation\")\n",
    "print(\"- Data Layer: Immutable files ensure consistency\")\n",
    "\n",
    "print(\"\\nCONSISTENCY:\")\n",
    "print(\"- Catalog Layer: Validates table schema constraints\")\n",
    "print(\"- Metadata Layer: Maintains referential integrity\")\n",
    "print(\"- Data Layer: Schema evolution without data rewrite\")\n",
    "\n",
    "print(\"\\nISOLATION:\")\n",
    "print(\"- Catalog Layer: Concurrent readers see consistent snapshots\")\n",
    "print(\"- Metadata Layer: MVCC through snapshot isolation\")\n",
    "print(\"- Data Layer: Immutable files prevent read/write conflicts\")\n",
    "\n",
    "print(\"\\nDURABILITY:\")\n",
    "print(\"- Catalog Layer: Persisted metadata locations\")\n",
    "print(\"- Metadata Layer: Immutable manifest and metadata files\")\n",
    "print(\"- Data Layer: Durable storage with replication\")\n",
    "\n",
    "# Show current transaction isolation\n",
    "print(\"\\n\\nCurrent Table State (Transaction View):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Multiple concurrent queries see the same snapshot\n",
    "snapshot_id = spark.sql(\"\"\"\n",
    "SELECT snapshot_id \n",
    "FROM rest.`play_iceberg`.users.history \n",
    "WHERE is_current_ancestor = true \n",
    "ORDER BY made_current_at DESC \n",
    "LIMIT 1\n",
    "\"\"\").collect()[0]['snapshot_id']\n",
    "\n",
    "print(f\"Current Snapshot ID: {snapshot_id}\")\n",
    "print(\"All concurrent readers will see this exact snapshot until a new transaction commits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-section",
   "metadata": {},
   "source": [
    "### 6.2 Scaling Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "scaling-characteristics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Characteristics by Layer:\n",
      "========================================\n",
      "\n",
      "CATALOG LAYER:\n",
      "  Bottleneck: Network latency and catalog service capacity\n",
      "  Scaling Strategy: Horizontal catalog service scaling, caching, regional distribution\n",
      "  Complexity: O(1) per table operation\n",
      "\n",
      "METADATA LAYER:\n",
      "  Bottleneck: Manifest file count and size\n",
      "  Scaling Strategy: Manifest compaction, partition pruning, manifest caching\n",
      "  Complexity: O(manifest_files × partitions_scanned)\n",
      "\n",
      "DATA LAYER:\n",
      "  Bottleneck: Data volume and file count\n",
      "  Scaling Strategy: Horizontal processing, file compaction, columnar storage\n",
      "  Complexity: O(data_files_scanned × file_size)\n",
      "\n",
      "\n",
      "Scaling Projection for Current Table:\n",
      "=============================================\n",
      "Current Scale:\n",
      "  Records: 1,012\n",
      "  Data Files: 6\n",
      "  Manifest Files: 2\n",
      "\n",
      "Projected Scale (100x growth):\n",
      "  Records: 101,200\n",
      "  Data Files: 600\n",
      "  Manifest Files: 20\n",
      "\n",
      "Scaling Recommendations:\n"
     ]
    }
   ],
   "source": [
    "# Analyze scaling characteristics of each layer\n",
    "print(\"Scaling Characteristics by Layer:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create scaling analysis\n",
    "scaling_analysis = [\n",
    "    {\n",
    "        'layer': 'CATALOG',\n",
    "        'bottleneck': 'Network latency and catalog service capacity',\n",
    "        'scaling_strategy': 'Horizontal catalog service scaling, caching, regional distribution',\n",
    "        'complexity': 'O(1) per table operation'\n",
    "    },\n",
    "    {\n",
    "        'layer': 'METADATA', \n",
    "        'bottleneck': 'Manifest file count and size',\n",
    "        'scaling_strategy': 'Manifest compaction, partition pruning, manifest caching',\n",
    "        'complexity': 'O(manifest_files × partitions_scanned)'\n",
    "    },\n",
    "    {\n",
    "        'layer': 'DATA',\n",
    "        'bottleneck': 'Data volume and file count',\n",
    "        'scaling_strategy': 'Horizontal processing, file compaction, columnar storage',\n",
    "        'complexity': 'O(data_files_scanned × file_size)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for analysis in scaling_analysis:\n",
    "    print(f\"\\n{analysis['layer']} LAYER:\")\n",
    "    print(f\"  Bottleneck: {analysis['bottleneck']}\")\n",
    "    print(f\"  Scaling Strategy: {analysis['scaling_strategy']}\")\n",
    "    print(f\"  Complexity: {analysis['complexity']}\")\n",
    "\n",
    "# Project scaling needs based on current metrics\n",
    "print(\"\\n\\nScaling Projection for Current Table:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "current_files = spark.sql(\"SELECT COUNT(*) as count FROM rest.`play_iceberg`.users.files\").collect()[0]['count']\n",
    "current_manifests = spark.sql(\"SELECT COUNT(*) as count FROM rest.`play_iceberg`.users.manifests\").collect()[0]['count']\n",
    "current_records = spark.sql(\"SELECT COUNT(*) as count FROM users\").collect()[0]['count']\n",
    "\n",
    "print(\"Current Scale:\")\n",
    "print(f\"  Records: {current_records:,}\")\n",
    "print(f\"  Data Files: {current_files}\")\n",
    "print(f\"  Manifest Files: {current_manifests}\")\n",
    "\n",
    "# Project 100x growth\n",
    "projected_records = current_records * 100\n",
    "projected_files = current_files * 100\n",
    "projected_manifests = current_manifests * 10  # Manifests grow slower due to compaction\n",
    "\n",
    "print(\"\\nProjected Scale (100x growth):\")\n",
    "print(f\"  Records: {projected_records:,}\")\n",
    "print(f\"  Data Files: {projected_files}\")\n",
    "print(f\"  Manifest Files: {projected_manifests}\")\n",
    "\n",
    "print(\"\\nScaling Recommendations:\")\n",
    "if projected_files > 10000:\n",
    "    print(\"- Implement automated compaction\")\n",
    "if projected_manifests > 100:\n",
    "    print(\"- Plan manifest compaction strategy\")\n",
    "if projected_records > 1000000:\n",
    "    print(\"- Consider partition strategy optimization\")\n",
    "    print(\"- Implement tiered storage for old data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting-section",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting by Layer\n",
    "\n",
    "Understanding which layer is causing issues helps with faster problem resolution.\n",
    "\n",
    "### 7.1 Layer-Specific Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "layer-diagnostics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-Specific Diagnostic Checks:\n",
      "========================================\n",
      "\n",
      "1. CATALOG LAYER DIAGNOSTICS:\n",
      "===================================\n",
      "✓ Catalog connectivity: OK (1 tables found)\n",
      "✓ Table resolution: OK (19 columns)\n",
      "\n",
      "2. METADATA LAYER DIAGNOSTICS:\n",
      "===================================\n",
      "✓ Metadata access: OK (11 snapshots)\n",
      "✓ Manifest access: OK (2 manifests)\n",
      "✓ File tracking: OK (6 files tracked)\n",
      "\n",
      "3. DATA LAYER DIAGNOSTICS:\n",
      "==============================\n",
      "✓ Data access: OK (1012 records)\n",
      "✓ Data integrity: OK (sample record retrieved)\n",
      "\n",
      "Common Issue Patterns:\n",
      "- Catalog errors: Authentication, network, service availability\n",
      "- Metadata errors: File corruption, permission issues, storage problems\n",
      "- Data errors: File not found, storage issues, format problems\n"
     ]
    }
   ],
   "source": [
    "# Layer-specific diagnostic checks\n",
    "print(\"Layer-Specific Diagnostic Checks:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Catalog layer diagnostics\n",
    "print(\"\\n1. CATALOG LAYER DIAGNOSTICS:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Test catalog connectivity\n",
    "    catalog_test = spark.sql(\"SHOW TABLES IN rest.`play_iceberg`\")\n",
    "    table_count = catalog_test.count()\n",
    "    print(f\"✓ Catalog connectivity: OK ({table_count} tables found)\")\n",
    "    \n",
    "    # Test table resolution\n",
    "    table_desc = spark.sql(\"DESCRIBE TABLE users\")\n",
    "    column_count = table_desc.count()\n",
    "    print(f\"✓ Table resolution: OK ({column_count} columns)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Catalog issue: {e}\")\n",
    "    print(\"  Check: Network connectivity, authentication, catalog service status\")\n",
    "\n",
    "# Metadata layer diagnostics\n",
    "print(\"\\n2. METADATA LAYER DIAGNOSTICS:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    # Test metadata access\n",
    "    history_check = spark.sql(\"SELECT COUNT(*) as count FROM rest.`play_iceberg`.users.history\")\n",
    "    snapshot_count = history_check.collect()[0]['count']\n",
    "    print(f\"✓ Metadata access: OK ({snapshot_count} snapshots)\")\n",
    "    \n",
    "    # Test manifest access\n",
    "    manifest_check = spark.sql(\"SELECT COUNT(*) as count FROM rest.`play_iceberg`.users.manifests\")\n",
    "    manifest_count = manifest_check.collect()[0]['count']\n",
    "    print(f\"✓ Manifest access: OK ({manifest_count} manifests)\")\n",
    "    \n",
    "    # Check for metadata consistency\n",
    "    files_check = spark.sql(\"SELECT COUNT(*) as count FROM rest.`play_iceberg`.users.files\")\n",
    "    file_count = files_check.collect()[0]['count']\n",
    "    print(f\"✓ File tracking: OK ({file_count} files tracked)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Metadata issue: {e}\")\n",
    "    print(\"  Check: Storage accessibility, metadata file corruption, permissions\")\n",
    "\n",
    "# Data layer diagnostics\n",
    "print(\"\\n3. DATA LAYER DIAGNOSTICS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Test data access\n",
    "    data_check = spark.sql(\"SELECT COUNT(*) as count FROM users\")\n",
    "    record_count = data_check.collect()[0]['count']\n",
    "    print(f\"✓ Data access: OK ({record_count} records)\")\n",
    "    \n",
    "    # Test data quality\n",
    "    sample_check = spark.sql(\"SELECT * FROM users LIMIT 1\")\n",
    "    sample_data = sample_check.collect()\n",
    "    if sample_data:\n",
    "        print(\"✓ Data integrity: OK (sample record retrieved)\")\n",
    "    else:\n",
    "        print(\"⚠ Data integrity: No records found\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Data layer issue: {e}\")\n",
    "    print(\"  Check: Storage connectivity, file permissions, data file corruption\")\n",
    "\n",
    "print(\"\\nCommon Issue Patterns:\")\n",
    "print(\"- Catalog errors: Authentication, network, service availability\")\n",
    "print(\"- Metadata errors: File corruption, permission issues, storage problems\")\n",
    "print(\"- Data errors: File not found, storage issues, format problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive exploration of Apache Iceberg's three-layer architecture revealed:\n",
    "\n",
    "### Key Architectural Insights:\n",
    "\n",
    "#### 1. **Catalog Layer**\n",
    "- **Role**: Table discovery and metadata location management\n",
    "- **Benefits**: Centralized table registry, authentication, namespace organization\n",
    "- **Optimization**: Connection pooling, caching, geographic distribution\n",
    "- **Scaling**: Horizontal service scaling, regional replication\n",
    "\n",
    "#### 2. **Metadata Layer**\n",
    "- **Role**: Schema management, transaction coordination, file tracking\n",
    "- **Benefits**: ACID transactions, time travel, schema evolution, query optimization\n",
    "- **Optimization**: Manifest compaction, partition pruning, statistics maintenance\n",
    "- **Scaling**: Manifest management, partition strategy, snapshot cleanup\n",
    "\n",
    "#### 3. **Data Layer**\n",
    "- **Role**: Actual data storage in optimized file formats\n",
    "- **Benefits**: Columnar storage, compression, partition pruning, predicate pushdown\n",
    "- **Optimization**: File sizing, compaction, partition alignment, compression tuning\n",
    "- **Scaling**: Horizontal processing, file organization, storage tiering\n",
    "\n",
    "### Architecture Benefits:\n",
    "\n",
    "1. **Separation of Concerns**: Each layer has distinct responsibilities\n",
    "2. **Independent Scaling**: Layers can be optimized independently\n",
    "3. **Flexibility**: Multiple implementations for each layer\n",
    "4. **Performance**: Optimized for different access patterns\n",
    "5. **Reliability**: Fault isolation and recovery at each layer\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Monitor All Layers**: Each layer has different performance characteristics\n",
    "2. **Optimize by Usage**: Align optimizations with query patterns\n",
    "3. **Plan for Scale**: Different layers scale differently\n",
    "4. **Troubleshoot Systematically**: Isolate issues by layer\n",
    "5. **Maintain Regularly**: Each layer needs different maintenance strategies\n",
    "\n",
    "Understanding these three layers is essential for:\n",
    "- Effective performance tuning\n",
    "- Proper troubleshooting\n",
    "- Successful production deployments\n",
    "- Optimal cost management\n",
    "\n",
    "This layered architecture is what makes Apache Iceberg a powerful, scalable, and reliable table format for modern data lakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture exploration completed!\n",
      "\n",
      "Key takeaway: Understanding the three layers is crucial for:\n",
      "- Performance optimization\n",
      "- Effective troubleshooting\n",
      "- Successful production operations\n",
      "\n",
      "Spark session cleanup...\n",
      "Session closed.\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "print(\"Architecture exploration completed!\")\n",
    "print(\"\\nKey takeaway: Understanding the three layers is crucial for:\")\n",
    "print(\"- Performance optimization\")\n",
    "print(\"- Effective troubleshooting\")\n",
    "print(\"- Successful production operations\")\n",
    "print(\"\\nSpark session cleanup...\")\n",
    "spark.stop()\n",
    "print(\"Session closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
