{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Insert Data with Spark - Iceberg\n\n## Objectives\n\n- Insert data into Iceberg tables using Spark DataFrame API\n- Demonstrate schema-aware data insertion with automatic validation\n- Show partition-aware writes for optimal performance\n- Implement data verification and quality checks\n- Understand Spark-Iceberg integration patterns",
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "25/07/02 08:11:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created DataFrame with 5 records\n",
                        "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
                        "|user_id|     username|               email|is_active|created_year|created_month|created_day|          updated_at|\n",
                        "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
                        "|      1|     john_doe|john.doe@example.com|     true|        2025|            7|          2|2025-07-02 08:11:...|\n",
                        "|      2|   jane_smith|jane.smith@exampl...|     true|        2025|            7|          2|2025-07-02 08:11:...|\n",
                        "|      3| alice_wonder|alice.wonder@exam...|    false|        2025|            7|          2|2025-07-02 08:11:...|\n",
                        "|      4|  bob_builder|bob.builder@examp...|     true|        2025|            7|          2|2025-07-02 08:11:...|\n",
                        "|      5|charlie_brown|charlie.brown@exa...|     true|        2025|            7|          2|2025-07-02 08:11:...|\n",
                        "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from datetime import datetime, timezone\n",
                "\n",
                "# Create Spark session\n",
                "spark = SparkSession.builder.appName(\"IcebergInsert\").getOrCreate()\n",
                "\n",
                "# Sample user data\n",
                "current_time = datetime.now(timezone.utc)\n",
                "sample_users_data = [\n",
                "    (1, \"john_doe\", \"john.doe@example.com\", True, current_time.year, current_time.month, current_time.day, current_time),\n",
                "    (2, \"jane_smith\", \"jane.smith@example.com\", True, current_time.year, current_time.month, current_time.day, current_time),\n",
                "    (3, \"alice_wonder\", \"alice.wonder@example.com\", False, current_time.year, current_time.month, current_time.day, current_time),\n",
                "    (4, \"bob_builder\", \"bob.builder@example.com\", True, current_time.year, current_time.month, current_time.day, current_time),\n",
                "    (5, \"charlie_brown\", \"charlie.brown@example.com\", True, current_time.year, current_time.month, current_time.day, current_time)\n",
                "]\n",
                "\n",
                "# Create DataFrame with schema derived from existing table\n",
                "table_schema = spark.table(\"rest.play_iceberg.users\").schema\n",
                "users_df = spark.createDataFrame(sample_users_data, schema=table_schema)\n",
                "\n",
                "print(f\"Created DataFrame with {users_df.count()} records\")\n",
                "users_df.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Records before insertion: 0\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Records after insertion: 5\n",
                        "Records inserted: 5\n",
                        "\n",
                        "All records in table:\n",
                        "+-------+-------------+-------------------------+---------+------------+-------------+-----------+--------------------------+\n",
                        "|user_id|username     |email                    |is_active|created_year|created_month|created_day|updated_at                |\n",
                        "+-------+-------------+-------------------------+---------+------------+-------------+-----------+--------------------------+\n",
                        "|1      |john_doe     |john.doe@example.com     |true     |2025        |7            |2          |2025-07-02 08:11:32.958859|\n",
                        "|2      |jane_smith   |jane.smith@example.com   |true     |2025        |7            |2          |2025-07-02 08:11:32.958859|\n",
                        "|3      |alice_wonder |alice.wonder@example.com |false    |2025        |7            |2          |2025-07-02 08:11:32.958859|\n",
                        "|4      |bob_builder  |bob.builder@example.com  |true     |2025        |7            |2          |2025-07-02 08:11:32.958859|\n",
                        "|5      |charlie_brown|charlie.brown@example.com|true     |2025        |7            |2          |2025-07-02 08:11:32.958859|\n",
                        "+-------+-------------+-------------------------+---------+------------+-------------+-----------+--------------------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Insert data into Iceberg table\n",
                "before_count = spark.sql(\"SELECT COUNT(*) as count FROM rest.play_iceberg.users\").collect()[0]['count']\n",
                "print(f\"Records before insertion: {before_count}\")\n",
                "\n",
                "# Perform insertion using optimized DataFrame API  \n",
                "users_df.writeTo(\"rest.play_iceberg.users\").append()\n",
                "\n",
                "after_count = spark.sql(\"SELECT COUNT(*) as count FROM rest.play_iceberg.users\").collect()[0]['count']\n",
                "print(f\"Records after insertion: {after_count}\")\n",
                "print(f\"Records inserted: {after_count - before_count}\")\n",
                "\n",
                "# Verify insertion\n",
                "print(\"\\nAll records in table:\")\n",
                "spark.sql(\"SELECT * FROM rest.play_iceberg.users ORDER BY user_id\").show(truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "source": "## Future Steps\n\n### Immediate Next Actions:\n1. **Query Operations**: Read and analyze inserted data (→ Notebook 3)\n2. **Update Operations**: Modify existing records with upserts (→ Notebook 4)\n3. **Schema Evolution**: Add new columns to existing data (→ Notebook 5)\n4. **Time Travel**: Query historical versions of data (→ Notebook 6)\n\n### Production Enhancements:\n- **Error Handling**: Implement robust error handling and retry logic\n- **Data Validation**: Add business rule validation before insertion\n- **Monitoring**: Track insertion metrics and performance\n- **Batch Processing**: Handle large-scale data ingestion efficiently\n- **Idempotency**: Ensure safe re-execution of insertion operations\n\n### Advanced Features:\n- **Streaming Ingestion**: Real-time data insertion with Spark Streaming\n- **Dynamic Partitioning**: Automatic partition creation for new date ranges\n- **Merge Operations**: Complex upsert patterns with conflict resolution\n- **Data Compaction**: Optimize storage with background compaction jobs",
            "metadata": {},
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}