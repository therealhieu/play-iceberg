{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Creating an Apache Iceberg Table\n\n## Objectives\n\n- Connect to an Iceberg REST catalog and manage namespaces\n- Define table schemas with proper data types and field IDs\n- Implement effective partitioning strategies for query performance\n- Create production-ready tables with best practices\n- Understand Iceberg metadata management and table structure",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyIceberg libraries imported successfully\n",
      "Ready to create Iceberg table\n"
     ]
    }
   ],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import (\n",
    "    NestedField,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    BooleanType,\n",
    "    TimestampType,\n",
    "    IntegerType\n",
    ")\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.transforms import IdentityTransform\n",
    "from pyiceberg.exceptions import TableAlreadyExistsError, NamespaceAlreadyExistsError\n",
    "\n",
    "print(\"PyIceberg libraries imported successfully\")\n",
    "print(\"Ready to create Iceberg table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "catalog-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog connected successfully!\n",
      "Existing namespaces: [('play_iceberg',)]\n"
     ]
    }
   ],
   "source": [
    "# Configure the REST catalog with MinIO backend\n",
    "catalog_config = {\n",
    "    \"uri\": \"http://localhost:8181\",\n",
    "    \"s3.endpoint\": \"http://localhost:9000\",\n",
    "    \"s3.access-key-id\": \"admin\",\n",
    "    \"s3.secret-access-key\": \"password\",\n",
    "    \"s3.path-style-access\": \"true\",\n",
    "}\n",
    "\n",
    "# Load the catalog\n",
    "try:\n",
    "    catalog = load_catalog(\"rest\", **catalog_config)\n",
    "    print(\"Catalog connected successfully!\")\n",
    "    \n",
    "    # List existing namespaces\n",
    "    namespaces = list(catalog.list_namespaces())\n",
    "    print(f\"Existing namespaces: {namespaces}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to catalog: {e}\")\n",
    "    print(\"Please ensure Docker services are running\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "namespace-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating namespace: play_iceberg\n",
      "Namespaces provide logical organization for tables\n",
      "They support hierarchical organization and access control\n",
      "Namespace 'play_iceberg' already exists - continuing\n",
      "Current namespaces: [('play_iceberg',)]\n"
     ]
    }
   ],
   "source": [
    "# Define namespace for our user management system\n",
    "namespace = \"play_iceberg\"\n",
    "\n",
    "print(f\"Creating namespace: {namespace}\")\n",
    "print(\"Namespaces provide logical organization for tables\")\n",
    "print(\"They support hierarchical organization and access control\")\n",
    "\n",
    "try:\n",
    "    catalog.create_namespace(namespace)\n",
    "    print(f\"Namespace '{namespace}' created successfully!\")\n",
    "except NamespaceAlreadyExistsError:\n",
    "    print(f\"Namespace '{namespace}' already exists - continuing\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating namespace: {e}\")\n",
    "    \n",
    "# Verify namespace creation\n",
    "updated_namespaces = list(catalog.list_namespaces())\n",
    "print(f\"Current namespaces: {updated_namespaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "schema-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining user table schema...\n",
      "\n",
      "Schema Design Considerations:\n",
      "- user_id: Primary key, long type for large scale\n",
      "- username/email: String types for text data\n",
      "- is_active: Boolean for efficient filtering\n",
      "- created_*: Integer fields for partition keys\n",
      "- updated_at: Timestamp for audit trails\n",
      "\n",
      "Schema defined with 8 fields\n",
      "Field IDs assigned: 1-8 (leaving gaps for future evolution)\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the User table\n",
    "print(\"Defining user table schema...\")\n",
    "print(\"\\nSchema Design Considerations:\")\n",
    "print(\"- user_id: Primary key, long type for large scale\")\n",
    "print(\"- username/email: String types for text data\")\n",
    "print(\"- is_active: Boolean for efficient filtering\")\n",
    "print(\"- created_*: Integer fields for partition keys\")\n",
    "print(\"- updated_at: Timestamp for audit trails\")\n",
    "\n",
    "user_schema = Schema(\n",
    "    # Primary identifier\n",
    "    NestedField(\n",
    "        field_id=1, \n",
    "        name=\"user_id\", \n",
    "        field_type=LongType(), \n",
    "        required=True\n",
    "    ),\n",
    "    \n",
    "    # User profile information\n",
    "    NestedField(\n",
    "        field_id=2, \n",
    "        name=\"username\", \n",
    "        field_type=StringType(), \n",
    "        required=True\n",
    "    ),\n",
    "    NestedField(\n",
    "        field_id=3, \n",
    "        name=\"email\", \n",
    "        field_type=StringType(), \n",
    "        required=True\n",
    "    ),\n",
    "    NestedField(\n",
    "        field_id=4, \n",
    "        name=\"is_active\", \n",
    "        field_type=BooleanType(), \n",
    "        required=True\n",
    "    ),\n",
    "    \n",
    "    # Partition key fields (created date components)\n",
    "    NestedField(\n",
    "        field_id=5, \n",
    "        name=\"created_year\", \n",
    "        field_type=IntegerType(), \n",
    "        required=True\n",
    "    ),\n",
    "    NestedField(\n",
    "        field_id=6, \n",
    "        name=\"created_month\", \n",
    "        field_type=IntegerType(), \n",
    "        required=True\n",
    "    ),\n",
    "    NestedField(\n",
    "        field_id=7, \n",
    "        name=\"created_day\", \n",
    "        field_type=IntegerType(), \n",
    "        required=True\n",
    "    ),\n",
    "    \n",
    "    # Audit timestamp\n",
    "    NestedField(\n",
    "        field_id=8, \n",
    "        name=\"updated_at\", \n",
    "        field_type=TimestampType(), \n",
    "        required=True\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"\\nSchema defined with {len(user_schema.fields)} fields\")\n",
    "print(\"Field IDs assigned: 1-8 (leaving gaps for future evolution)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "partition-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining partitioning strategy...\n",
      "\n",
      "Partitioning by date (year/month/day):\n",
      "- Enables efficient time-range queries\n",
      "- Supports data retention policies\n",
      "- Improves query performance through partition pruning\n",
      "- Facilitates data lifecycle management\n",
      "\n",
      "Partition specification created:\n",
      "- 3 partition fields (year, month, day)\n",
      "- Identity transform (no data transformation)\n",
      "- Field IDs: 1000-1002 (reserved for partition fields)\n",
      "\n",
      "Expected partition structure:\n",
      "created_year=2025/created_month=6/created_day=27/\n",
      "This creates a hierarchical directory structure\n"
     ]
    }
   ],
   "source": [
    "# Define partitioning strategy\n",
    "print(\"Defining partitioning strategy...\")\n",
    "print(\"\\nPartitioning by date (year/month/day):\")\n",
    "print(\"- Enables efficient time-range queries\")\n",
    "print(\"- Supports data retention policies\")\n",
    "print(\"- Improves query performance through partition pruning\")\n",
    "print(\"- Facilitates data lifecycle management\")\n",
    "\n",
    "# Create partition specification\n",
    "# Partition by year, month, and day for fine-grained control\n",
    "partition_spec = PartitionSpec(\n",
    "    PartitionField(\n",
    "        source_id=5,  # created_year field\n",
    "        field_id=1000, \n",
    "        transform=IdentityTransform(), \n",
    "        name=\"created_year\"\n",
    "    ),\n",
    "    PartitionField(\n",
    "        source_id=6,  # created_month field\n",
    "        field_id=1001, \n",
    "        transform=IdentityTransform(), \n",
    "        name=\"created_month\"\n",
    "    ),\n",
    "    PartitionField(\n",
    "        source_id=7,  # created_day field\n",
    "        field_id=1002, \n",
    "        transform=IdentityTransform(), \n",
    "        name=\"created_day\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\nPartition specification created:\")\n",
    "print(\"- 3 partition fields (year, month, day)\")\n",
    "print(\"- Identity transform (no data transformation)\")\n",
    "print(\"- Field IDs: 1000-1002 (reserved for partition fields)\")\n",
    "\n",
    "print(\"\\nExpected partition structure:\")\n",
    "print(\"created_year=2025/created_month=6/created_day=27/\")\n",
    "print(\"This creates a hierarchical directory structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "table-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: play_iceberg.users\n",
      "\n",
      "Table creation process:\n",
      "1. Validate schema and partition specification\n",
      "2. Create metadata files in object storage\n",
      "3. Register table in catalog\n",
      "4. Return table reference for operations\n",
      "\n",
      "Table 'play_iceberg.users' already exists\n",
      "Loading existing table reference...\n",
      "Existing table loaded successfully\n",
      "\n",
      "Table reference obtained: Table\n"
     ]
    }
   ],
   "source": [
    "# Create the User table\n",
    "table_name = f\"{namespace}.users\"\n",
    "\n",
    "print(f\"Creating table: {table_name}\")\n",
    "print(\"\\nTable creation process:\")\n",
    "print(\"1. Validate schema and partition specification\")\n",
    "print(\"2. Create metadata files in object storage\")\n",
    "print(\"3. Register table in catalog\")\n",
    "print(\"4. Return table reference for operations\")\n",
    "\n",
    "try:\n",
    "    user_table = catalog.create_table(\n",
    "        table_name, \n",
    "        schema=user_schema, \n",
    "        partition_spec=partition_spec\n",
    "    )\n",
    "    print(f\"\\nTable '{table_name}' created successfully!\")\n",
    "    print(\"Table is ready for data operations\")\n",
    "    \n",
    "except TableAlreadyExistsError:\n",
    "    print(f\"\\nTable '{table_name}' already exists\")\n",
    "    print(\"Loading existing table reference...\")\n",
    "    user_table = catalog.load_table(table_name)\n",
    "    print(\"Existing table loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError creating table: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nTable reference obtained: {type(user_table).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "table-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Inspection:\n",
      "========================================\n",
      "\n",
      "Table Schema:\n",
      "table {\n",
      "  1: user_id: required long\n",
      "  2: username: required string\n",
      "  3: email: required string\n",
      "  4: is_active: required boolean\n",
      "  5: created_year: required int\n",
      "  6: created_month: required int\n",
      "  7: created_day: required int\n",
      "  8: updated_at: required timestamp\n",
      "}\n",
      "\n",
      "Partition Specification:\n",
      "[\n",
      "  1000: created_year: identity(5)\n",
      "  1001: created_month: identity(6)\n",
      "  1002: created_day: identity(7)\n",
      "]\n",
      "\n",
      "Table Properties:\n",
      "  write.parquet.compression-codec: zstd\n",
      "\n",
      "Current Snapshot:\n",
      "  No data snapshots (empty table)\n",
      "\n",
      "Table Status: Ready for data operations\n"
     ]
    }
   ],
   "source": [
    "# Inspect the created table\n",
    "print(\"Table Inspection:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Display table schema\n",
    "print(\"\\nTable Schema:\")\n",
    "print(user_table.schema())\n",
    "\n",
    "# Display partition specification\n",
    "print(\"\\nPartition Specification:\")\n",
    "print(user_table.spec())\n",
    "\n",
    "# Table properties\n",
    "print(\"\\nTable Properties:\")\n",
    "properties = user_table.properties\n",
    "if properties:\n",
    "    for key, value in properties.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"  No custom properties set\")\n",
    "\n",
    "# Current snapshot information\n",
    "print(\"\\nCurrent Snapshot:\")\n",
    "current_snapshot = user_table.current_snapshot()\n",
    "if current_snapshot:\n",
    "    print(f\"  Snapshot ID: {current_snapshot.snapshot_id}\")\n",
    "    print(f\"  Timestamp: {current_snapshot.timestamp_ms}\")\n",
    "else:\n",
    "    print(\"  No data snapshots (empty table)\")\n",
    "\n",
    "print(\"\\nTable Status: Ready for data operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "table-verification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Verification:\n",
      "==============================\n",
      "\n",
      "Tables in 'play_iceberg' namespace:\n",
      "  - ('play_iceberg', 'users')\n",
      "\n",
      "Table accessibility test: SUCCESS\n",
      "Table can be loaded and accessed\n",
      "\n",
      "Table Creation Summary:\n",
      "- Namespace: play_iceberg\n",
      "- Table: users\n",
      "- Full name: play_iceberg.users\n",
      "- Schema fields: 8\n",
      "- Partition fields: 3\n",
      "- Status: Ready for data operations\n"
     ]
    }
   ],
   "source": [
    "# Verify table creation\n",
    "print(\"Table Verification:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# List tables in namespace\n",
    "try:\n",
    "    tables_in_namespace = list(catalog.list_tables(namespace))\n",
    "    print(f\"\\nTables in '{namespace}' namespace:\")\n",
    "    for table_id in tables_in_namespace:\n",
    "        print(f\"  - {table_id}\")\n",
    "    \n",
    "    if len(tables_in_namespace) == 0:\n",
    "        print(\"  No tables found\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error listing tables: {e}\")\n",
    "\n",
    "# Test table accessibility\n",
    "try:\n",
    "    test_table = catalog.load_table(table_name)\n",
    "    print(\"\\nTable accessibility test: SUCCESS\")\n",
    "    print(\"Table can be loaded and accessed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\nTable accessibility test: FAILED\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nTable Creation Summary:\")\n",
    "print(f\"- Namespace: {namespace}\")\n",
    "print(\"- Table: users\")\n",
    "print(f\"- Full name: {table_name}\")\n",
    "print(f\"- Schema fields: {len(user_schema.fields)}\")\n",
    "print(f\"- Partition fields: {len(partition_spec.fields)}\")\n",
    "print(\"- Status: Ready for data operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": "## Future Steps\n\n### Immediate Next Actions:\n1. **Data Ingestion**: Insert sample user data (→ Notebook 2)\n2. **Query Operations**: Read and filter data with Spark (→ Notebook 3)\n3. **Data Modifications**: Update and upsert operations (→ Notebook 4)\n4. **Schema Evolution**: Add columns without breaking changes (→ Notebook 5)\n\n### Production Considerations:\n- **Access Control**: Implement namespace-level permissions\n- **Monitoring**: Set up query performance tracking\n- **Maintenance**: Plan for compaction and snapshot cleanup\n- **Backup Strategy**: Design disaster recovery procedures\n- **Data Governance**: Establish data quality and lineage tracking\n\n### Advanced Features to Explore:\n- **Time Travel**: Query historical table versions\n- **Partition Evolution**: Change partitioning strategy over time\n- **Column Statistics**: Optimize query planning with metadata\n- **Snapshot Management**: Control table history and retention",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}