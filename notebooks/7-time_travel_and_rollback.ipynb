{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Time Travel and Rollback with Apache Iceberg\n",
    "\n",
    "This notebook demonstrates one of Iceberg's most powerful features: **time travel** and **rollback** capabilities. These features enable you to:\n",
    "\n",
    "## Key Capabilities:\n",
    "- **Query Historical Data**: Access any previous version of your table\n",
    "- **Data Recovery**: Rollback to previous states after mistakes\n",
    "- **Audit and Compliance**: Track data changes over time\n",
    "- **A/B Testing**: Compare different versions of data\n",
    "- **Debugging**: Investigate when and how data changed\n",
    "\n",
    "## What We'll Cover:\n",
    "1. **Snapshot Management**: Understanding table versions\n",
    "2. **Time Travel Queries**: Querying data at specific points in time\n",
    "3. **Rollback Operations**: Reverting to previous table states\n",
    "4. **Practical Examples**: Real-world use cases\n",
    "5. **Best Practices**: Performance and retention considerations\n",
    "\n",
    "## Prerequisites:\n",
    "- Run notebooks 1-6 to have a table with history\n",
    "- Understanding of Iceberg snapshots and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Initialize Spark session and verify table access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 13:50:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n",
      "Session initialized for time travel operations\n",
      "Using database: rest.play_iceberg\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Time Travel and Rollback\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Session initialized for time travel operations\")\n",
    "\n",
    "# Set default database\n",
    "spark.sql(\"USE rest.`play_iceberg`\")\n",
    "print(\"Using database: rest.play_iceberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-state-section",
   "metadata": {},
   "source": [
    "## 1. Current Table State Analysis\n",
    "\n",
    "Let's first examine the current state of our table and understand its history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "current-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Table Statistics:\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-----------+-----------+------------+\n",
      "|total_records|unique_users|min_user_id|max_user_id|active_users|\n",
      "+-------------+------------+-----------+-----------+------------+\n",
      "|         1012|        1012|          1|       3003|         510|\n",
      "+-------------+------------+-----------+-----------+------------+\n",
      "\n",
      "\n",
      "Current Table Sample (Latest 5 users):\n",
      "==================================================\n",
      "+-------+-----------------------+-------------------+---------+-------+--------------------------+\n",
      "|user_id|username               |email              |is_active|country|updated_at                |\n",
      "+-------+-----------------------+-------------------+---------+-------+--------------------------+\n",
      "|3003   |partition_test_active2 |active2@test.com   |true     |France |2025-07-01 13:48:37.205836|\n",
      "|3002   |partition_test_inactive|inactive@test.com  |false    |UK     |2025-07-01 13:48:37.205836|\n",
      "|3001   |partition_test_active  |active@test.com    |true     |USA    |2025-07-01 13:48:37.205836|\n",
      "|2003   |global_user_3          |global3@company.com|false    |Japan  |2025-07-01 13:48:28.740494|\n",
      "|2002   |global_user_2          |global2@company.com|true     |Germany|2025-07-01 13:48:28.740494|\n",
      "+-------+-----------------------+-------------------+---------+-------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check current table data\n",
    "current_data = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as total_records,\n",
    "       COUNT(DISTINCT user_id) as unique_users,\n",
    "       MIN(user_id) as min_user_id,\n",
    "       MAX(user_id) as max_user_id,\n",
    "       SUM(CASE WHEN is_active THEN 1 ELSE 0 END) as active_users\n",
    "FROM users\n",
    "\"\"\")\n",
    "\n",
    "print(\"Current Table Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "current_data.show()\n",
    "\n",
    "# Show sample of current data\n",
    "print(\"\\nCurrent Table Sample (Latest 5 users):\")\n",
    "print(\"=\" * 50)\n",
    "spark.sql(\"\"\"\n",
    "SELECT user_id, username, email, is_active, country, updated_at\n",
    "FROM users \n",
    "ORDER BY user_id DESC \n",
    "LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-section",
   "metadata": {},
   "source": [
    "## 2. Table History and Snapshots\n",
    "\n",
    "Iceberg maintains a complete history of all table changes. Each change creates a new snapshot with a unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "table-history",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Table History:\n",
      "==================================================\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|operation_type|\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "|2025-07-01 13:41:23.087|3824926247776142618|NULL               |true               |INITIAL_LOAD  |\n",
      "|2025-07-01 13:46:22.566|3682366184055111348|3824926247776142618|true               |UPDATE        |\n",
      "|2025-07-01 13:46:29.92 |5474445044599496672|3682366184055111348|true               |UPDATE        |\n",
      "|2025-07-01 13:46:43.276|703749108260326071 |5474445044599496672|true               |UPDATE        |\n",
      "|2025-07-01 13:46:51.537|1680807781020980082|703749108260326071 |true               |UPDATE        |\n",
      "|2025-07-01 13:47:01.334|8866811446601788626|1680807781020980082|true               |UPDATE        |\n",
      "|2025-07-01 13:48:29.944|5929459840228022626|8866811446601788626|true               |UPDATE        |\n",
      "|2025-07-01 13:48:37.868|1926503896228031236|5929459840228022626|true               |UPDATE        |\n",
      "+-----------------------+-------------------+-------------------+-------------------+--------------+\n",
      "\n",
      "\n",
      "Total snapshots found: 8\n",
      "First snapshot: 3824926247776142618\n",
      "Latest snapshot: 1926503896228031236\n"
     ]
    }
   ],
   "source": [
    "# Get complete table history\n",
    "history_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor,\n",
    "    CASE \n",
    "        WHEN parent_id IS NULL THEN 'INITIAL_LOAD'\n",
    "        ELSE 'UPDATE'\n",
    "    END as operation_type\n",
    "FROM rest.`play_iceberg`.users.history\n",
    "ORDER BY made_current_at\n",
    "\"\"\")\n",
    "\n",
    "print(\"Complete Table History:\")\n",
    "print(\"=\" * 50)\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "# Store snapshot IDs for later use\n",
    "snapshots = history_df.collect()\n",
    "print(f\"\\nTotal snapshots found: {len(snapshots)}\")\n",
    "\n",
    "if len(snapshots) >= 2:\n",
    "    first_snapshot = snapshots[0]['snapshot_id']\n",
    "    latest_snapshot = snapshots[-1]['snapshot_id']\n",
    "    print(f\"First snapshot: {first_snapshot}\")\n",
    "    print(f\"Latest snapshot: {latest_snapshot}\")\n",
    "else:\n",
    "    print(\"Insufficient history for time travel demo - run previous notebooks first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-travel-section",
   "metadata": {},
   "source": [
    "## 3. Time Travel Queries\n",
    "\n",
    "Now let's demonstrate time travel by querying the table at different points in its history.\n",
    "\n",
    "### 3.1 Query by Snapshot ID\n",
    "The most precise way to time travel is using specific snapshot IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "time-travel-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at First Snapshot (3824926247776142618):\n",
      "==================================================\n",
      "+-------+-------------+-------------------------+---------+--------------------------+\n",
      "|user_id|username     |email                    |is_active|updated_at                |\n",
      "+-------+-------------+-------------------------+---------+--------------------------+\n",
      "|1      |john_doe     |john.doe@example.com     |true     |2025-07-01 13:41:16.173663|\n",
      "|2      |jane_smith   |jane.smith@example.com   |true     |2025-07-01 13:41:16.173663|\n",
      "|3      |alice_wonder |alice.wonder@example.com |false    |2025-07-01 13:41:16.173663|\n",
      "|4      |bob_builder  |bob.builder@example.com  |true     |2025-07-01 13:41:16.173663|\n",
      "|5      |charlie_brown|charlie.brown@example.com|true     |2025-07-01 13:41:16.173663|\n",
      "+-------+-------------+-------------------------+---------+--------------------------+\n",
      "\n",
      "Records in first snapshot: 5\n",
      "Records in current state: 1012\n",
      "Records added since first snapshot: 1007\n"
     ]
    }
   ],
   "source": [
    "if len(snapshots) >= 2:\n",
    "    # Query the first snapshot (original table state)\n",
    "    print(f\"Data at First Snapshot ({first_snapshot}):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    first_snapshot_data = spark.sql(f\"\"\"\n",
    "    SELECT user_id, username, email, is_active, updated_at\n",
    "    FROM users FOR SYSTEM_VERSION AS OF {first_snapshot}\n",
    "    ORDER BY user_id\n",
    "    \"\"\")\n",
    "    \n",
    "    first_snapshot_data.show(truncate=False)\n",
    "    \n",
    "    # Get record count at first snapshot\n",
    "    first_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as record_count \n",
    "    FROM users FOR SYSTEM_VERSION AS OF {first_snapshot}\n",
    "    \"\"\").collect()[0]['record_count']\n",
    "    \n",
    "    print(f\"Records in first snapshot: {first_count}\")\n",
    "    \n",
    "    # Compare with current state\n",
    "    current_count = spark.sql(\"SELECT COUNT(*) as record_count FROM users\").collect()[0]['record_count']\n",
    "    print(f\"Records in current state: {current_count}\")\n",
    "    print(f\"Records added since first snapshot: {current_count - first_count}\")\n",
    "else:\n",
    "    print(\"Need more table history to demonstrate time travel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timestamp-travel-section",
   "metadata": {},
   "source": [
    "### 3.2 Query by Timestamp\n",
    "You can also time travel using timestamps, which is useful when you know when a change occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "time-travel-timestamp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Travel to: 2025-07-01 13:41:23.087000\n",
      "==================================================\n",
      "+-------+-------------+-------------------------+---------+\n",
      "|user_id|username     |email                    |is_active|\n",
      "+-------+-------------+-------------------------+---------+\n",
      "|1      |john_doe     |john.doe@example.com     |true     |\n",
      "|2      |jane_smith   |jane.smith@example.com   |true     |\n",
      "|3      |alice_wonder |alice.wonder@example.com |false    |\n",
      "|4      |bob_builder  |bob.builder@example.com  |true     |\n",
      "|5      |charlie_brown|charlie.brown@example.com|true     |\n",
      "+-------+-------------+-------------------------+---------+\n",
      "\n",
      "Records at timestamp 2025-07-01 13:41:23.087000: 5\n"
     ]
    }
   ],
   "source": [
    "if len(snapshots) >= 2:\n",
    "    # Get timestamp of first snapshot\n",
    "    first_timestamp = snapshots[0]['made_current_at']\n",
    "    \n",
    "    print(f\"Time Travel to: {first_timestamp}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Query using timestamp\n",
    "    timestamp_data = spark.sql(f\"\"\"\n",
    "    SELECT user_id, username, email, is_active\n",
    "    FROM users FOR SYSTEM_TIME AS OF '{first_timestamp}'\n",
    "    ORDER BY user_id\n",
    "    \"\"\")\n",
    "    \n",
    "    timestamp_data.show(truncate=False)\n",
    "    \n",
    "    # Verify this matches the snapshot query\n",
    "    timestamp_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as record_count \n",
    "    FROM users FOR SYSTEM_TIME AS OF '{first_timestamp}'\n",
    "    \"\"\").collect()[0]['record_count']\n",
    "    \n",
    "    print(f\"Records at timestamp {first_timestamp}: {timestamp_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "make-changes-section",
   "metadata": {},
   "source": [
    "## 4. Making Changes for Rollback Demo\n",
    "\n",
    "Let's make some intentional changes that we can later rollback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "make-changes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current snapshot before changes: 1926503896228031236\n",
      "Timestamp: 2025-07-01 13:48:37.868000\n",
      "Record count before changes: 1012\n"
     ]
    }
   ],
   "source": [
    "# Record the current snapshot before making changes\n",
    "pre_change_history = spark.sql(\"\"\"\n",
    "SELECT snapshot_id, made_current_at \n",
    "FROM rest.`play_iceberg`.users.history \n",
    "ORDER BY made_current_at DESC \n",
    "LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "pre_change_snapshot = pre_change_history['snapshot_id']\n",
    "pre_change_time = pre_change_history['made_current_at']\n",
    "\n",
    "print(f\"Current snapshot before changes: {pre_change_snapshot}\")\n",
    "print(f\"Timestamp: {pre_change_time}\")\n",
    "\n",
    "# Count records before changes\n",
    "before_count = spark.sql(\"SELECT COUNT(*) as count FROM users\").collect()[0]['count']\n",
    "print(f\"Record count before changes: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a455b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting test data for rollback demonstration...\n",
      "Test data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# FIXED: Insert test data with complete schema (including evolved columns)\n",
    "print(\"Inserting test data for rollback demonstration...\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO users \n",
    "(user_id, username, email, is_active, created_year, created_month, created_day, updated_at, \n",
    " country, registration_source, engagement_score, last_login_at) \n",
    "VALUES \n",
    "    (4001, 'test_user_1', 'test1@rollback.com', true, 2025, 7, 1, current_timestamp(), \n",
    "     'TEST', 'demo', 85.0, current_timestamp()),\n",
    "    (4002, 'test_user_2', 'test2@rollback.com', false, 2025, 7, 1, current_timestamp(), \n",
    "     'TEST', 'demo', 65.5, null),\n",
    "    (4003, 'temp_user', 'temp@rollback.com', true, 2025, 7, 1, current_timestamp(), \n",
    "     'TEMP', 'rollback_test', 90.0, current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "print(\"Test data inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "update-test-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making updates for rollback demonstration...\n",
      "Updates completed!\n",
      "Record count after changes: 1015\n",
      "Records added: 3\n",
      "\n",
      "Current state after changes:\n",
      "+-------+-------------+-------------------------+---------+-------+\n",
      "|user_id|username     |email                    |is_active|country|\n",
      "+-------+-------------+-------------------------+---------+-------+\n",
      "|101    |bulk_user_101|bulk.user.101@example.com|true     |NULL   |\n",
      "|102    |bulk_user_102|bulk.user.102@example.com|false    |NULL   |\n",
      "|103    |bulk_user_103|bulk.user.103@example.com|true     |NULL   |\n",
      "|104    |bulk_user_104|bulk.user.104@example.com|false    |NULL   |\n",
      "|106    |bulk_user_106|bulk.user.106@example.com|false    |NULL   |\n",
      "|107    |bulk_user_107|bulk.user.107@example.com|false    |NULL   |\n",
      "|108    |bulk_user_108|bulk.user.108@example.com|false    |NULL   |\n",
      "|109    |bulk_user_109|bulk.user.109@example.com|false    |NULL   |\n",
      "|111    |bulk_user_111|bulk.user.111@example.com|true     |NULL   |\n",
      "|112    |bulk_user_112|bulk.user.112@example.com|false    |NULL   |\n",
      "|113    |bulk_user_113|bulk.user.113@example.com|false    |NULL   |\n",
      "|114    |bulk_user_114|bulk.user.114@example.com|false    |NULL   |\n",
      "|115    |bulk_user_115|bulk.user.115@example.com|false    |NULL   |\n",
      "|116    |bulk_user_116|bulk.user.116@example.com|true     |NULL   |\n",
      "|117    |bulk_user_117|bulk.user.117@example.com|true     |NULL   |\n",
      "|118    |bulk_user_118|bulk.user.118@example.com|true     |NULL   |\n",
      "|119    |bulk_user_119|bulk.user.119@example.com|false    |NULL   |\n",
      "|120    |bulk_user_120|bulk.user.120@example.com|false    |NULL   |\n",
      "|121    |bulk_user_121|bulk.user.121@example.com|true     |NULL   |\n",
      "|122    |bulk_user_122|bulk.user.122@example.com|false    |NULL   |\n",
      "+-------+-------------+-------------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make some updates that we'll want to rollback\n",
    "print(\"Making updates for rollback demonstration...\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "UPDATE users \n",
    "SET is_active = false, \n",
    "    updated_at = current_timestamp(),\n",
    "    country = 'DEACTIVATED'\n",
    "WHERE country = 'USA'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Updates completed!\")\n",
    "\n",
    "# Show current state after changes\n",
    "after_count = spark.sql(\"SELECT COUNT(*) as count FROM users\").collect()[0]['count']\n",
    "print(f\"Record count after changes: {after_count}\")\n",
    "print(f\"Records added: {after_count - before_count}\")\n",
    "\n",
    "# Show some of the changes\n",
    "print(\"\\nCurrent state after changes:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT user_id, username, email, is_active, country\n",
    "FROM users \n",
    "WHERE user_id >= 101 OR country IN ('DEACTIVATED', 'TEST', 'TEMP')\n",
    "ORDER BY user_id\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-history-section",
   "metadata": {},
   "source": [
    "## 5. Review Updated History\n",
    "\n",
    "Let's see how our changes affected the table history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "updated-history",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Table History (most recent first):\n",
      "=======================================================\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-07-01 13:50:46.609|841024702459819741 |7274330971587238251|true               |\n",
      "|2025-07-01 13:50:45.161|7274330971587238251|1926503896228031236|true               |\n",
      "|2025-07-01 13:48:37.868|1926503896228031236|5929459840228022626|true               |\n",
      "|2025-07-01 13:48:29.944|5929459840228022626|8866811446601788626|true               |\n",
      "|2025-07-01 13:47:01.334|8866811446601788626|1680807781020980082|true               |\n",
      "|2025-07-01 13:46:51.537|1680807781020980082|703749108260326071 |true               |\n",
      "|2025-07-01 13:46:43.276|703749108260326071 |5474445044599496672|true               |\n",
      "|2025-07-01 13:46:29.92 |5474445044599496672|3682366184055111348|true               |\n",
      "|2025-07-01 13:46:22.566|3682366184055111348|3824926247776142618|true               |\n",
      "|2025-07-01 13:41:23.087|3824926247776142618|NULL               |true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Current (latest) snapshot: 841024702459819741\n",
      "Snapshot we want to rollback to: 1926503896228031236\n"
     ]
    }
   ],
   "source": [
    "# Check updated history\n",
    "updated_history = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor\n",
    "FROM rest.`play_iceberg`.users.history\n",
    "ORDER BY made_current_at DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Updated Table History (most recent first):\")\n",
    "print(\"=\" * 55)\n",
    "updated_history.show(truncate=False)\n",
    "\n",
    "# Get the latest snapshot ID\n",
    "latest_snapshots = updated_history.collect()\n",
    "current_snapshot = latest_snapshots[0]['snapshot_id']\n",
    "print(f\"\\nCurrent (latest) snapshot: {current_snapshot}\")\n",
    "print(f\"Snapshot we want to rollback to: {pre_change_snapshot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollback-section",
   "metadata": {},
   "source": [
    "## 6. Rollback Operations\n",
    "\n",
    "Now let's demonstrate the rollback functionality to undo our changes.\n",
    "\n",
    "### 6.1 Preview Rollback Target\n",
    "Before rolling back, let's verify what the data looked like at our target snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preview-rollback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview: Data at snapshot 1926503896228031236 (before our changes):\n",
      "======================================================================\n",
      "+-------------+------------+---------+-----------+\n",
      "|total_records|active_users|usa_users|max_user_id|\n",
      "+-------------+------------+---------+-----------+\n",
      "|         1012|         510|        1|       3003|\n",
      "+-------------+------------+---------+-----------+\n",
      "\n",
      "Sample of original data (users with country info):\n",
      "+-------+-----------------------+---------+-------+\n",
      "|user_id|username               |is_active|country|\n",
      "+-------+-----------------------+---------+-------+\n",
      "|2001   |global_user_1          |true     |Canada |\n",
      "|2002   |global_user_2          |true     |Germany|\n",
      "|2003   |global_user_3          |false    |Japan  |\n",
      "|3001   |partition_test_active  |true     |USA    |\n",
      "|3002   |partition_test_inactive|false    |UK     |\n",
      "|3003   |partition_test_active2 |true     |France |\n",
      "+-------+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview what the table looked like before our changes\n",
    "print(f\"Preview: Data at snapshot {pre_change_snapshot} (before our changes):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "preview_data = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*) as total_records,\n",
    "       SUM(CASE WHEN is_active THEN 1 ELSE 0 END) as active_users,\n",
    "       SUM(CASE WHEN country = 'USA' THEN 1 ELSE 0 END) as usa_users,\n",
    "       MAX(user_id) as max_user_id\n",
    "FROM users FOR SYSTEM_VERSION AS OF {pre_change_snapshot}\n",
    "\"\"\")\n",
    "\n",
    "preview_data.show()\n",
    "\n",
    "# Show sample of original data\n",
    "print(\"Sample of original data (users with country info):\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT user_id, username, is_active, country\n",
    "FROM users FOR SYSTEM_VERSION AS OF {pre_change_snapshot}\n",
    "WHERE country IS NOT NULL\n",
    "ORDER BY user_id\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perform-rollback-section",
   "metadata": {},
   "source": [
    "### 6.2 Perform the Rollback\n",
    "Now let's actually rollback to the previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "perform-rollback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling back to snapshot: 1926503896228031236\n",
      "Rolling back to time: 2025-07-01 13:48:37.868000\n",
      "==================================================\n",
      "Rollback operation result:\n",
      "+--------------------+-------------------+\n",
      "|previous_snapshot_id|current_snapshot_id|\n",
      "+--------------------+-------------------+\n",
      "|  841024702459819741|1926503896228031236|\n",
      "+--------------------+-------------------+\n",
      "\n",
      "Rollback completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Perform the rollback\n",
    "print(f\"Rolling back to snapshot: {pre_change_snapshot}\")\n",
    "print(f\"Rolling back to time: {pre_change_time}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rollback_result = spark.sql(f\"\"\"\n",
    "CALL rest.system.rollback_to_snapshot('rest.`play_iceberg`.users', {pre_change_snapshot})\n",
    "\"\"\")\n",
    "\n",
    "print(\"Rollback operation result:\")\n",
    "rollback_result.show()\n",
    "\n",
    "print(\"Rollback completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-rollback-section",
   "metadata": {},
   "source": [
    "### 6.3 Verify Rollback Results\n",
    "Let's confirm that the rollback worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "verify-rollback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Current table state after rollback:\n",
      "==================================================\n",
      "+-------------+------------+---------+-----------+\n",
      "|total_records|active_users|usa_users|max_user_id|\n",
      "+-------------+------------+---------+-----------+\n",
      "|         1012|         510|        1|       3003|\n",
      "+-------------+------------+---------+-----------+\n",
      "\n",
      "Checking if test users were removed:\n",
      "Test users remaining: 0 (should be 0)\n",
      "\n",
      "Checking USA users status:\n",
      "+-------+---------------------+---------+-------+\n",
      "|user_id|username             |is_active|country|\n",
      "+-------+---------------------+---------+-------+\n",
      "|3001   |partition_test_active|true     |USA    |\n",
      "+-------+---------------------+---------+-------+\n",
      "\n",
      "\n",
      "Final record count: 1012\n",
      "Original count before changes: 1012\n",
      "Rollback successful: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the rollback worked\n",
    "print(\"Verification: Current table state after rollback:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check record counts\n",
    "post_rollback_stats = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as total_records,\n",
    "       SUM(CASE WHEN is_active THEN 1 ELSE 0 END) as active_users,\n",
    "       SUM(CASE WHEN country = 'USA' THEN 1 ELSE 0 END) as usa_users,\n",
    "       MAX(user_id) as max_user_id\n",
    "FROM users\n",
    "\"\"\")\n",
    "\n",
    "post_rollback_stats.show()\n",
    "\n",
    "# Verify specific changes were rolled back\n",
    "print(\"Checking if test users were removed:\")\n",
    "test_users_check = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as test_user_count\n",
    "FROM users \n",
    "WHERE user_id >= 4001\n",
    "\"\"\").collect()[0]['test_user_count']\n",
    "\n",
    "print(f\"Test users remaining: {test_users_check} (should be 0)\")\n",
    "\n",
    "# Check if USA users are active again\n",
    "print(\"\\nChecking USA users status:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT user_id, username, is_active, country\n",
    "FROM users \n",
    "WHERE country = 'USA'\n",
    "ORDER BY user_id\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Final verification\n",
    "final_count = spark.sql(\"SELECT COUNT(*) as count FROM users\").collect()[0]['count']\n",
    "print(f\"\\nFinal record count: {final_count}\")\n",
    "print(f\"Original count before changes: {before_count}\")\n",
    "print(f\"Rollback successful: {final_count == before_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-after-rollback-section",
   "metadata": {},
   "source": [
    "### 6.4 History After Rollback\n",
    "Let's see how the rollback affected our table history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "history-after-rollback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table History After Rollback:\n",
      "==================================================\n",
      "+-----------------------+-------------------+-------------------+-------------------+---------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|status         |\n",
      "+-----------------------+-------------------+-------------------+-------------------+---------------+\n",
      "|2025-07-01 13:41:23.087|3824926247776142618|NULL               |true               |ACTIVE         |\n",
      "|2025-07-01 13:46:22.566|3682366184055111348|3824926247776142618|true               |ACTIVE         |\n",
      "|2025-07-01 13:46:29.92 |5474445044599496672|3682366184055111348|true               |ACTIVE         |\n",
      "|2025-07-01 13:46:43.276|703749108260326071 |5474445044599496672|true               |ACTIVE         |\n",
      "|2025-07-01 13:46:51.537|1680807781020980082|703749108260326071 |true               |ACTIVE         |\n",
      "|2025-07-01 13:47:01.334|8866811446601788626|1680807781020980082|true               |ACTIVE         |\n",
      "|2025-07-01 13:48:29.944|5929459840228022626|8866811446601788626|true               |ACTIVE         |\n",
      "|2025-07-01 13:48:37.868|1926503896228031236|5929459840228022626|true               |ROLLBACK_TARGET|\n",
      "|2025-07-01 13:50:45.161|7274330971587238251|1926503896228031236|false              |ORPHANED       |\n",
      "|2025-07-01 13:50:46.609|841024702459819741 |7274330971587238251|false              |ORPHANED       |\n",
      "|2025-07-01 13:50:48.992|1926503896228031236|5929459840228022626|true               |ROLLBACK_TARGET|\n",
      "+-----------------------+-------------------+-------------------+-------------------+---------------+\n",
      "\n",
      "\n",
      "History Summary:\n",
      "+---------------+---------------+------------------+\n",
      "|total_snapshots|current_lineage|orphaned_snapshots|\n",
      "+---------------+---------------+------------------+\n",
      "|             11|              9|                 2|\n",
      "+---------------+---------------+------------------+\n",
      "\n",
      "Note: Orphaned snapshots are the ones we rolled back from.\n",
      "They're still available for time travel but not in the current lineage.\n"
     ]
    }
   ],
   "source": [
    "# Check history after rollback\n",
    "final_history = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor,\n",
    "    CASE \n",
    "        WHEN snapshot_id = {} THEN 'ROLLBACK_TARGET'\n",
    "        WHEN is_current_ancestor = false THEN 'ORPHANED'\n",
    "        ELSE 'ACTIVE'\n",
    "    END as status\n",
    "FROM rest.`play_iceberg`.users.history\n",
    "ORDER BY made_current_at\n",
    "\"\"\".format(pre_change_snapshot))\n",
    "\n",
    "print(\"Table History After Rollback:\")\n",
    "print(\"=\" * 50)\n",
    "final_history.show(truncate=False)\n",
    "\n",
    "# Count different types of snapshots\n",
    "history_summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_snapshots,\n",
    "    SUM(CASE WHEN is_current_ancestor THEN 1 ELSE 0 END) as current_lineage,\n",
    "    SUM(CASE WHEN is_current_ancestor = false THEN 1 ELSE 0 END) as orphaned_snapshots\n",
    "FROM rest.`play_iceberg`.users.history\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nHistory Summary:\")\n",
    "history_summary.show()\n",
    "\n",
    "print(\"Note: Orphaned snapshots are the ones we rolled back from.\")\n",
    "print(\"They're still available for time travel but not in the current lineage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-examples-section",
   "metadata": {},
   "source": [
    "## 7. Advanced Time Travel Examples\n",
    "\n",
    "Let's explore some advanced time travel scenarios.\n",
    "\n",
    "### 7.1 Data Comparison Across Time\n",
    "Compare data between different time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "data-comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Evolution Analysis:\n",
      "========================================\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `country` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `username`, `created_day`, `is_active`].; line 7 pos 36;\n'Aggregate [2025-07-01 13:41:23.087000 AS snapshot_time#956, 3824926247776142618 AS snapshot_id#957L, count(1) AS total_users#958L, sum(CASE WHEN is_active#964 THEN 1 ELSE 0 END) AS active_users#959L, 'COUNT(distinct 'COALESCE('country, NULL)) AS countries#960]\n+- SubqueryAlias rest.play_iceberg.users\n   +- RelationV2[user_id#961L, username#962, email#963, is_active#964, created_year#965, created_month#966, created_day#967, updated_at#968] rest.play_iceberg.users rest.play_iceberg.users\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m snapshot_id \u001b[38;5;241m=\u001b[39m snapshot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnapshot_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m snapshot[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmade_current_at\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m count_data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43mSELECT \u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtimestamp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m as snapshot_time,\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msnapshot_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m as snapshot_id,\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m    COUNT(*) as total_users,\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m    SUM(CASE WHEN is_active THEN 1 ELSE 0 END) as active_users,\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m    COUNT(DISTINCT COALESCE(country, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNULL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)) as countries\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43mFROM users FOR SYSTEM_VERSION AS OF \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msnapshot_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSnapshot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m count_data\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `country` cannot be resolved. Did you mean one of the following? [`email`, `user_id`, `username`, `created_day`, `is_active`].; line 7 pos 36;\n'Aggregate [2025-07-01 13:41:23.087000 AS snapshot_time#956, 3824926247776142618 AS snapshot_id#957L, count(1) AS total_users#958L, sum(CASE WHEN is_active#964 THEN 1 ELSE 0 END) AS active_users#959L, 'COUNT(distinct 'COALESCE('country, NULL)) AS countries#960]\n+- SubqueryAlias rest.play_iceberg.users\n   +- RelationV2[user_id#961L, username#962, email#963, is_active#964, created_year#965, created_month#966, created_day#967, updated_at#968] rest.play_iceberg.users rest.play_iceberg.users\n"
     ]
    }
   ],
   "source": [
    "if len(snapshots) >= 2:\n",
    "    # Compare user counts across snapshots\n",
    "    print(\"Data Evolution Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, snapshot in enumerate(snapshots[:3]):  # Show first 3 snapshots\n",
    "        snapshot_id = snapshot['snapshot_id']\n",
    "        timestamp = snapshot['made_current_at']\n",
    "        \n",
    "        count_data = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            '{timestamp}' as snapshot_time,\n",
    "            {snapshot_id} as snapshot_id,\n",
    "            COUNT(*) as total_users,\n",
    "            SUM(CASE WHEN is_active THEN 1 ELSE 0 END) as active_users,\n",
    "            COUNT(DISTINCT COALESCE(country, 'NULL')) as countries\n",
    "        FROM users FOR SYSTEM_VERSION AS OF {snapshot_id}\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"\\nSnapshot {i+1} ({timestamp}):\")\n",
    "        count_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audit-example-section",
   "metadata": {},
   "source": [
    "### 7.2 Audit Trail Example\n",
    "Track changes to specific records over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "audit-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit Trail for User ID 1:\n",
      "==================================================\n",
      "At 2025-07-01 13:41:23.087000: Error querying - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or fu...\n",
      "\n",
      "At 2025-07-01 13:46:22.566000: Error querying - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or fu...\n",
      "\n",
      "At 2025-07-01 13:46:29.920000: Error querying - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or fu...\n",
      "\n",
      "At 2025-07-01 13:46:43.276000: Error querying - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or fu...\n",
      "\n",
      "At 2025-07-01 13:46:51.537000: Error querying - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or fu...\n",
      "\n",
      "At 2025-07-01 13:47:01.334000: Error querying - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or fu...\n",
      "\n",
      "At 2025-07-01 13:48:29.944000:\n",
      "  Username: updated_user_1\n",
      "  Email: updated.user.1@example.com\n",
      "  Active: True\n",
      "  Country: None\n",
      "\n",
      "At 2025-07-01 13:48:37.868000:\n",
      "  Username: updated_user_1\n",
      "  Email: updated.user.1@example.com\n",
      "  Active: True\n",
      "  Country: None\n",
      "\n",
      "Total audit records found: 2\n"
     ]
    }
   ],
   "source": [
    "# Create an audit trail for a specific user\n",
    "user_to_audit = 1  # Track changes to user_id = 1\n",
    "\n",
    "print(f\"Audit Trail for User ID {user_to_audit}:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "audit_results = []\n",
    "\n",
    "for snapshot in snapshots:\n",
    "    snapshot_id = snapshot['snapshot_id']\n",
    "    timestamp = snapshot['made_current_at']\n",
    "    \n",
    "    try:\n",
    "        user_data = spark.sql(f\"\"\"\n",
    "        SELECT username, email, is_active, country, updated_at\n",
    "        FROM users FOR SYSTEM_VERSION AS OF {snapshot_id}\n",
    "        WHERE user_id = {user_to_audit}\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        if user_data:\n",
    "            user = user_data[0]\n",
    "            audit_results.append({\n",
    "                'snapshot_time': timestamp,\n",
    "                'username': user['username'],\n",
    "                'email': user['email'],\n",
    "                'is_active': user['is_active'],\n",
    "                'country': user['country']\n",
    "            })\n",
    "            \n",
    "            print(f\"At {timestamp}:\")\n",
    "            print(f\"  Username: {user['username']}\")\n",
    "            print(f\"  Email: {user['email']}\")\n",
    "            print(f\"  Active: {user['is_active']}\")\n",
    "            print(f\"  Country: {user['country']}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"At {timestamp}: User not found (may not exist yet)\")\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f\"At {timestamp}: Error querying - {str(e)[:50]}...\")\n",
    "        print()\n",
    "\n",
    "print(f\"Total audit records found: {len(audit_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-section",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Considerations\n",
    "\n",
    "### 8.1 Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "performance-tips",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Considerations:\n",
      "========================================\n",
      "Total snapshots: 8\n",
      "Current data files: 6\n",
      "\n",
      "Best Practices:\n",
      "1. Use snapshot IDs for precise time travel\n",
      "2. Use timestamps for approximate time travel\n",
      "3. Test rollbacks in non-production environments first\n",
      "4. Monitor snapshot retention policies\n",
      "5. Consider performance impact of many snapshots\n",
      "6. Document rollback procedures for your team\n"
     ]
    }
   ],
   "source": [
    "# Check table metadata for performance insights\n",
    "print(\"Performance Considerations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Number of snapshots\n",
    "total_snapshots = len(snapshots)\n",
    "print(f\"Total snapshots: {total_snapshots}\")\n",
    "\n",
    "if total_snapshots > 10:\n",
    "    print(\"RECOMMENDATION: Consider snapshot cleanup for large numbers of snapshots\")\n",
    "\n",
    "# Check file count\n",
    "file_count = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as file_count \n",
    "FROM rest.`play_iceberg`.users.files\n",
    "\"\"\").collect()[0]['file_count']\n",
    "\n",
    "print(f\"Current data files: {file_count}\")\n",
    "\n",
    "if file_count > 100:\n",
    "    print(\"RECOMMENDATION: Consider compaction for large numbers of files\")\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\nBest Practices:\")\n",
    "print(\"1. Use snapshot IDs for precise time travel\")\n",
    "print(\"2. Use timestamps for approximate time travel\")\n",
    "print(\"3. Test rollbacks in non-production environments first\")\n",
    "print(\"4. Monitor snapshot retention policies\")\n",
    "print(\"5. Consider performance impact of many snapshots\")\n",
    "print(\"6. Document rollback procedures for your team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "### 8.2 Snapshot Cleanup (Optional)\n",
    "Demonstrate how to clean up old snapshots if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "snapshot-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current snapshots: 11\n",
      "\n",
      "Snapshot cleanup options:\n",
      "1. Expire snapshots older than timestamp:\n",
      "   CALL rest.system.expire_snapshots('rest.`play_iceberg`.users', '2025-01-01 00:00:00')\n",
      "\n",
      "2. Retain only last N snapshots:\n",
      "   CALL rest.system.expire_snapshots('rest.`play_iceberg`.users', retain_last => 5)\n",
      "\n",
      "Note: Snapshot cleanup is irreversible - use with caution!\n",
      "\n",
      "Skipping actual cleanup for demo safety\n"
     ]
    }
   ],
   "source": [
    "# Show current snapshot count\n",
    "current_snapshot_count = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as snapshot_count \n",
    "FROM rest.`play_iceberg`.users.history\n",
    "\"\"\").collect()[0]['snapshot_count']\n",
    "\n",
    "print(f\"Current snapshots: {current_snapshot_count}\")\n",
    "\n",
    "# Example of how to expire old snapshots (uncomment to use)\n",
    "print(\"\\nSnapshot cleanup options:\")\n",
    "print(\"1. Expire snapshots older than timestamp:\")\n",
    "print(\"   CALL rest.system.expire_snapshots('rest.`play_iceberg`.users', '2025-01-01 00:00:00')\")\n",
    "print(\"\\n2. Retain only last N snapshots:\")\n",
    "print(\"   CALL rest.system.expire_snapshots('rest.`play_iceberg`.users', retain_last => 5)\")\n",
    "print(\"\\nNote: Snapshot cleanup is irreversible - use with caution!\")\n",
    "\n",
    "# For demo purposes, we'll not actually clean up snapshots\n",
    "print(\"\\nSkipping actual cleanup for demo safety\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the powerful time travel and rollback capabilities of Apache Iceberg:\n",
    "\n",
    "### What We Accomplished:\n",
    "1. **Explored Table History**: Understood how Iceberg tracks all changes\n",
    "2. **Time Travel Queries**: Accessed historical data using snapshots and timestamps\n",
    "3. **Rollback Operations**: Successfully reverted unwanted changes\n",
    "4. **Data Auditing**: Tracked changes to specific records over time\n",
    "5. **Best Practices**: Learned performance and operational considerations\n",
    "\n",
    "### Key Benefits:\n",
    "- **Data Recovery**: Quick recovery from accidental changes\n",
    "- **Debugging**: Investigate when and how data changed\n",
    "- **Compliance**: Complete audit trail of all changes\n",
    "- **Testing**: Safe experimentation with rollback capability\n",
    "- **Analysis**: Compare data across different time periods\n",
    "\n",
    "### Use Cases:\n",
    "- **ETL Error Recovery**: Rollback failed batch jobs\n",
    "- **Data Quality Issues**: Revert to clean state and re-process\n",
    "- **Regulatory Compliance**: Maintain complete change history\n",
    "- **A/B Testing**: Compare results across different data versions\n",
    "- **Incident Response**: Quickly restore service after data corruption\n",
    "\n",
    "Time travel and rollback make Iceberg an excellent choice for production data lakes where data reliability and recoverability are critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "print(\"Time travel and rollback demonstration completed!\")\n",
    "print(\"Spark session cleanup...\")\n",
    "spark.stop()\n",
    "print(\"Session closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f210be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42295d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
