{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Upsert Operations with Apache Iceberg and Spark\n",
    "\n",
    "This notebook provides a comprehensive tutorial on performing upsert (MERGE) operations with Apache Iceberg tables using Apache Spark. We'll explore various merge strategies, performance optimizations, and best practices for data synchronization.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Iceberg's MERGE INTO capabilities\n",
    "- Learn different upsert patterns and strategies\n",
    "- Explore conditional merge logic\n",
    "- Practice performance optimization techniques\n",
    "- Handle complex merge scenarios\n",
    "\n",
    "## Prerequisites\n",
    "- Previous notebooks (1-4) completed\n",
    "- Understanding of Spark SQL and DataFrames\n",
    "- Basic knowledge of data synchronization concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Initialization\n",
    "\n",
    "First, let's establish our Spark session and verify the environment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, BooleanType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/27 06:52:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n",
      "Default Catalog: rest\n",
      "Adaptive Query Execution: true\n",
      "\n",
      "Catalog context set to: rest.play-iceberg\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session with optimized configuration for merge operations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced Iceberg Upsert Tutorial\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Default Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Adaptive Query Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Set catalog context\n",
    "spark.sql(\"USE rest.`play-iceberg`\")\n",
    "print(\"\\nCatalog context set to: rest.play-iceberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Current Table State Analysis\n",
    "\n",
    "Before performing merge operations, let's analyze our current table state and understand the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Table Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+-----------+-----------+\n",
      "|total_records|active_users|inactive_users|min_user_id|max_user_id|\n",
      "+-------------+------------+--------------+-----------+-----------+\n",
      "|            5|           4|             1|          1|          5|\n",
      "+-------------+------------+--------------+-----------+-----------+\n",
      "\n",
      "\n",
      "Current Records:\n",
      "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
      "|user_id|     username|               email|is_active|created_year|created_month|created_day|          updated_at|\n",
      "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
      "|      1|     john_doe|john.doe@example.com|     true|        2025|            6|         27|2025-06-27 06:37:...|\n",
      "|      2|   jane_smith|jane.smith@exampl...|     true|        2025|            6|         27|2025-06-27 06:37:...|\n",
      "|      3| alice_wonder|alice.wonder@exam...|    false|        2025|            6|         27|2025-06-27 06:37:...|\n",
      "|      4|  bob_builder|bob.builder@examp...|     true|        2025|            6|         27|2025-06-27 06:37:...|\n",
      "|      5|charlie_brown|charlie.brown@exa...|     true|        2025|            6|         27|2025-06-27 06:37:...|\n",
      "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze current table state\n",
    "current_data = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(CASE WHEN is_active = true THEN 1 END) as active_users,\n",
    "        COUNT(CASE WHEN is_active = false THEN 1 END) as inactive_users,\n",
    "        MIN(user_id) as min_user_id,\n",
    "        MAX(user_id) as max_user_id\n",
    "    FROM users\n",
    "\"\"\")\n",
    "\n",
    "print(\"Current Table Statistics:\")\n",
    "current_data.show()\n",
    "\n",
    "# Display current records\n",
    "print(\"\\nCurrent Records:\")\n",
    "spark.sql(\"SELECT * FROM users ORDER BY user_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Upsert Operation\n",
    "\n",
    "Let's start with a basic MERGE INTO operation that demonstrates the fundamental upsert pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data to be upserted:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|user_id|username           |email                          |is_active|created_year|created_month|created_day|updated_at                |\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|1      |john_doe_updated   |john.doe.updated@example.com   |false    |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "|3      |alice_wonder_active|alice.wonder.active@example.com|true     |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "|6      |marketing_user     |marketing@company.com          |true     |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "|7      |support_user       |support@company.com            |true     |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create sample data for basic upsert\n",
    "# This will update existing users and insert new ones\n",
    "current_timestamp = datetime.now()\n",
    "current_year = current_timestamp.year\n",
    "current_month = current_timestamp.month\n",
    "current_day = current_timestamp.day\n",
    "\n",
    "# Define changes:\n",
    "# - User 1: Update email and username\n",
    "# - User 3: Activate user account\n",
    "# - User 6: New user insertion\n",
    "# - User 7: Another new user\n",
    "basic_upsert_data = [\n",
    "    {\n",
    "        'user_id': 1, \n",
    "        'username': 'john_doe_updated', \n",
    "        'email': 'john.doe.updated@example.com', \n",
    "        'is_active': False, \n",
    "        'created_year': current_year, \n",
    "        'created_month': current_month, \n",
    "        'created_day': current_day, \n",
    "        'updated_at': current_timestamp\n",
    "    },\n",
    "    {\n",
    "        'user_id': 3, \n",
    "        'username': 'alice_wonder_active', \n",
    "        'email': 'alice.wonder.active@example.com', \n",
    "        'is_active': True, \n",
    "        'created_year': current_year, \n",
    "        'created_month': current_month, \n",
    "        'created_day': current_day, \n",
    "        'updated_at': current_timestamp\n",
    "    },\n",
    "    {\n",
    "        'user_id': 6, \n",
    "        'username': 'marketing_user', \n",
    "        'email': 'marketing@company.com', \n",
    "        'is_active': True, \n",
    "        'created_year': current_year, \n",
    "        'created_month': current_month, \n",
    "        'created_day': current_day, \n",
    "        'updated_at': current_timestamp\n",
    "    },\n",
    "    {\n",
    "        'user_id': 7, \n",
    "        'username': 'support_user', \n",
    "        'email': 'support@company.com', \n",
    "        'is_active': True, \n",
    "        'created_year': current_year, \n",
    "        'created_month': current_month, \n",
    "        'created_day': current_day, \n",
    "        'updated_at': current_timestamp\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame and temporary view\n",
    "updates_df = spark.createDataFrame(basic_upsert_data, spark.table(\"users\").schema)\n",
    "updates_df.createOrReplaceTempView(\"user_updates\")\n",
    "\n",
    "print(\"Data to be upserted:\")\n",
    "updates_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic merge operation completed in 13.237 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execute basic MERGE INTO operation\n",
    "merge_start_time = time.time()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO users AS target\n",
    "    USING user_updates AS source\n",
    "    ON target.user_id = source.user_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.username = source.username,\n",
    "            target.email = source.email,\n",
    "            target.is_active = source.is_active,\n",
    "            target.updated_at = source.updated_at\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            user_id, username, email, is_active, \n",
    "            created_year, created_month, created_day, updated_at\n",
    "        )\n",
    "        VALUES (\n",
    "            source.user_id, source.username, source.email, source.is_active,\n",
    "            source.created_year, source.created_month, source.created_day, source.updated_at\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "merge_duration = time.time() - merge_start_time\n",
    "print(f\"Basic merge operation completed in {merge_duration:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table state after basic merge:\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|user_id|username           |email                          |is_active|created_year|created_month|created_day|updated_at                |\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "|1      |john_doe_updated   |john.doe.updated@example.com   |false    |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "|2      |jane_smith         |jane.smith@example.com         |true     |2025        |6            |27         |2025-06-27 06:37:58.488064|\n",
      "|3      |alice_wonder_active|alice.wonder.active@example.com|true     |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "|4      |bob_builder        |bob.builder@example.com        |true     |2025        |6            |27         |2025-06-27 06:37:58.488064|\n",
      "|5      |charlie_brown      |charlie.brown@example.com      |true     |2025        |6            |27         |2025-06-27 06:37:58.488064|\n",
      "|6      |marketing_user     |marketing@company.com          |true     |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "|7      |support_user       |support@company.com            |true     |2025        |6            |27         |2025-06-27 06:53:17.799054|\n",
      "+-------+-------------------+-------------------------------+---------+------------+-------------+-----------+--------------------------+\n",
      "\n",
      "\n",
      "Post-merge statistics:\n",
      "+-------------+------------+--------------+\n",
      "|total_records|active_users|inactive_users|\n",
      "+-------------+------------+--------------+\n",
      "|            7|           6|             1|\n",
      "+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the merge results\n",
    "print(\"Table state after basic merge:\")\n",
    "result_df = spark.sql(\"SELECT * FROM users ORDER BY user_id\")\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Show statistics after merge\n",
    "print(\"\\nPost-merge statistics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(CASE WHEN is_active = true THEN 1 END) as active_users,\n",
    "        COUNT(CASE WHEN is_active = false THEN 1 END) as inactive_users\n",
    "    FROM users\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conditional Merge Operations\n",
    "\n",
    "Now let's explore more sophisticated merge patterns with conditional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional update data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---------+------------+-------------+-----------+--------------------+-------------------+\n",
      "|user_id|            username|               email|is_active|created_year|created_month|created_day|          updated_at|last_login_days_ago|\n",
      "+-------+--------------------+--------------------+---------+------------+-------------+-----------+--------------------+-------------------+\n",
      "|      2|  jane_smith_premium|jane.smith.premiu...|     true|        2025|            6|         27|2025-06-27 06:54:...|                  5|\n",
      "|      4|bob_builder_inactive|bob.builder@examp...|    false|        2025|            6|         27|2025-06-27 06:54:...|                 45|\n",
      "|      8|    new_premium_user| premium@company.com|     true|        2025|            6|         27|2025-06-27 06:54:...|                  0|\n",
      "+-------+--------------------+--------------------+---------+------------+-------------+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data for conditional merge example\n",
    "# This demonstrates selective updates based on conditions\n",
    "conditional_data = [\n",
    "    {\n",
    "        'user_id': 2,\n",
    "        'username': 'jane_smith_premium',\n",
    "        'email': 'jane.smith.premium@example.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now(),\n",
    "        'last_login_days_ago': 5  # Additional field for conditional logic\n",
    "    },\n",
    "    {\n",
    "        'user_id': 4,\n",
    "        'username': 'bob_builder_inactive',\n",
    "        'email': 'bob.builder@example.com',\n",
    "        'is_active': False,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now(),\n",
    "        'last_login_days_ago': 45  # Inactive due to long absence\n",
    "    },\n",
    "    {\n",
    "        'user_id': 8,\n",
    "        'username': 'new_premium_user',\n",
    "        'email': 'premium@company.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now(),\n",
    "        'last_login_days_ago': 0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create schema that includes the additional field\n",
    "extended_schema = StructType([\n",
    "    StructField(\"user_id\", LongType(), False),\n",
    "    StructField(\"username\", StringType(), False),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"is_active\", BooleanType(), False),\n",
    "    StructField(\"created_year\", IntegerType(), False),\n",
    "    StructField(\"created_month\", IntegerType(), False),\n",
    "    StructField(\"created_day\", IntegerType(), False),\n",
    "    StructField(\"updated_at\", TimestampType(), False),\n",
    "    StructField(\"last_login_days_ago\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "conditional_df = spark.createDataFrame(conditional_data, extended_schema)\n",
    "conditional_df.createOrReplaceTempView(\"conditional_updates\")\n",
    "\n",
    "print(\"Conditional update data:\")\n",
    "conditional_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional merge completed\n"
     ]
    }
   ],
   "source": [
    "# Execute conditional merge with multiple WHEN clauses\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO users AS target\n",
    "    USING conditional_updates AS source\n",
    "    ON target.user_id = source.user_id\n",
    "    WHEN MATCHED AND source.last_login_days_ago < 30 THEN\n",
    "        UPDATE SET\n",
    "            target.username = source.username,\n",
    "            target.email = source.email,\n",
    "            target.is_active = source.is_active,\n",
    "            target.updated_at = source.updated_at\n",
    "    WHEN MATCHED AND source.last_login_days_ago >= 30 THEN\n",
    "        UPDATE SET\n",
    "            target.is_active = false,\n",
    "            target.updated_at = source.updated_at\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            user_id, username, email, is_active,\n",
    "            created_year, created_month, created_day, updated_at\n",
    "        )\n",
    "        VALUES (\n",
    "            source.user_id, source.username, source.email, source.is_active,\n",
    "            source.created_year, source.created_month, source.created_day, source.updated_at\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Conditional merge completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after conditional merge:\n",
      "+-------+-------------------+-------------------------------+---------+--------------------------+-----------------------------+\n",
      "|user_id|username           |email                          |is_active|updated_at                |merge_status                 |\n",
      "+-------+-------------------+-------------------------------+---------+--------------------------+-----------------------------+\n",
      "|1      |john_doe_updated   |john.doe.updated@example.com   |false    |2025-06-27 06:53:17.799054|Unchanged                    |\n",
      "|2      |jane_smith_premium |jane.smith.premium@example.com |true     |2025-06-27 06:54:07.253577|Modified by conditional merge|\n",
      "|3      |alice_wonder_active|alice.wonder.active@example.com|true     |2025-06-27 06:53:17.799054|Unchanged                    |\n",
      "|4      |bob_builder        |bob.builder@example.com        |false    |2025-06-27 06:54:07.253598|Modified by conditional merge|\n",
      "|5      |charlie_brown      |charlie.brown@example.com      |true     |2025-06-27 06:37:58.488064|Unchanged                    |\n",
      "|6      |marketing_user     |marketing@company.com          |true     |2025-06-27 06:53:17.799054|Unchanged                    |\n",
      "|7      |support_user       |support@company.com            |true     |2025-06-27 06:53:17.799054|Unchanged                    |\n",
      "|8      |new_premium_user   |premium@company.com            |true     |2025-06-27 06:54:07.253601|Modified by conditional merge|\n",
      "+-------+-------------------+-------------------------------+---------+--------------------------+-----------------------------+\n",
      "\n",
      "\n",
      "Note: User 4 should be deactivated due to last_login_days_ago >= 30\n"
     ]
    }
   ],
   "source": [
    "# Verify conditional merge results\n",
    "print(\"Results after conditional merge:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        user_id, username, email, is_active, updated_at,\n",
    "        CASE \n",
    "            WHEN user_id IN (2, 4, 8) THEN 'Modified by conditional merge'\n",
    "            ELSE 'Unchanged'\n",
    "        END as merge_status\n",
    "    FROM users \n",
    "    ORDER BY user_id\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\nNote: User 4 should be deactivated due to last_login_days_ago >= 30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bulk Upsert Performance Optimization\n",
    "\n",
    "Let's demonstrate handling larger datasets and optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1007 records for bulk upsert\n",
      "- New users: 1000\n",
      "- Updated existing users: 7\n"
     ]
    }
   ],
   "source": [
    "# Generate bulk data for performance testing\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_bulk_users(start_id, count):\n",
    "    \"\"\"Generate bulk user data for testing.\"\"\"\n",
    "    bulk_data = []\n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    for i in range(count):\n",
    "        user_id = start_id + i\n",
    "        bulk_data.append({\n",
    "            'user_id': user_id,\n",
    "            'username': f'bulk_user_{user_id}',\n",
    "            'email': f'bulk.user.{user_id}@example.com',\n",
    "            'is_active': random.choice([True, False]),\n",
    "            'created_year': 2025,\n",
    "            'created_month': 6,\n",
    "            'created_day': 27,\n",
    "            'updated_at': base_time + timedelta(seconds=i)\n",
    "        })\n",
    "    \n",
    "    return bulk_data\n",
    "\n",
    "# Generate 1000 new users and 100 updates to existing users\n",
    "new_users = generate_bulk_users(100, 1000)\n",
    "existing_updates = []\n",
    "\n",
    "# Update some existing users (1-8)\n",
    "for user_id in range(1, 8):\n",
    "    existing_updates.append({\n",
    "        'user_id': user_id,\n",
    "        'username': f'updated_user_{user_id}',\n",
    "        'email': f'updated.user.{user_id}@example.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    })\n",
    "\n",
    "# Combine all data\n",
    "bulk_upsert_data = new_users + existing_updates\n",
    "print(f\"Generated {len(bulk_upsert_data)} records for bulk upsert\")\n",
    "print(f\"- New users: {len(new_users)}\")\n",
    "print(f\"- Updated existing users: {len(existing_updates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk DataFrame created and optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=================================================>    (183 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 200\n",
      "\n",
      "Sample of bulk update data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
      "|user_id|     username|               email|is_active|created_year|created_month|created_day|          updated_at|\n",
      "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
      "|    100|bulk_user_100|bulk.user.100@exa...|     true|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    101|bulk_user_101|bulk.user.101@exa...|    false|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    102|bulk_user_102|bulk.user.102@exa...|    false|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    103|bulk_user_103|bulk.user.103@exa...|    false|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    104|bulk_user_104|bulk.user.104@exa...|     true|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    105|bulk_user_105|bulk.user.105@exa...|    false|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    106|bulk_user_106|bulk.user.106@exa...|     true|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    107|bulk_user_107|bulk.user.107@exa...|    false|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    108|bulk_user_108|bulk.user.108@exa...|    false|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "|    109|bulk_user_109|bulk.user.109@exa...|     true|        2025|            6|         27|2025-06-27 06:54:...|\n",
      "+-------+-------------+--------------------+---------+------------+-------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with optimal partitioning\n",
    "bulk_df = spark.createDataFrame(bulk_upsert_data, spark.table(\"users\").schema)\n",
    "\n",
    "# Optimize DataFrame for merge operation\n",
    "bulk_df_optimized = bulk_df \\\n",
    "    .repartition(col(\"created_year\"), col(\"created_month\")) \\\n",
    "    .cache()\n",
    "\n",
    "bulk_df_optimized.createOrReplaceTempView(\"bulk_updates\")\n",
    "\n",
    "print(\"Bulk DataFrame created and optimized\")\n",
    "print(f\"Partitions: {bulk_df_optimized.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Show sample of bulk data\n",
    "print(\"\\nSample of bulk update data:\")\n",
    "bulk_df_optimized.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk merge operation completed in 4.492 seconds\n",
      "Throughput: 224 records/second\n"
     ]
    }
   ],
   "source": [
    "# Execute bulk merge with performance monitoring\n",
    "bulk_start_time = time.time()\n",
    "\n",
    "# Enable query plan analysis\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "merge_result = spark.sql(\"\"\"\n",
    "    MERGE INTO users AS target\n",
    "    USING bulk_updates AS source\n",
    "    ON target.user_id = source.user_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.username = source.username,\n",
    "            target.email = source.email,\n",
    "            target.is_active = source.is_active,\n",
    "            target.updated_at = source.updated_at\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            user_id, username, email, is_active,\n",
    "            created_year, created_month, created_day, updated_at\n",
    "        )\n",
    "        VALUES (\n",
    "            source.user_id, source.username, source.email, source.is_active,\n",
    "            source.created_year, source.created_month, source.created_day, source.updated_at\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "bulk_duration = time.time() - bulk_start_time\n",
    "print(f\"Bulk merge operation completed in {bulk_duration:.3f} seconds\")\n",
    "print(f\"Throughput: {len(bulk_upsert_data) / bulk_duration:.0f} records/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table statistics:\n",
      "+-------------+------------+--------------+-----------+-----------+-------------------+\n",
      "|total_records|active_users|inactive_users|min_user_id|max_user_id|bulk_inserted_users|\n",
      "+-------------+------------+--------------+-----------+-----------+-------------------+\n",
      "|         1008|         518|           490|          1|       1099|               1000|\n",
      "+-------------+------------+--------------+-----------+-----------+-------------------+\n",
      "\n",
      "\n",
      "Sample of bulk-inserted users:\n",
      "+-------+-------------+--------------------+---------+\n",
      "|user_id|     username|               email|is_active|\n",
      "+-------+-------------+--------------------+---------+\n",
      "|    100|bulk_user_100|bulk.user.100@exa...|     true|\n",
      "|    101|bulk_user_101|bulk.user.101@exa...|    false|\n",
      "|    102|bulk_user_102|bulk.user.102@exa...|    false|\n",
      "|    103|bulk_user_103|bulk.user.103@exa...|    false|\n",
      "|    104|bulk_user_104|bulk.user.104@exa...|     true|\n",
      "|    105|bulk_user_105|bulk.user.105@exa...|    false|\n",
      "|    106|bulk_user_106|bulk.user.106@exa...|     true|\n",
      "|    107|bulk_user_107|bulk.user.107@exa...|    false|\n",
      "|    108|bulk_user_108|bulk.user.108@exa...|    false|\n",
      "|    109|bulk_user_109|bulk.user.109@exa...|     true|\n",
      "+-------+-------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify bulk merge results\n",
    "final_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(CASE WHEN is_active = true THEN 1 END) as active_users,\n",
    "        COUNT(CASE WHEN is_active = false THEN 1 END) as inactive_users,\n",
    "        MIN(user_id) as min_user_id,\n",
    "        MAX(user_id) as max_user_id,\n",
    "        COUNT(CASE WHEN user_id >= 100 THEN 1 END) as bulk_inserted_users\n",
    "    FROM users\n",
    "\"\"\")\n",
    "\n",
    "print(\"Final table statistics:\")\n",
    "final_stats.show()\n",
    "\n",
    "# Show sample of recently added users\n",
    "print(\"\\nSample of bulk-inserted users:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, username, email, is_active \n",
    "    FROM users \n",
    "    WHERE user_id BETWEEN 100 AND 109 \n",
    "    ORDER BY user_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Delete Operations with MERGE\n",
    "\n",
    "Iceberg also supports DELETE operations within MERGE statements for data cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup actions to be performed:\n",
      "+-------+----------+\n",
      "|user_id|    action|\n",
      "+-------+----------+\n",
      "|    105|    delete|\n",
      "|    110|    delete|\n",
      "|    115|deactivate|\n",
      "|    120|deactivate|\n",
      "+-------+----------+\n",
      "\n",
      "\n",
      "Current state of affected users:\n",
      "+-------+-------------+--------------------+---------+\n",
      "|user_id|     username|               email|is_active|\n",
      "+-------+-------------+--------------------+---------+\n",
      "|    105|bulk_user_105|bulk.user.105@exa...|    false|\n",
      "|    110|bulk_user_110|bulk.user.110@exa...|     true|\n",
      "|    115|bulk_user_115|bulk.user.115@exa...|    false|\n",
      "|    120|bulk_user_120|bulk.user.120@exa...|     true|\n",
      "+-------+-------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a cleanup dataset - users to be deleted or deactivated\n",
    "cleanup_data = [\n",
    "    {'user_id': 105, 'action': 'delete'},\n",
    "    {'user_id': 110, 'action': 'delete'},\n",
    "    {'user_id': 115, 'action': 'deactivate'},\n",
    "    {'user_id': 120, 'action': 'deactivate'}\n",
    "]\n",
    "\n",
    "cleanup_schema = StructType([\n",
    "    StructField(\"user_id\", LongType(), False),\n",
    "    StructField(\"action\", StringType(), False)\n",
    "])\n",
    "\n",
    "cleanup_df = spark.createDataFrame(cleanup_data, cleanup_schema)\n",
    "cleanup_df.createOrReplaceTempView(\"cleanup_actions\")\n",
    "\n",
    "print(\"Cleanup actions to be performed:\")\n",
    "cleanup_df.show()\n",
    "\n",
    "# Check current state of these users\n",
    "print(\"\\nCurrent state of affected users:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, username, email, is_active \n",
    "    FROM users \n",
    "    WHERE user_id IN (105, 110, 115, 120)\n",
    "    ORDER BY user_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup merge operation completed\n"
     ]
    }
   ],
   "source": [
    "# Execute conditional delete/update merge\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO users AS target\n",
    "    USING cleanup_actions AS source\n",
    "    ON target.user_id = source.user_id\n",
    "    WHEN MATCHED AND source.action = 'delete' THEN\n",
    "        DELETE\n",
    "    WHEN MATCHED AND source.action = 'deactivate' THEN\n",
    "        UPDATE SET\n",
    "            target.is_active = false,\n",
    "            target.updated_at = current_timestamp()\n",
    "\"\"\")\n",
    "\n",
    "print(\"Cleanup merge operation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of cleanup actions:\n",
      "Deleted users found: 0\n",
      "\n",
      "Deactivated users:\n",
      "+-------+-------------+---------+--------------------+\n",
      "|user_id|     username|is_active|          updated_at|\n",
      "+-------+-------------+---------+--------------------+\n",
      "|    115|bulk_user_115|    false|2025-06-27 06:55:...|\n",
      "|    120|bulk_user_120|    false|2025-06-27 06:55:...|\n",
      "+-------+-------------+---------+--------------------+\n",
      "\n",
      "\n",
      "Final record count:\n",
      "+-----------+\n",
      "|total_users|\n",
      "+-----------+\n",
      "|       1006|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify cleanup results\n",
    "print(\"Verification of cleanup actions:\")\n",
    "\n",
    "# Check for deleted users (should return no results)\n",
    "deleted_check = spark.sql(\"\"\"\n",
    "    SELECT user_id, username \n",
    "    FROM users \n",
    "    WHERE user_id IN (105, 110)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Deleted users found: {deleted_check.count()}\")\n",
    "if deleted_check.count() > 0:\n",
    "    deleted_check.show()\n",
    "\n",
    "# Check deactivated users\n",
    "print(\"\\nDeactivated users:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, username, is_active, updated_at \n",
    "    FROM users \n",
    "    WHERE user_id IN (115, 120)\n",
    "    ORDER BY user_id\n",
    "\"\"\").show()\n",
    "\n",
    "# Final count\n",
    "print(\"\\nFinal record count:\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_users FROM users\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Performance Optimization\n",
    "\n",
    "Let's explore key best practices for efficient upsert operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table maintenance operations:\n",
      "\n",
      "1. Current table files:\n",
      "+----------+\n",
      "|file_count|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n",
      "\n",
      "2. Partition information:\n",
      "+------------+-------------+------------+\n",
      "|created_year|created_month|record_count|\n",
      "+------------+-------------+------------+\n",
      "|        2025|            6|        1006|\n",
      "+------------+-------------+------------+\n",
      "\n",
      "\n",
      "3. Table optimization recommendations:\n",
      "- Use MERGE operations during low-traffic periods\n",
      "- Partition source data to match target partitioning\n",
      "- Use appropriate cluster sizes for bulk operations\n",
      "- Monitor file counts and perform compaction when needed\n",
      "- Consider using Delta Lake's OPTIMIZE command for maintenance\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate table maintenance for optimal performance\n",
    "print(\"Table maintenance operations:\")\n",
    "\n",
    "# Show current table metadata\n",
    "print(\"\\n1. Current table files:\")\n",
    "files_info = spark.sql(\"SELECT COUNT(*) as file_count FROM rest.`play-iceberg`.users.files\")\n",
    "files_info.show()\n",
    "\n",
    "# Show table partitions\n",
    "print(\"\\n2. Partition information:\")\n",
    "partition_info = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        created_year, created_month, \n",
    "        COUNT(*) as record_count\n",
    "    FROM users \n",
    "    GROUP BY created_year, created_month \n",
    "    ORDER BY created_year, created_month\n",
    "\"\"\")\n",
    "partition_info.show()\n",
    "\n",
    "# Demonstrate compaction (if supported in your environment)\n",
    "print(\"\\n3. Table optimization recommendations:\")\n",
    "print(\"- Use MERGE operations during low-traffic periods\")\n",
    "print(\"- Partition source data to match target partitioning\")\n",
    "print(\"- Use appropriate cluster sizes for bulk operations\")\n",
    "print(\"- Monitor file counts and perform compaction when needed\")\n",
    "print(\"- Consider using Delta Lake's OPTIMIZE command for maintenance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Merge Patterns\n",
    "\n",
    "Explore complex scenarios and edge cases in merge operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 3\n",
      "Valid records: 1\n",
      "Invalid records filtered out: 2\n",
      "\n",
      "Valid records to be merged:\n",
      "+-------+----------+-----------------+---------+------------+-------------+-----------+--------------------+\n",
      "|user_id|  username|            email|is_active|created_year|created_month|created_day|          updated_at|\n",
      "+-------+----------+-----------------+---------+------------+-------------+-----------+--------------------+\n",
      "|   1001|valid_user|valid@example.com|     true|        2025|            6|         27|2025-06-27 06:56:...|\n",
      "+-------+----------+-----------------+---------+------------+-------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate merge with data validation\n",
    "# Create source data with some invalid records\n",
    "validation_data = [\n",
    "    {\n",
    "        'user_id': 1001,\n",
    "        'username': 'valid_user',\n",
    "        'email': 'valid@example.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    },\n",
    "    {\n",
    "        'user_id': 1002,\n",
    "        'username': '',  # Invalid: empty username\n",
    "        'email': 'invalid@example.com',\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    },\n",
    "    {\n",
    "        'user_id': 1003,\n",
    "        'username': 'another_valid_user',\n",
    "        'email': 'not-an-email',  # Invalid: bad email format\n",
    "        'is_active': True,\n",
    "        'created_year': 2025,\n",
    "        'created_month': 6,\n",
    "        'created_day': 27,\n",
    "        'updated_at': datetime.now()\n",
    "    }\n",
    "]\n",
    "\n",
    "validation_df = spark.createDataFrame(validation_data, spark.table(\"users\").schema)\n",
    "\n",
    "# Add validation logic\n",
    "validated_df = validation_df.filter(\n",
    "    (col(\"username\") != \"\") & \n",
    "    (col(\"username\").isNotNull()) &\n",
    "    (col(\"email\").contains(\"@\")) &\n",
    "    (col(\"email\").contains(\".\"))\n",
    ")\n",
    "\n",
    "validated_df.createOrReplaceTempView(\"validated_updates\")\n",
    "\n",
    "print(f\"Original records: {validation_df.count()}\")\n",
    "print(f\"Valid records: {validated_df.count()}\")\n",
    "print(f\"Invalid records filtered out: {validation_df.count() - validated_df.count()}\")\n",
    "\n",
    "print(\"\\nValid records to be merged:\")\n",
    "validated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated merge completed\n",
      "+-------+--------------+--------------------+\n",
      "|user_id|      username|               email|\n",
      "+-------+--------------+--------------------+\n",
      "|   1001|bulk_user_1001|bulk.user.1001@ex...|\n",
      "|   1002|bulk_user_1002|bulk.user.1002@ex...|\n",
      "|   1003|bulk_user_1003|bulk.user.1003@ex...|\n",
      "+-------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute merge with validated data\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO users AS target\n",
    "    USING validated_updates AS source\n",
    "    ON target.user_id = source.user_id\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            user_id, username, email, is_active,\n",
    "            created_year, created_month, created_day, updated_at\n",
    "        )\n",
    "        VALUES (\n",
    "            source.user_id, source.username, source.email, source.is_active,\n",
    "            source.created_year, source.created_month, source.created_day, source.updated_at\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Validated merge completed\")\n",
    "\n",
    "# Verify only valid record was inserted\n",
    "spark.sql(\"\"\"\n",
    "    SELECT user_id, username, email \n",
    "    FROM users \n",
    "    WHERE user_id IN (1001, 1002, 1003)\n",
    "    ORDER BY user_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Cleanup\n",
    "\n",
    "Let's summarize what we've learned and clean up our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UPSERT OPERATIONS TUTORIAL SUMMARY ===\n",
      "\n",
      "+----------------------------+-----+\n",
      "|metric                      |value|\n",
      "+----------------------------+-----+\n",
      "|Active Users                |516  |\n",
      "|Bulk Inserted Users (>= 100)|998  |\n",
      "|Inactive Users              |490  |\n",
      "|Total Records               |1006 |\n",
      "+----------------------------+-----+\n",
      "\n",
      "\n",
      "Key Concepts Covered:\n",
      "1. Basic MERGE INTO operations\n",
      "2. Conditional merge logic with multiple WHEN clauses\n",
      "3. Bulk upsert performance optimization\n",
      "4. DELETE operations within MERGE statements\n",
      "5. Data validation before merge\n",
      "6. Performance monitoring and best practices\n",
      "\n",
      "Best Practices Demonstrated:\n",
      "- Partition alignment for performance\n",
      "- Data validation and quality checks\n",
      "- Conditional merge logic for complex scenarios\n",
      "- Performance monitoring and optimization\n",
      "- Proper error handling and edge cases\n"
     ]
    }
   ],
   "source": [
    "# Final table summary\n",
    "print(\"=== UPSERT OPERATIONS TUTORIAL SUMMARY ===\")\n",
    "print()\n",
    "\n",
    "final_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Total Records' as metric,\n",
    "        CAST(COUNT(*) as STRING) as value\n",
    "    FROM users\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Active Users' as metric,\n",
    "        CAST(COUNT(*) as STRING) as value\n",
    "    FROM users WHERE is_active = true\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Inactive Users' as metric,\n",
    "        CAST(COUNT(*) as STRING) as value\n",
    "    FROM users WHERE is_active = false\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Bulk Inserted Users (>= 100)' as metric,\n",
    "        CAST(COUNT(*) as STRING) as value\n",
    "    FROM users WHERE user_id >= 100\n",
    "    \n",
    "    ORDER BY metric\n",
    "\"\"\")\n",
    "\n",
    "final_summary.show(truncate=False)\n",
    "\n",
    "print(\"\\nKey Concepts Covered:\")\n",
    "print(\"1. Basic MERGE INTO operations\")\n",
    "print(\"2. Conditional merge logic with multiple WHEN clauses\")\n",
    "print(\"3. Bulk upsert performance optimization\")\n",
    "print(\"4. DELETE operations within MERGE statements\")\n",
    "print(\"5. Data validation before merge\")\n",
    "print(\"6. Performance monitoring and best practices\")\n",
    "\n",
    "print(\"\\nBest Practices Demonstrated:\")\n",
    "print(\"- Partition alignment for performance\")\n",
    "print(\"- Data validation and quality checks\")\n",
    "print(\"- Conditional merge logic for complex scenarios\")\n",
    "print(\"- Performance monitoring and optimization\")\n",
    "print(\"- Proper error handling and edge cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session cleanup completed successfully\n",
      "\n",
      "Spark session active: PySparkShell\n",
      "Tutorial completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Clean up cached DataFrames and temporary views\n",
    "try:\n",
    "    spark.catalog.dropTempView(\"user_updates\")\n",
    "    spark.catalog.dropTempView(\"conditional_updates\")\n",
    "    spark.catalog.dropTempView(\"bulk_updates\")\n",
    "    spark.catalog.dropTempView(\"cleanup_actions\")\n",
    "    spark.catalog.dropTempView(\"validated_updates\")\n",
    "    \n",
    "    # Unpersist cached DataFrames\n",
    "    if 'bulk_df_optimized' in locals():\n",
    "        bulk_df_optimized.unpersist()\n",
    "    \n",
    "    print(\"Session cleanup completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Cleanup warning: {e}\")\n",
    "\n",
    "print(f\"\\nSpark session active: {spark.sparkContext.getConf().get('spark.app.name')}\")\n",
    "print(\"Tutorial completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
